# Narrative Technical Approach for Injecting Merchant–location realism into a Synthetic Data Generator for Fraud Detection

## Segment 1: `transaction_schema` → From merchants to physical sites

### From merchants to physical sites

The generator’s opening manoeuvre is to transform the symbolic `merchant_id` rows in the bare `transaction_schema` into a concrete list of outlet stubs—still placeless, still timezone‑agnostic, but already carrying every decision about *how many* outlets the merchant operates and *in which countries* the chain will legally trade. The reviewer must be able to rerun this stage, byte‑for‑byte, months later and see identical results, so the narrative below treats determinism and auditability as first‑class requirements.

---

The build process begins with a tight handshake between code and parameters. Immediately after start‑up the pipeline reads three artefact bundles that live under *git LFS* and are tagged by semantic version: `hurdle_coefficients.yaml`, `crossborder_hyperparams.yaml`, and the *spatial‐prior bundle* directory. The loader computes a SHA‑256 digest for every file it touches and concatenates those digests—plus the commit hash of the source tree—into a single 256‑bit manifest fingerprint. That fingerprint accompanies every record produced downstream; if a colleague changes even one coefficient and forgets to increment the manifest version, the downstream pipeline will abort because the fingerprint no longer matches the value embedded inside the data being read.

With the governing artefacts frozen, the code constructs the *merchant design matrix*. For each merchant the matrix holds three fixed‑effect columns—MCC one‑hot, channel one‑hot, and an ordinal development bucket derived from the merchant’s registered country GDP per capita—and an intercept. GDP per capita is looked up in the most recent World‑Bank CSV and discretised into five buckets using Jenks natural breaks; the mapping table is also stamped with its own SHA‑256 so reviewers can rerun the binning step on a later vintage. All bucket assignments are therefore deterministic functions of ISO‑3166 code and the versioned GDP data‑set.

The *single‑versus‑multi* decision is the first random action. The generator multiplies the design matrix row by the logistic‑regression coefficient vector stored in `hurdle_coefficients.yaml`, applies the logistic function, and obtains a probability π that this merchant is multi‑site. Philox 2¹²⁸ supplies a single uniform deviate; if it is less than π the merchant advances to the multi‑site branch, otherwise it is locked to exactly one outlet. Because the logistic coefficients were estimated on a proprietary acquirer sample and released only as YAML, the open‑source code never sees raw data yet reproduces the same probabilities. The Bernoulli outcome is persisted to the row so that auditors can stratify later statistics by single‑versus‑multi without re‑deriving the decision.

For every merchant declared multi‑site the pipeline computes the mean μ and dispersion φ of a negative‑binomial. Both are log‑linear functions of the same design matrix that drove the logistic, with the additional nuance that log φ carries a continuous GDP per capita term. Low GDP shifts log φ upwards, inflating variance and lengthening the right tail of the outlet‑count distribution; high GDP pulls log φ downward, producing tighter clouds that match the more disciplined expansion behaviour of chains in rich markets. A deviate from NB(μ, φ) is drawn. If the variate is zero, rejection sampling triggers immediately, draws again, and records the rejection in an *RNG audit log* that the pipeline writes alongside the final Parquet. The log includes the seed state and the rejected value, giving validators a categorical record of every instance where the parametric family needed help to enforce its logical domain.

The raw outlet count N is now known, but the geographic footprint is still limited to “home country”. The pipeline therefore evaluates a zero‑truncated Poisson for the *number of extra countries* beyond the home jurisdiction. The rate of this Poisson, $\lambda_{extra}$, is a linear function of log N and two hyper‑parameters stored in `crossborder_hyperparams.yaml`. This functional form mirrors the empirically observed sub‑linear growth of geographic sprawl: doubling the chain size does not double the number of countries, but it does increase the probability of at least one foreign market appearing. A rejection‑free draw from ZTPoisson yields K; K may be zero.

When K is non‑zero, the pipeline assembles a candidate list of foreign jurisdictions. The list is sampled without replacement from the ISO‑3166 universe by probabilities proportional to a pre‑computed *network share vector*. That vector captures the fraction of the home currency’s card spend that clears against each partner currency and is updated quarterly from publicly released interchange tables; it therefore changes slowly, granting temporal stability without freezing the world economy in time. With K extras selected and the home country appended, the stage is set for a Dirichlet allocation.

The code loads the α‑vector applicable to the tuple `(home_country, mcc_code, channel)` from `crossborder_hyperparams.yaml`; the vector has length K+1 and has been calibrated so that the expected outbound ratio for each originating country reproduces historical cross‑border settlement. A single draw from Dirichlet(α) produces fractional weights w. Multiplying w by the integer outlet count N gives real‑valued allocations; largest‑remainder rounding converts them to integers that still sum to N. The rounding procedure is deterministic: floor every weight, compute the deficit d = N − ∑ floor, order the residual fractions descending and give one extra outlet to the first d entries. Because fractions sort stably, two runs with the same floating‑point output yield identical integer allocations—an important subtlety when floating‑point tie‑breakers can otherwise spoil reproducibility.

At this point the algorithm assigns monotonically increasing `site_id`s. The numbering is merchant‑local and deterministic: sites are sorted lexicographically by ISO‑country code and then by the random integer that came from the largest‑remainder tie‑breaker so that the catalogue diff is stable across runs. Each stub row now contains `merchant_id`, `site_id`, `country_iso`, the single‑versus‑multi flag, the raw draw N, the final integer outlet count per country, and the manifest fingerprint. The PRNG stream is then jumped forward by a *module‑specific stride* derived by hashing the string `"cross-border-allocation"`; subsequent modules therefore consume untouched entropy.

The narrative halts here intentionally. Every outlet is counted and legally domiciled, but no latitude, longitude or timezone exists yet. The coordinate sampling logic belongs to the next sub‑segment, and because the manifest fingerprint already reflects every parameter that influenced the decisions above, the downstream geographic sampler can never retroactively change outlet counts or country assignments without emitting a new fingerprint and therefore establishing an explicit break in lineage. This strict layering—abstract counts first, concrete coordinates later—is what allows the entire location pipeline to be audited in two clean, orthogonal chunks, a property that weighs heavily in our favour during the inevitable ruthless review.

### Placing outlets on the planet

The moment the catalogue says **“merchant X operates N sites in country Y,”** a second engine has to materialise those sites on the globe with enough fidelity that a geospatial analyst could overlay them on open data and nod that the pattern looks real. That engine is the sub‑segment we labelled **“Placing outlets on the planet.”** Its job is to turn an unlocated site identifier into a latitude, longitude, elevation proxy, prior weight and provenance trail, all recorded with cryptographic hashes that prove which spatial artefacts were involved. What follows is the full narrative of how I would build that engine so that every stochastic decision is traceable, every edge‑case is handled deterministically, and every modelling assumption can be overturned by editing a YAML file rather than touching code.

---

The first step is to prepare a **library of spatial priors** keyed by `(mcc_code, channel)`. Each prior is either a raster (GeoTIFF) or a vector layer (ESRI shapefile or GeoPackage) that assigns a non‑negative weight to every location within a country. For high‑street retail the prior is the 100‑metre resolution population density raster published by Facebook’s High Resolution Settlement Layer; for vehicle‑oriented merchants the prior is a clipped Open‑Street‑Map primary‑road network whose line segments carry an AADT attribute scraped from government traffic counts; for duty‑free and travel‑retail MCCs the prior is the IATA polygon of commercial‑airport boundaries; for big‑box categories the prior is a blended raster formed by taking a convex combination of suburban population density and road traffic density. The blend coefficients live in `spatial_blend.yaml`; a reviewer can change a coefficient and regenerate a new catalogue to test sensitivity. Every file in the prior library is accompanied by a checksum file; the build concatenates those checksums and records a grand digest in the site catalogue so that anyone can reproduce exactly which map was used.

With priors ready, the engine enters an **importance‑sampling loop** that is deterministic given the global RNG stream. For a given `site_id` in country `c` the engine first checks whether a category‑specific prior exists for that country. If it does, the engine opens the raster header, reads the grid of weights, and constructs a cumulative‑distribution function over pixel indices. Because the raster may contain millions of pixels, the CDF is stored in a succinct Fenwick tree that allows O(log n) prefix‑sum queries but only O(n) memory once across the whole build. The engine samples a pixel index proportional to its weight, then converts the grid coordinate to a geographic bounding box. If the prior is a vector layer instead of a raster, the engine first samples a feature with probability proportional to its associated weight (segment length times AADT for roads, polygon area for airports), then chooses a random point uniformly along the feature’s geometry; if the geometry is a polyline the point is uniform in arc length, if a polygon it is uniform in surface area via rejection sampling on an enclosing rectangle.

Once a coordinate candidate has been drawn, a **land‑water sanity pass** runs. The candidate is tested against the 1:10 m Natural‑Earth land polygon via point‑in‑polygon; if it is in water, or if it lies more than fifty metres from the parent road when the prior is a road network, the point is rejected and the sampling loop repeats. This ensures that all sites end up on dry land or precisely on the intended transport corridor. Because the spatial‑prior weights can occasionally put mass directly on the shoreline, the expected number of iterations is still low; the loop terminates almost always on the first or second try.

Every accepted coordinate is tagged with a **`prior_tag`** string that records the exact prior artefact (for example, `pop_density_hrsl_2024q2`, `osm_primary_road_aadt_202503`) and the numeric weight value that was used in the CDF. The tag is written into the site catalogue so that a GIS audit can sample rows stratified by tag and overlay them on the underlying map to verify plausibility.

Elevation is not needed for fraud‑detection models, but some downstream anomaly features rely on approximate travel time. Rather than query a digital elevation model for every point, the engine stores the **Haversine distance to the country’s capital** and, when the prior is a road network, the shortest on‑network driving distance to the capital as well. The former is computed analytically, the latter by invoking a pre‑built contraction‑hierarchies routing graph loaded into RAM. Both numbers are recorded in the catalogue and serve as cheap proxies for remoteness without bloating the temporal generator.

Edge cases arise when a country is so small—or an MCC so specialised—that the chosen prior has zero support inside its borders. One example is a land‑locked micro‑state with no roads that meet the primary‑road tag. To maintain determinism the build falls back to a **country‑level population raster** in those cases. The fallback is not silent: the catalogue writes `prior_tag='FALLBACK_POP'` and CI checks that the fraction of fallbacks remains below one percent globally. If a reviewer considers the fallback rate too high they can add a bespoke prior for the missing category.

After the coordinate is finalised, the engine calls `tz_world.lookup(lat, lon)` to retrieve the **civil time zone**, but only to verify that the coordinate lies in the expected country; full time‑zone assignment is deferred to the next sub‑segment. If the lookup returns a time‑zone whose ISO‑alpha‑2 country code does not match the country assigned at the allocation stage, that is evidence of a border overlap or a data‑error in the prior and the point is resampled. Because border overlaps are rare and the engine logs each failure with the offending geometry ID, a reviewer can isolate and inspect problematic priors.

Finally, the **foot‑traffic scalar** is computed. The engine multiplies a category‑specific load factor κ (read from `footfall_coefficients.yaml`) with the numeric weight of the raster pixel or vector feature and then with a Log‑Normal residual. The residual’s variance parameter σ² sits in the same YAML artefact and is tuned so that, in a dry run of the temporal engine, the Fano factor of hourly legitimate counts lies inside the 1.3–1.7 corridor. If κ or σ² change, the catalogue’s manifest hash changes, forcing regeneration downstream; this prevents silent drift in temporal dispersion.

Each completed site row is written immediately to a Parquet partition keyed by `merchant_id` and `site_id` to make the build crash‑tolerant; a re‑run with the same seed picks up only missing partitions. When all merchants are processed the build writes a **manifest JSON** that records the seed, the composite hash of every spatial artefact, and the wall‑clock build time. That manifest is itself checksummed and appended to the top‑level catalogue directory. Any downstream simulation step verifies the checksum before proceeding so that an out‑of‑date catalogue cannot be paired with newer artefacts by accident.

Because every coordinate arises from a publicly documentable prior, every randomness comes from a known seed in an isolated Philox stream, every fallback is logged, every tag is retained, and every artefact is hashed into the manifest, the **“Placing outlets on the planet”** engine withstands not only statistical checks but forensic audits of the entire spatial pipeline.

## Segment 2: Deriving the civil time zone → Routing transactions through sites

### Deriving the civil time zone

The material that follows is not a synopsis; it is a verbatim exposure of every premise, data‑source dependency, numerical convention and protective rail that underpins the **“Deriving the civil time zone”** stage. The language is continuous and discursive so that the implementer can translate it line‑for‑line into code without needing to infer anything. All references to artefacts—whether shapefiles, YAML registries or version strings—are the literal filenames used in the build tree, and every safeguard is identified by the exact condition that triggers it.

---

The stage begins by opening the **tz‑world shapefile** whose basename is `tz_world_2025a.shp`. Its SHA‑256 digest is embedded in the catalogue manifest under the key `tz_polygon_digest`. The file’s coordinate reference system is EPSG:4326; the build refuses to continue if `fiona.open()` reports anything else. Every polygon record’s attribute table contains the column `TZID`. Immediately after loading, the code populates an **STR‑tree spatial index** with each polygon’s bounding box; the index’s construction is deterministic because the polygons are inserted in lexicographic order of `TZID`. Determinism of the index matters because the order in which bounding‑box enlargement occurs inside the STR‑tree algorithm affects its shape and therefore its traversal path later. An audit routine computes an MD5 digest of the pickled STR‑tree object and stores it in the manifest. A re‑run with the same shapefile and seed produces the identical digest, proving that the spatial index itself is reproducible.

When an outlet coordinate is fed into the engine, the STR‑tree is queried for candidate polygons. The shortlist is traversed in the order returned by the index; for each polygon the engine calls `prepared_polygon.contains(point)` from the *Shapely* library. If exactly one polygon claims the point, the `TZID` from that polygon becomes the site’s provisional civil time zone. If zero polygons claim the point, the engine raises `TimeZoneLookupError` with the offending latitude, longitude and the `prior_tag` that generated the coordinate; the CI harness intercepts the exception, marks the artefact as defective and halts the build. If two polygons claim the point—a circumstance that occurs along borders where vertex precision is lower than floating‑point representation—the engine performs a deterministic tie‑break. Let the two polygons be $P_1$ and $P_2$ with areas $A_1$ and $A_2$. The algorithm computes the vector from the point $x$ to the centroid of the smaller polygon,
$$
c = \operatorname{centroid}(\arg\min\{A_1, A_2\}).
$$
It normalises this vector to unit length, multiplies it by a scalar ε where ε = 0.0001° (read from `tz_nudge.yml`), then adds the displacement to $x$ producing
$$
x' = x + \varepsilon \frac{c - x}{\lVert c - x\rVert}.
$$
Because the smallest strip of land in tz‑world exceeds one kilometre in width, a 0.0001° nudge (~11 m at the equator) cannot push the point into an unrelated third polygon. The nudge vector is stored in the site catalogue columns `nudge_lat` and `nudge_lon` so that a forensic examiner can replicate the calculation with independent software.

Tz‑world polygons, however comprehensive, lag behind political decrees. All deviations are centralised in the file `tz_overrides.yaml`. Each override item is a mapping with fields: `scope`, `tzid`, `evidence_url`, `expiry_yyyy_mm_dd`. The scope can be one of three forms: the string `"country:CA"` meaning the entire ISO country, the pattern `"mcc:5411"` meaning every grocery outlet, or the tuple `("merchant_id", "site_id")` targeting an individual site. A pre‑commit Git hook verifies that `evidence_url` is a valid URL and that `expiry_yyyy_mm_dd` is parsable. During the lookup phase the engine checks overrides in precedence order: site‑specific first, then MCC‑wide, then country‑wide. If an override matches, its `TZID` supersedes the polygon result. Nightly CI opens the fresh site catalogue, walks through every row, reapplies the override logic and asserts that at least one override location would change if the shapefile alone were used; a count of zero triggers a rejection because it proves the override is obsolete.

With a final `TZID` attached, the engine consults the **IANA zoneinfo database** version `tzdata2025a`. The version string lives in `zoneinfo_version.yml`; changing it changes the manifest hash. Zoneinfo is queried through the standard `zoneinfo.ZoneInfo` API. For each distinct `TZID` present in the catalogue the engine enumerates its `_utc_transition_times` list, intersects it with the simulation horizon and writes a compact two‑column timetable `(transition_epoch, offset_minutes)`. The timetable is stored in memory as two numpy arrays of dtype `int64` and `int16` respectively, then compressed with run‑length encoding before being attached to the `tz_cache` singleton as Python bytes. Run‑length encoding is adopted because zones that abolished DST decades ago consist chiefly of repeated offsets, and encoding each repeated segment as `(count, value)` pairs reduces average size to about 1 kB per zone. A memory gauge after construction writes the exact byte size of the entire cache into the manifest; if a future upgrade to the IANA file inadvertently triples memory, CI surfaces the expansion where a silent leak could otherwise go unnoticed.

When the temporal engine later generates a **local epoch second** $t_{\text{local}}$, the civil‑time module must map it to UTC. The timetable for the site’s zone is bisection‑searched (via `numpy.searchsorted`) to locate the most recent transition epoch not later than $t_{\text{local}}$. The associated offset $o$ in minutes is read; the provisional UTC epoch is
$$
t_{\text{utc}} = t_{\text{local}} - 60\,o.
$$
If $t_{\text{local}}$ lies strictly between a transition epoch and that epoch plus the forward gap length, it is illegal. The engine replaces it by
$$
t_{\text{legal}} = t_{\text{gap_end}},
$$
sets `dst_adjusted=True` and writes `gap_seconds = t_{\text{legal}} - t_{\text{local}}`. That surplus interval is returned to the arrival engine so that the waiting‑time distribution remains statistically faithful. If instead $t_{\text{local}}$ matches two legal instants in the fall‑back fold, the engine determines the correct fold by parity of a hash on `(site_id, t_{\text{local}})`, assigns `fold=0` for the first hour or `fold=1` for the second, and moves on. This parity rule ensures identical behaviour across re‑runs even though CPython’s hash randomisation changes between interpreter invocations.

The module then writes the integer **local‑time offset** $o$ to the transaction buffer. The row now contains `(event_time_utc, local_time_offset, dst_adjusted, fold)`; these four fields are sufficient for any consumer to reconstruct unambiguously the civil time of the transaction. A validation routine outside the critical path performs a Monte‑Carlo check each night: it samples one million rows, reverts them to local time by adding the offset, looks up the `TZID` in tz‑world, checks whether that `TZID` equals the row’s original zone or if the row is covered by an override, and asserts equality. Failure causes the build to abort, guaranteeing that no silent inconsistency can creep in.

The random‑number generator does not influence the civil‑time mapping directly, but the deterministic parity hash for fold assignment uses SHA‑256 with the global seed concatenated to `(site_id, local_epoch)`, ensuring that fold disambiguation remains stable under seed changes but will differ if `site_id` mutates, as required for reproducibility.

Every constant—ε for the border nudge, the IANA version string, the maximum allowed memory for the timetable cache—is stored in a YAML artefact whose digest is printed inside the site catalogue manifest. Every external shapefile, every override line and every computed STR‑tree digest is likewise captured. Therefore, if a reviewer alters *any* of these inputs, downstream consumers will detect the manifest drift and insist on a complete regeneration, maintaining the chain of reproducibility.


### Routing transactions through sites

Once the catalogue has given every merchant a concrete constellation of sites, each with a latitude, longitude, time‑zone and foot‑traffic scalar, the simulation must decide—millions of times per synthetic day—**which single outlet actually receives a candidate transaction**. That decision is orchestrated in the sub‑segment called *“Routing transactions through sites,”* and its ambition is to be indistinguishable from the logic that lives inside a real acquirer’s authorisation switch. The core requirement is to translate an abstract arrival event, generated upstream by the Log‑Gaussian‑Cox process in “local civil time”, into an `(event_time_utc, site_id)` pair whose spatial choice respects (1) the long‑run market share implied by foot‑traffic weights, (2) the cross‑zone synchrony observed when corporate promotions roll out, and (3) the brutally unforgiving reproducibility contract that governs the entire pipeline.

---

The routing engine’s first responsibility is to **freeze an immutable probability law** that maps every merchant’s outlet list to a set of normalised weights. Let the merchant have sites indexed by $i = 1, \dots, N_m$. Each site carries a positive foot‑traffic scalar $F_i$ inherited verbatim from the placement stage. The engine computes the raw share

$$
w_i = F_i, \quad \text{for all } i,
$$

then normalises to obtain

$$
p_i = \frac{w_i}{\sum_{j=1}^{N_m} w_j}.
$$

These weights are written to disk once as a two‑column table `(site_id, p_i)` sorted lexicographically by `site_id`; this strict ordering, together with the fact that sums of IEEE‑754 doubles are rounded identically on any IEEE‑compliant CPU, means two developers running the build on different machines will obtain byte‑identical $p_i$.

Because naïve multinomial sampling in $O(N_m)$ time would choke on global merchants that own thousands of outlets, the pipeline constructs **an alias table** per merchant. The deterministic alias construction proceeds by streaming through the $p_i$ vector in the order stored on disk, pushing indices into a “small” or “large” stack according to whether $p_i < 1/N_m$ or not, and then repeatedly popping one from each until neither stack holds any elements, filling the `prob` and `alias` arrays in place. At no point does the algorithm draw fresh random numbers: the entire table is a pure, deterministic function of the ordered probability vector. The resulting pair of arrays occupies exactly two 32‑bit integers per site and therefore fits comfortably in memory even for the largest chains in the synthetic universe.

Long‑run shares, however, are not enough; real data reveal a subtle **cross‑zone co‑movement** that flares up when a corporate promotion starts at 00:00 local time independent of the head‑office’s zone. To reproduce that phenomenon the routing engine introduces a **latent “corporate‑day” random effect** $\gamma_d$, drawn once per merchant per simulated UTC day $d$ from

$$
\log \gamma_d \sim \mathcal{N}\bigl(-\tfrac12 \sigma_{\gamma}^2,\;\sigma_{\gamma}^2\bigr),
$$

where the mean shift keeps $\mathbb{E}[\gamma_d] = 1$. The variance $\sigma_{\gamma}^2$ is stored in `routing_day_effect.yml`, calibrated so that, in the real JPM audit logs used as reference, the Pearson correlation of site‑level hourly counts across time‑zones settles around 0.35. The draw occurs on the Philox sub‑stream reserved for the routing module, keyed by the merchant’s identifier; since only one $\gamma_d$ is drawn per UTC day per merchant, the random‑number budget is negligible and deterministic.

When the arrival engine proposes a local timestamp $t_{\text{local}}$ for some merchant, it passes control to the router before UTC conversion. The router first computes the candidate’s UTC date $d$ by subtracting the site’s current offset from $t_{\text{local}}$; it multiplies every $p_i$ by $\gamma_d$ and then **re‑normalises within the time‑zone group** to retain each zone’s diurnal shape. This modulation reproduces the observed fact that when a promo email lands in inboxes, the blast lifts every store in the company’s footprint at once, yet the uplift feels strongest inside each zone’s normal trading hours. Because $\gamma_d$ is multiplicative and common, re‑normalisation preserves the strictly positive ordering $p_i > 0$ and therefore the already‑built alias table remains valid: the router only needs to scale the threshold it compares against the uniform random draw, avoiding any per‑transaction table rebuild.

With weights modulated, the router pulls a single 64‑bit uniform $u$ from its Philox sub‑stream, computes the integer column index

$$
k = \lfloor u \times N_m \rfloor
$$

and uses the pre‑computed `prob[k]` to decide whether to accept $k$ or fall through to `alias[k]`. The whole operation is $O(1)$ and branch‑predictable, so the CPU cost is a few nanoseconds even at high throughput.

Certain merchants are marked **“purely virtual”** in the catalogue. For them, $N_m$ equals one by definition because an e‑commerce gateway with no physical storefront owns only the settlement site. Nevertheless, reviewers expect variability in the apparent source country of IP addresses. To honour that, the single site gets a shadow list of virtual “edge nodes” whose country attribution follows the regional CDN distribution stored in `cdn_country_weights.yml`. Routing among those virtual nodes happens with its own alias table built exactly as before, and the chosen node’s geo appears in the row as `ip_country` rather than `merchant_country`, letting downstream location‑mismatch features light up without ever falsifying the merchant’s brick‑and‑mortar footprint.

After an outlet is selected, the router returns its coordinate and `tzid` to the temporal engine, which converts $t_{\text{local}}$ to UTC, completes gap/fold logic, and writes the transaction record. Because the router never alters $t_{\text{local}}$, all day‑part seasonality already injected by the LGCP engine survives intact at site granularity.

Finally, the engine logs a **routing checksum** once per million routed events: it hashes the tuple `(merchant_id, batch_index, cumulative_site_counts[])`, where the last element is the vector of how many events each site has received so far, and writes the checksum to `routing_audit.log`. A downstream integration‑test reruns the router in isolation with the same seed and asserts that the checksums match. If a developer mistakenly changes the order of random draws inside the router or alters the alias‑table logic, the audit will fail at the very first batch, guarding against silent erosion of reproducibility.

By anchoring the selection law in static, foot‑traffic‑based alias tables; by introducing a single, low‑entropy, corporate‑day random effect that synchronises zone‑separated sites; by honouring virtual gateways with a country‑weighted edge‑selection layer; and by welding every step to a manifest checksum and Philox stream isolation, the **“Routing transactions through sites”** sub‑segment delivers outlet‑level realism, computational speed, and forensic repeatability that satisfy the most severe production readiness review.  


## Segment 3: "Capturing cross-zone merchants" → "Special treatment for purely virtual merchants"
### Capturing cross‑zone merchants

In the synthetic universe a “cross‑zone” merchant must behave like the household names in the real settlement stream: one legal entity, one settlement currency, yet outlets appear in several civil time‑zones and their local clocks all tick when headquarters launches a promo. To reproduce that behaviour the generator inserts a zone‑allocation layer that sits squarely between the country‑level outlet split finished in the hurdle step and the per‑site coordinate sampler. The layer’s charter is three‑fold:

1. Transform every merchant’s country‑mass vector into legally plausible time‑zone mass while preserving the exact integer outlet counts already drawn for each country.  
2. Sample from a parameterised prior whose hyperparameters come straight from public network‑settlement statistics—any regulator can rerun the exercise and watch the geography morph in a fully predictable way.  
3. Once the zone allocation exists, the temporal engine must be able to make stores in different offsets surge together when corporate marketing acts—otherwise the data fail the forensic “offset‑barcode” test auditors love to run.  

---

The procedure begins as soon as the hurdle layer commits a merchant’s outlet counts by ISO‑3166 country. Call that vector $v$, normalized so that $\sum v = 1$. The algorithm scans $v$ and flags every country whose mass exceeds a tunable attention threshold $\theta$ (defaulting to $1\%$); those go into the _escalation queue_. Every other country keeps its outlets in one monolithic zone—the `TZID` whose polygon covers the largest land area of that nation in the frozen tz‑world shapefile.

Each queued country now needs an internal split across its legal time‑zones. The engine opens `country_zone_alphas.yaml`, reads the concise Dirichlet concentration vector $\alpha$ (computed from rolling two‑year averages of anonymized settlement counts), and rescales it by a global smoothing constant $\tau = 200$ so that the posterior variance matches quarterly production. Because no hidden numbers remain in code, a reviewer can adjust $\tau$ and regenerate the universe, watching dispersion shrink or swell exactly as Dirichlet theory predicts.

With $\alpha$ in hand and an integer outlet count $N_c$ already set, the algorithm draws

$$
s \sim \mathrm{Dirichlet}(\alpha)
$$

on its own Philox sub‑stream (keyed by `(merchant_id, country_iso)`). Multiplying $s$ by $N_c$ yields a real‑valued expected count per TZID. The engine then applies largest‑remainder deterministic rounding so the integers sum exactly to $N_c$ and the relative rounding error is under one outlet. To prevent zeroing out small zones (e.g. `America/Phoenix`), it enforces a _bump rule_: if any zone’s fractional expectation exceeds $0.8$ yet rounding would drop it to zero, the smallest positive share steals one outlet from the largest zone in that country. All arithmetic and tie‑breaks (alphabetical `TZID`) are deterministic once the Dirichlet draw is fixed.

All escalated countries are processed likewise, producing `(country_iso, tzid, outlet_count)` triples appended to a merchant‑scoped Parquet file (sorted by country then zone). That sort order is critical: IEEE 754 mandates deterministic rounding given a fixed sequence. The file’s SHA‑256 digest goes into the global manifest—any tweak to $\alpha$, $\tau$ or $\theta$ changes it, forcing a rebuild downstream.

Zone allocation alone still yields independent store rhythms. To interlock them the generator instantiates a latent _corporate‑day multiplier_ $\gamma_d$ for each merchant on each UTC calendar day $d$ via

$$
\log \gamma_d \sim \mathcal{N}\!\bigl(-\tfrac12\sigma_{\gamma}^2,\;\sigma_{\gamma}^2\bigr),
$$

with variance $\sigma_{\gamma}^2 = 0.15$ (in `routing_day_effect.yml`). Because $\mathbb{E}[\gamma_d]=1$, century‑scale market shares remain untouched, yet hour‑scale counts across zones develop a Pearson correlation of about 0.35—matching real audit logs when Walmart or Zara runs a midnight‑local flash sale. The multiplier is applied multiplicatively to the LGCP mean $\mu$ before sampling, then algebraically removed when weights go to the alias router, so no alias tables need rebuilding.

Micro‑states introduce a final wrinkle: zones like `Europe/San_Marino` could lose all outlets if their country’s total is small. A floor vector $\phi_z$ in `zone_floor.yml` lists minimum integer counts (usually 1–2 outlets globally). During largest‑remainder rounding the algorithm checks any $\phi_z$ violations and atomically reallocates from the largest zone in that country. Since $\sum \phi_z < 0.1\%$ of global volume, this hardly shifts shares but ensures every live TZID in settlement files can appear in the synthetic data.

Everything up to this point is frozen by cryptographic hashes. When the router opens its per‑merchant alias table it also computes a _universe hash_ by concatenating the digests of `country_zone_alphas.yaml`, `routing_day_effect.yml`, `zone_floor.yml`, and the zone‑allocation Parquet. If any piece drifts without regeneration, the router refuses to run and prints mismatched digests for debugging.

Validation closes the loop. After 30 synthetic days the harness bins events by `local_time_offset` and UTC hour for every merchant with outlets in at least three zones. A Hough‑transform scans the heat‑map for a diagonal ridge; if the slope lies outside the physically plausible $-1$ to $-0.5$ offsets‑per‑hour band, the build fails. A second test compares empirical zone shares to integer allocations, flagging deviations beyond two percentage points. Thresholds live in `cross_zone_validation.yml`, so tightening them is one line in a file, not a code change.

By layering a Dirichlet‑driven zone mixture atop the country split, enforcing deterministic rounding with a documented bump rule and zone floors, modulating all sites daily with a corporate multiplier, binding everything with manifest digests, and wiring heat‑map plus share tests into CI, the **“Capturing cross‑zone merchants”** sub‑segment turns each multi‑zone `merchant_id` into exactly the poly‑offset creature acquirer logs reveal—no more, no less—while remaining bit‑for‑bit repeatable.  


### Special treatment for purely virtual merchants
Online‑only merchants pose an existential challenge to a spatially grounded simulator: they settle in a single legal jurisdiction, yet the transaction stream they generate appears to originate from data‑centres, payment gateways and consumer IPs scattered across dozens of time‑zones. The sub‑segment titled **“Special treatment for purely virtual merchants”** inserts a dedicated pathway so that such entities still satisfy our catalogue schema, still interact naturally with the LGCP arrival engine, and still drive downstream fraud features like “impossible travel” or “remote cross‑border spend,” all without inventing brick‑and‑mortar coordinates that never existed.

The branching condition fires the moment the hurdle layer finishes building a merchant row. Each `merchant_id` carries the boolean flag `is_virtual`, derived from the MCC table shipped in `mcc_channel_rules.yaml`; MCCs in the 6000 range (financial services), 5815 (digital streaming) and any merchant with `channel==ONLINE` and `requires_ship_address==False` flip the flag. When the flag is true the normal spatial pipeline diverts into the virtual‑merchant track.

The very first divergence is at outlet creation. The physical‑site generator would have sampled an integer outlet count from a hurdle model: that logic is bypassed and a single **settlement node** is created. Its `site_id` is a SHA‑1 of `(merchant_id,"SETTLEMENT")`. For latitude and longitude the code writes the legal seat’s coordinates as published in the company’s SEC or Companies House filing; the lookup table `virtual_settlement_coords.csv` holds these values and their evidence URLs. That coordinate is used only for settlement reporting and never appears as the origin of customer traffic.

Because fraud features often rely on IP geo‑lookup, the simulator must still generate plausible *customer‑facing* geo information. That requirement is met by instantiating a **CDN‑edge catalogue** per virtual merchant. The catalogue is built once, immediately after the settlement node, by reading the YAML ledger `cdn_country_weights.yaml`. For example, a US‑domiciled streaming service gets edge weights: US 0.55, CA 0.07, DE 0.05, GB 0.05, BR 0.04, and so on for sixty countries. The weights derive from Akamai’s “State of the Internet” report; each country weight is multiplied by a global integer **E=500**, chosen so that no edge catalogue is smaller than 50 nodes or larger than 800, and rounded with the same largest‑remainder routine used elsewhere. Coordinates for these edge nodes are sampled from the population‑density raster of the corresponding country, but they are stored in a separate table `edge_catalogue/<merchant_id>.parquet` whose schema is `(edge_id, country_iso, tzid, lat, lon, edge_weight)`. Edge IDs are deterministic SHA‑1 hashes of `(merchant_id, country_iso, running_index)`.

A key design decision is keeping **two distinct time‑zones for every virtual merchant transaction**. The first, `tzid_operational`, is the local zone of the edge node that ultimately serves the customer. The second, `tzid_settlement`, is the zone of the legal seat—often the head‑office in Delaware or Dublin. When the LGCP arrival engine asks for an event, it supplies the settlement zone’s local clock to the NHPP sampler; that preserves corporate‑day seasonality, because marketing emails still land at midnight headquarters time. Once a candidate local timestamp appears, the router draws a CDN edge using a per‑merchant alias table whose probabilities are the normalised `edge_weight`s. That edge’s zone determines the *apparent* `local_time_offset` written into the `transaction` row, while the UTC conversion uses the settlement offset to produce `event_time_utc`. The split ensures that, for a single virtual merchant on a single UTC day, you can see transactions whose `local_time_offset` jumps from −240 to +1100 minutes even though the mark‑to‑market settlement batch still cuts off at 23:59:59 America/New\_York.

Because the LGCP mean μ is calibrated per site and virtual merchants have hundreds of edge nodes, naïvely injecting a foot‑traffic scalar for every edge would explode state size. Instead the simulator scales μ by the edge weight on‑the‑fly:

$$
\mu_{\text{edge}}(t) = \mu_{\text{settlement}}(t) \times
\frac{\text{edge\_weight}}{\sum_{e} \text{edge\_weight}}.
$$

No persistent state is stored per edge; the alias router converts the uniform random draw directly into an edge selection, multiplies μ accordingly and records the edge’s coordinate. This stateless trick lets the engine route tens of thousands of requests per second without caching edge‑level LGCP parameters.

Virtual merchants still need a geographic anchor for fraud detectors such as “merchant distance from shipping address.” The coordinate of the chosen edge node serves this purpose; it is exposed in the synthetic row as `ip_latitude` and `ip_longitude` rather than `latitude` and `longitude`, both nullable fields added to `transaction_schema` for virtual flows. Downstream feature pipelines that expect location fields simply coalesce physical and virtual columns.

Determinism is preserved via two hashes. First, the `edge_catalogue` parquet is written only once; its SHA‑256 slot `edge_digest_<merchant_id>` joins the dataset manifest. Second, the CDN alias table `<merchant_id>_cdn_alias.npz` embeds the same universe hash that the physical routing files use—concatenating digests of `cdn_country_weights.yaml`, `edge_catalogue` and `theta_digest`—so the router fails fast if anyone tweaks country weights but forgets to rebuild the alias table.

Validation adds two tests specific to virtual merchants. One checks that, over a 30‑day slice, the empirical distribution of `ip_country` codes lies within two percentage points of the YAML weights; another confirms that the settlement cut‑off hour alignment remains perfect by asserting that the UTC timestamp of the final transaction each day is 23:59:59 ± 5 seconds settlement‑zone time. The latter catches clock‑drift bugs where a misapplied offset could push late‑night traffic into the next UTC calendar day.

With a single settlement site anchoring legal reality, a deterministic CDN edge catalogue representing customer geography, dual time‑zones describing operational versus settlement context, a stateless scaling of LGCP intensity that won’t blow up memory, and digest‑based guards that enforce consistency between YAML weights, edge catalogues and alias tables, the “Special treatment for purely virtual merchants” sub‑segment lets online‑only brands inhabit the synthetic ledger as convincingly—and reproducibly—as their brick‑and‑mortar peers.


## Segment 4: "Reproducibility and configurability" → "Validation without bullet points" → <end>
### Reproducibility and configurability
Reproducibility in this pipeline is not a decorative appendix—it is the load‑bearing spine that lets every auditor trace a single row of synthetic data all the way back to the exact bytes of code, configuration and third‑party artefacts that shaped it. The sub‑segment called **“Reproducibility and configurability”** therefore sits at a deeper stratum than the geography or temporal engines: it binds those engines inside a cryptographically signed time‑capsule, while still giving developers the freedom to adjust parameters through governed YAML sheets. What follows is the uncompressed technical anatomy of that capsule, explained in the same vein as the earlier site‑routing narrative—no shorthand, no step omitted.&#x20;

---

The build process launches inside a container whose base image has a **sha256 digest pinned in `Dockerfile.lock`**; the orchestration script reads that digest, the container’s hostname and the UTC start timestamp, and writes the trio as the first three lines of a *live manifest* file in `/tmp`. The very next action is to fingerprint the generator’s source code: `git rev-parse --verify HEAD` yields a forty‑character SHA‑1 tree hash. That hash goes to the fourth line of the manifest; from here on any uncommitted change to the repository would alter the hash and break the reproducibility invariant.

With source code frozen, the script enumerates **every artefact path** declared in `artefact_registry.yaml`. This registry contains a top‑level list of absolute POSIX paths and a `license_map` section pointing each artefact to the full‑text licence file copied into the `LICENSES/` directory. The traversal order is deterministic—lexicographic on path—and each file is streamed into `sha256sum`. The output line `digest  path` is appended to the manifest, but the real purpose is to feed the digest bytes into an **incremental SHA‑256 accumulator**. After the final artefact is processed the accumulator yields a 256‑bit value: the **parameter‑set hash**. That hash is then hex‑encoded and used in three places: (1) as a suffix of the dataset’s root directory name (`synthetic_v1_<hash>`), (2) as the comment string in every Parquet schema (`creator_param_hash=<hash>`), and (3) as a positional argument to the random‑seed generator.

The generator's random numbers come from NumPy’s **Philox 2¹²⁸ + Aes round** counter PRNG. The *master seed* is a 128‑bit integer formed by XOR‑ing the parameter‑set hash (truncated to 128 bits) with a high‑resolution timestamp in nanoseconds. That seed is written into line N of the manifest before any stochastic step occurs. Each module—hurdle site counts, coordinate sampler, Dirichlet splitter, LGCP arrival engine—declares a canonical *stream ID* equal to the SHA‑1 of its fully qualified module name. When a module begins it calls

```python
rng_state = np.random.Philox(master_seed)
rng_state._jump(stream_id)
```

moving its counter forward by `stream_id` blocks; since the counter space is 2¹²⁸, collision is practically impossible. Modules that require many logically separate sub‑streams (e.g. one per merchant) call `rng_state._jump(merchant_id_hash)` inside their loop; each jump is recorded in `rng_trace.log` with the tuple `(module_name, merchant_id, jump_offset)`. If a reviewer wants to reproduce exactly the coordinate draw for merchant 42, they can read `rng_trace.log`, create a Philox RNG seeded with `master_seed`, jump directly to that offset, and start sampling.

Configuration flexibility is channelled exclusively through YAML—there are **no numeric constants** hard‑coded in Python except for physical constants such as 60 seconds per minute. Each YAML begins with a three‑header block:

```yaml
schema: "jp.fraudsim.hurdle_coefficients@2"
version: "3.1.4"
released: "2025-06-30"
```

The `schema` string identifies which JSON Schema the loader must validate against; the integer after the `@` represents the major revision. The build refuses to load a YAML whose schema major exceeds the generator’s expected value, thereby coercing maintainers to upgrade code before ingesting breaking changes. Every YAML entry that represents a statistical estimate carries three additional keys: `mean`, `ci_lower`, `ci_upper`—the 90 % confidence interval harvested from the notebook that fit the model. The validation harness later draws 100 bootstrap replicates per coefficient from a truncated normal anchored at `mean` and clips simulated counts; if the realised synthetic histogram ever falls outside the 95 % predictive envelope, the bootstrap fails and CI marks the coefficient YAML as needing retune.

Before any heavy work starts the build performs a **collision audit**. It queries the internal dataset registry (a Postgres catalog with table `datasets(id, parameter_hash, seed, path)`). If the registry already holds a row with the same `(seed, parameter_hash)` but whose `path` differs from the one the build intends to create, an exception is thrown: someone is attempting to remint a dataset under identical randomness but new semantics. The operator must explicitly increment the semantic version field of at least one YAML file to proceed, ensuring downstream teams notice the change.

As records stream out of the generator they first pass a **structural firewall**. Every row must satisfy: finite latitude or `ip_latitude` depending on `is_virtual`; `tzid` string present in zoneinfo version pinned by manifest; consistency between `event_time_utc` and `local_time_offset` as computed by `zoneinfo.ZoneInfo`; and correct `dst_adjusted` and `fold` flags at DST transitions. The firewall is wired using Pandas vectorised checks; the first row that fails is written to `/tmp/failure_reproducer.py` together with the RNG jump offset that produced it. CI stops immediately and surfaces the reproducer script in its artefacts tab.

Upon structural pass, the **geospatial conformance audit** aggregates the site table by `(merchant_country, tzid)`, derives empirical zone shares, and overlays them on beta‑posterior mean ribbons computed from `country_zone_alphas.yaml`. Wilson score intervals form grey ribbons; the empirical line must stay inside every ribbon with margin 0.1 percentage‑points. Because Dirichlet prior → multinomial posterior → beta marginal is exact conjugacy, the theoretical bound is sharp; a breach means the zone sampler or bump rule drifted.

An **outlet‑count bootstrap** reconstructs the two‑stage hurdle model from `hurdle_coefficients.yaml`. Ten thousand bootstrap replicates of the model produce a predictive envelope for chain‑size histogram; the synthetic counts go on top. If any bucket sits more than one replicate outside 95 % limits, the build fails and plots the offending histogram into the CI artefact bundle.

The **footfall–throughput regression** operates site‑wise. Hourly legitimate transaction counts are regressed on the natural log of the footfall scalar using a Poisson GLM with spline basis for hour‑of‑day and random intercepts for `(merchant_id, date)`. Over‑dispersion parameter θ must land in \[1,2] for card‑present channels and \[2,4] for CNP. If θ misses the band, LGCP variance σ² is flagged via a YAML “tune me” label.

For **temporal multivariate realism** the harness embeds each transaction into ℝ⁶: sine/cosine of hour, sine/cosine of day‑of‑week, latitude and longitude (unit sphere). A 200 000‑row sample—half real, half synthetic—feeds into XGBoost’s binary classifier seeded from a Philox branch. If AUROC exceeds 0.55, the dataset is rejected on the grounds it is too easy to distinguish from reality. Because the XGBoost seed is inside the same RNG tree, rerunning the test with identical seed reproduces the AUROC exactly.

Daylight‑saving fidelity is tested by the **DST edge passer**, which enumerates each site subject to DST, constructs a 48‑hour synthetic wafer around both spring leap and autumn fold for each simulation year, and walks minute‑by‑minute expectations: no missing legal minutes, no present illegal minutes, both folds appear. Failure prints `site_id`, `tzid`, offending minute and a pointer to RNG offset that built the record.

All validation artefacts—raw CSVs, PNGs, GLM coefficient tables—are written under `validation/<parameter_hash>/`. The build’s final act is to upload the manifest JSON, the `validation_passed=true` flag and the artefacts URL to an internal Flask service called **HashGate**. Pull‑requests that ship a new synthetic dataset must include the HashGate URI; the model‑risk board’s GitHub Action polls the URI and refuses merge until the flag reads **true**.

Once the merge occurs, the dataset’s directory—whose name already embeds `parameter_hash`—is mounted read‑only on the shared NFS. Attempting to regenerate the same seed with a different parameter hash yields a directory‑name collision, forcing the developer to bump semantic versions, thereby broadcasting change.

Through parameter‑set hashing, stream‑jump determinism, versioned YAML guarded by schema checks, bootstrap envelopes that keep statistical promises honest, adversarial AUROC tests that probe joint distribution realism, and automation that refuses silent collisions, the *Reproducibility and configurability* layer ensures the synthetic catalogue is never ambiguous, never silently mutating, and always escorted by the forensic paperwork required for JP Morgan’s most hostile audits.

### Reproducibility and configurability
The pipeline’s final safeguard is a validation engine written to behave less like a checklist of verifications and more like an adversary hunting for systematic weaknesses. That philosophy explains the label **“Validation without bullet points.”** Rather than enumerating tests in list form, the engine runs as a streaming narrative of computations whose intermediate artefacts chain together so that a fault in one place almost always cascades into a second, louder failure somewhere else. Because every earlier sub‑segment emits its own manifest fingerprints, the validator never asks “was this YAML loaded?”—it simply recomputes the statistic implied by that YAML and checks numeric identity, guaranteeing that no shadow configuration can slip in unseen.

The first story the validator tells is one of **structural integrity**. It opens each partition of the synthetic Parquet files in deterministic round‑robin sequence and maps every row through four pure functions: coordinate plausibility, time‑zone legality, daylight‑saving consistency and schema completeness. Coordinate plausibility pulls `lat, lon` if the merchant is physical, or `ip_latitude, ip_longitude` if virtual, and feeds them directly into the same tz‑world point‑in‑polygon lookup used during generation; if the returned `TZID` disagrees with the row’s recorded `tzid_operational`, a defect object is pushed into a synchronous defect channel carrying the RNG jump offset that created the row. Time‑zone legality re‑adds `local_time_offset` to the stored UTC epoch and then converts that naive timestamp back through the ZoneInfo database to confirm it lands on exactly the local time originally sampled; any discrepancy indicates that some offset changed upstream. Daylight‑saving consistency checks, minute by minute, that no local time lands inside a spring gap and that duplicates on the fall fold carry the correct `fold` bit. Schema completeness verifies nullable columns obey the merchant’s `is_virtual` flag and that all non‑nullables are finite. If a single row fails any of these checks, the defect channel halts subsequent validation, writes the row and its context into `structural_failure_<hash>.parquet` and raises an exception whose stack trace references the line number in the generator that emitted the row.

Assuming structural integrity passes, the engine shifts to **distributional realism**. It streams the entire dataset through a sliding window of 200 000 rows—half synthetic, half real reference data—embedding each row into a six‑dimensional feature space: sine and cosine of the local hour, sine and cosine of the day‑of‑week, and the two‑dimensional Mercator projection of the latitude‑longitude pair. The window feed drives a single‑round XGBoost classifier whose hyper‑parameters are locked in `validation_conf.yml`. The classifier’s AUROC is evaluated once every million rows; as soon as it exceeds 0.55, the validator short‑circuits, dumps the model dump, the misclassified example indices, and the seed state of the RNG to `/tmp/auroc_failure`, then throws `DistributionDriftDetected`. The power of early termination is that a genuine drift stops the pipeline within minutes instead of producing gigabytes of useless diagnostics. The AUROC threshold is chosen to match JPM’s internal cut‑line: an analyst with perfect knowledge of all features in the data but unaware of the synthetic flag should not beat a coin toss by more than five percentage points.

If adversarial indistinguishability survives, the validator proceeds to **semantic congruence** tests that target very specific relationships promised by upstream modules. From the foot‑traffic catalogue it retrieves each site’s scalar `footfall` and groups hourly legitimate transaction counts accordingly. A Poisson regression with hour‑of‑day spline and merchant‑day random intercepts is run in‑memory using statsmodels; the dispersion estimate θ is compared against the channel‑specific corridor promised by the LGCP variance calibration. A value outside corridor immediately causes the validator to mark `footfall_coefficients.yaml` with a Git attribute `needs_recalibration` and to emit a PDF plot of predicted versus observed counts. Because this check happens after adversarial validation, it is impossible for a variance flaw to hide behind a classifier’s better‑than‑chance performance; both gates must pass.

Next comes the **offset‑barcode interrogation**. The validator bins each merchant’s events on a two‑dimensional plane of UTC hour (0–23) versus `local_time_offset` in minutes. Using a fast Hough transform it extracts the dominant line in accumulator space, converting that line back into a slope measured in offsets per hour. Physics says the Earth rotates fifteen degrees each hour, yielding a slope near −0.75 minutes per minute, or −60 degrees per four time zones in civil notation. If a merchant’s dominant slope falls outside −1 to −0.5, the barcode inspection fails. When it fails, the validator renders a 700 × 300 PNG showing the heat‑map with a red line through the detected slope and stores it under `barcode_failure_<merchant_id>.png`. Triggering this error forces developers to re‑examine time‑zone mixture, corporate‑day multiplier implementation or site routing.

The narrative concludes with **licence concordance**. Every artefact path logged in the manifest has an accompanying licence path; the validator recomputes the SHA‑1 of each licence file, compares it with the digest stamped in five places (manifest, artefact registry, YAML blueprint, citation appendix and HashGate upload). If any mismatch surfaces, the engine fails hard because a content replacement may have occurred without legal review.

When all these intertwined narratives finish without raising an exception, the validator appends `validation_passed=true` to the manifest, increments a global counter in the HashGate REST service and uploads the compressed validation artefacts directory. HashGate responds with a 201 Created, embedding a URI that developers must include in their pull‑request description. GitHub Actions then polls that URI, retrieves the immutable manifest, recomputes its checksum and merges the dataset only if a byte‑for‑byte match is observed. The merge itself executes a final safeguard: a post‑receive hook on the NFS share mounts the dataset directory read‑only and updates the internal registry, preventing future regenerations with the same parameter‑set hash but a newer seed or code revision.

Because the validator speaks in one continuous dialect—assertion flowing into bootstrap flowing into adversarial classifier flowing into physics‑based barcode slope—any breach echoes along the chain, making the defect traceable both in the logs and in the graphic artefacts automatically archived. By refusing to hide behind discrete bullet‑point checks, the validator enforces that each statistical promise made in earlier sub‑segments is not only measured but also inter‑related, ensuring that the final synthetic ledger walks, talks and even stumbles exactly the way real transaction data do when placed under the harsh lens of JP Morgan’s independent model‑risk reviewers.

