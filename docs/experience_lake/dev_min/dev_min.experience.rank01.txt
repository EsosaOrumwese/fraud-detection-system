managed_dev_minimal.experience.rank01.txt

0) HEADER (one screen, recruiter-ready)

Title (recruiter language)
Restored managed end-to-end progress under real failure modes (CI identity + managed messaging + runtime artifact integrity + data access)

One-line claim (what you achieved)
Moved a production-shaped streaming spine into a managed minimal dev environment and restored forward progress by clearing release-plane, messaging-plane, artifact-integrity, and data-access blockers — without bypassing the platform from a laptop or weakening fail-closed gates.

Stack (plain names)
GitHub Actions • AWS (ECR + ECS + S3 + CloudWatch) • Confluent Cloud Kafka

Impact anchors (hard, recruiter-legible)
- Time-to-first-managed-green: 03:08:03 (from first live failure observation → managed semantic-20 PASS publication).
- Release plane recovery: CI took 3 dispatches (2 fail → 1 pass) to reach a publishable runtime artifact.
- Fail-fast prevention: key readiness checks now surface the same failure classes in ~1–2 minutes total (measured combined envelope ~92s), instead of surfacing deep into a run.
- Post-fix credibility: after the first managed semantic-20 pass was restored, the managed semantic-200 lane also closed (overall pass, blockers empty, elapsed 418s).

What this proves (signals, not self-labeling)
- You treat identity/release/runtime/data-plane issues as part of the product, not “setup.”
- You debug by layer and change the correct layer (identity vs wiring vs artifact vs permissions), rather than patching symptoms.
- You close with proof (pass/fail artifacts + closure), and you convert incidents into paved roads (preflight checks + operational rules).


1) 60-SECOND STORY (screen narration)

Situation:
I migrated a local-parity streaming spine into a managed minimal dev environment where the laptop is only an operator console and the platform must run on managed compute with explicit run gating (fail-closed).

Complication:
The first attempt didn’t fail in one place — it failed across layers: the release plane couldn’t reliably publish a trusted artifact, managed messaging failed in multiple ways (wiring + auth + broker/client compatibility), the runtime artifact deployed but crash-looped due to missing runtime dependencies, and a one-shot job failed due to object-store permissions.

Approach:
I held strict constraints throughout (fail-closed, no local bypass, no secret sprawl, immutable artifact identity), classified each failure by layer, made the smallest correct change at that layer, and then added fast readiness checks so the same defect classes are caught early next time.

Result:
Managed semantic green was restored in ~3 hours, followed by a longer managed semantic pass, and the recurring blockers were converted into readiness checks and contracts that fail fast in ~1–2 minutes instead of failing deep into runs.


2) INCIDENT CHAIN (believable, with concrete anchors)

A) RELEASE PLANE BLOCKED (CI couldn’t produce a trusted artifact)
Symptom:
- CI could not reliably publish the runtime image needed for managed execution (build lane stopped before an artifact usable by ECS existed).

Classified as:
- Identity/release boundary (not application logic).

Decision under constraint:
- No “just build locally.” No long-lived admin keys. Fix CI trust, then grant only the minimum permissions required for publishing.

What changed (high-level):
- Fixed CI identity/trust so GitHub Actions could assume the cloud role.
- Granted the missing minimal ECR actions needed for CI to authenticate and push.

Closure proof:
- CI run history shows a clear fail → fix → pass across 3 dispatches.
- After the pass, an immutable artifact identity existed (tag+digest) so the runtime could run “what CI built,” not “what a laptop built.”

Prevention created:
- “Authoritative CI build lane” becomes the only source of runtime artifacts.
- Immutable artifact identity becomes part of run closure (if you can’t name what ran, you can’t claim a green run).


B) MANAGED MESSAGING FAILED (wiring drift → auth mismatch → compatibility)
Symptom:
- First, WSP retries exhausted while pushing to localhost (a classic managed-runtime wiring error).
- Then IG returned 401s (auth boundary rejecting the presented principal).
- After credential reseed, publish still failed due to client/broker negotiation behavior in the managed setup (auth no longer the blocker, but publish still unreliable).

Classified as:
- Managed service viability under runtime identity (wiring + auth + managed broker/client constraints).

Decision under constraint:
- Refuse to bypass Kafka and refuse to weaken ingestion gates.
- Fix in a narrowing sequence: correct wiring → correct auth identity → ensure rotation takes effect → then address remaining managed-compatibility issue with the lowest-risk change that restores reliability.

What changed (high-level):
- Wiring: stopped treating “localhost” as meaningful in managed compute and corrected the endpoint assumptions.
- Auth: replaced placeholder/incorrect principal so the boundary would accept the caller.
- Rotation discipline: treated “reseed” as incomplete until services were redeployed and actually loaded the new values.
- Compatibility: switched to a managed-broker-compatible client path (minimizing blast radius by keeping the adapter boundary stable) once evidence showed auth was no longer the root cause.

Closure proof:
- Messaging-plane readiness now passes under runtime credentials (so the platform proves Kafka usability before starting long runs).
- Post-change smoke confirms the runtime can authenticate and interact with required topics in the managed broker context (not “works once on a laptop”).

Prevention created:
- A fast “Kafka usability” check that fails within ~1 minute if auth/topic/broker access is broken.
- An explicit operational rule: “rotation requires redeploy,” preventing false fixes where credentials changed but runtime didn’t.


C) “DEPLOY SUCCEEDED, RUNTIME CRASHED” (packaging drift)
Symptom:
- After the messaging fix, the environment didn’t become green — services crash-looped with a runtime import failure (missing dependency), even though the image built and deployed.

Classified as:
- Artifact-content mismatch (release artifact is wrong), not “runtime flakiness.”

Decision under constraint:
- Treat packaging as part of correctness.
- Rebuild and republish via the authoritative CI lane to a new immutable artifact identity; do not mark green until the runtime stops crash-looping and the semantic lane proceeds.

What changed (high-level):
- Tightened the container dependency selection so the runtime includes the dependency required by the Kafka client path.
- Republished a corrected image via CI and redeployed.

Closure proof:
- Crash-loop stopped after redeploy.
- The subsequent managed run chain proceeded and the semantic-200 lane closed successfully (overall pass, blockers empty).

Prevention created:
- Packaging contract tightened so “deploy succeeded but runtime broken” is prevented by design (the dependency selection is no longer silently incomplete).
- (Honest note: there isn’t a dedicated CI import-smoke test yet; the prevention is selector discipline + runtime smoke + gate snapshots.)


D) ONE-SHOT JOB FAILED (S3 AccessDenied)
Symptom:
- A managed one-shot lane failed reading required inputs from object storage with AccessDenied, blocking the job from producing its outputs.

Classified as:
- Data-plane permission gap (IAM as part of the product), not job logic.

Decision under constraint:
- Keep lane fail-closed; patch least-privilege permissions for the required object-store prefix; require rerun proof.

What changed (high-level):
- Added the missing object-store permissions for the runtime lane identity covering the oracle input/output prefixes.

Closure proof:
- Logs show the AccessDenied failure at a specific object path.
- After the permission patch, the lane moved past the AccessDenied gate on rerun.

Prevention created:
- Data-plane permissions for lane roles treated as a tested substrate contract (not an implicit assumption).
- Evidence survives teardown, so proof of the failure and the fix remains inspectable.


Timeline anchors (makes this feel real)
- 00:22:57 — first live semantic failure observed (messaging chain surfaced).
- 03:31:00 — semantic-20 managed PASS published (time-to-green 03:08:03).
- 04:56:37 — semantic-200 managed PASS published (elapsed 418s).
- Later: recovery and reproducibility closures also passed (shows this wasn’t “one lucky micro-run”).


3) “PAVED ROAD” UPGRADES (what now exists and how early it triggers)

Fast checks (measured envelope ~92 seconds total):
- Kafka usability check (~73s):
  “Using runtime credentials, can we connect to the broker and confirm required topics exist?”
  Catches: broker/auth/topic wiring failures before a long run begins.

- IG boundary readiness check (~16s):
  “Is ingress alive and stable? Does unauth fail the way we expect? Does auth succeed?”
  Catches: endpoint drift and principal mismatch early.

- Ingest publish/readback readiness check (~3s):
  “Can we admit one message, write a durable receipt, and read it back?”
  Catches: the “it publishes but doesn’t commit/read back” class of failure early.

Operational rules and contracts created:
- “Rotate → redeploy”:
  Rotation isn’t considered complete until runtime reloads it (prevents false fixes).
- “Packaging is a contract”:
  Artifact identity alone isn’t enough; artifact contents must satisfy runtime requirements (prevents crash-loop regressions).


4) PROOF HOOKS (no code dump; what you would point to if challenged)
- A consolidated certification verdict artifact showing overall pass and referencing underlying lane outcomes (best single “proof hook”).
- CI fail→pass timeline showing the release plane unblock and the artifact identity produced.
- Readiness check outputs (pass/fail) for Kafka usability, IG readiness, ingest publish/readback.
- Semantic gate snapshots (semantic-20 and semantic-200) showing closure under gates.


5) SCOPE BOUNDARY (keep credibility)
This entry proves managed-substrate platform/ops ownership: secure CI release discipline, managed messaging viability under runtime identity, artifact integrity closure, data-plane permissions fixes, fail-closed gating, and paved-road prevention.
It does NOT claim full senior MLOps lifecycle closure (model registry promotion/rollback + drift monitoring); those are dev_full / prod-target proofs.