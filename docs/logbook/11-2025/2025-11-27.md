# Logbook
### Date: 27th November 2025
### Project: Fraud Detection System
### Reference commits: Check commits on this date (if any)

* 3:42am
   * Added synthetic S2/S3 implementations to extend 3B:
      - S2 edge catalogue (packages/engine/src/engine/layers/l1/seg_3B/s2_edges/l2/runner.py): loads S0/S1 outputs, merchant universe, and sealed inputs; builds deterministic edge rows (one edge per virtual merchant) and index; writes run-report and segment-state report.
      - S3 alias packaging (packages/engine/src/engine/layers/l1/seg_3B/s3_alias/l2/runner.py): consumes S2 outputs, produces alias blob/index, and a universe-hash JSON; emits run-report and segment-state report.
      - Exported new runners in seg_3B/__init__.py, wired orchestration options in scenario_runner/l1_seg_3B.py and CLI flags --run-s2/--run-s3 in engine.cli.segment3b.
      - Marked TODOs on synthetic datasets; kept S1 test passing.

* 04:50am
   - Country polygons: Natural Earth admin-0 110m GeoJSON downloaded and tracked at `reference/spatial/world_countries/2024-11-27/ne_110m_admin_0_countries.geojson` (canonical for now).
   - Timezones: Timezone Boundary Builder 2024b shapefile+zip under `artefacts/spatial/tz_world/2024b/raw/`; tzdb archive 2024b at `reference/time/tzdb/2024b/tzdata2024b.tar.gz`.
   - IP geo: MaxMind GeoLite2 Country downloaded with provided key to `reference/ip_geo/maxmind_geolite2/20251127/GeoLite2-Country.mmdb` (with COPYRIGHT/LICENSE).
   - Synthetic policies still in place (TODO to replace with governed): `mcc_channel_rules` (synthetic-v0.3), `cdn_country_weights` (synthetic-v0.3 from population), `virtual_settlement_coords` (country centroids), `virtual_validation` (synthetic tolerances).
   - Possible to acquire next: Cloudflare Radar country-traffic snapshot (to seed `web_traffic_by_country`), HRSL rasters (need scope), Overture Places POI base (need release/filter).
   - Not externally obtainable: governed MCC policy, real settlement coords/tz/evidence, governed validation policy, edge RNG/alias layout policies and per-merchant overrides.

* 05:10am
   - Downloaded Cloudflare Radar HTTP traffic snapshot (30d, top locations) using provided API token; saved to `reference/web_traffic/cloudflare_radar/20251127/traffic_by_country.{json,csv}`. Updated dataset dictionary entry `web_traffic_by_country` with version `radar_http_top_locations_30d` and snapshot path.

* 07:20am
   - Adopted lightweight HRSL strategy: treat HRSL as build-time only, aggregate to tile-level weights, discard raw rasters. Added `tile_population_weights` dataset entry (3B) pointing to a compact CSV under `reference/spatial/tile_weights/from_hrsl/{source_vintage}/tile_population_weights.csv`; marked legacy `hrsl_raster` as deprecated/build-time external.

* 07:40am
   - Removed the sample country scope file (`config/engine_countries.yaml`) to avoid implying any footprint constraint. Registered the lightweight global population raster (`artefacts/spatial/population_raster/2025/raw/global_2020_1km_UNadj_uncounstrained.tif`) as `global_population_raster` in the 3B dictionary and sealed-asset list. TODO: add the aggregation step to produce `tile_population_weights` from the global raster + 1B tile_index, then discard temp rasters.

* 07:55am
   - Added `scripts/compute_tile_population_weights.py` to derive per-tile population weights from the lightweight global raster and the 1B tile_index; writes `reference/spatial/tile_weights/from_hrsl/{source_vintage}/tile_population_weights.csv` and treats the raster as build-time input only.

* 09:31am
   - Replaced synthetic CDN weights with governed weights derived from Cloudflare Radar snapshot (`governed-radar-20251127`) in `config/virtual/cdn_country_weights.yaml`; edge_scale left at 500.
   - Replaced synthetic MCC×channel ladder with governed heuristics (`version: governed-v1`) in `config/virtual/mcc_channel_rules.yaml`, defaulting to physical except explicitly virtual-heavy CNP MCCs.

* 09:45am
   - Regenerated `artefacts/virtual/virtual_settlement_coords.csv` as synthetic-but-documented coordinates: loaded merchant universe (`transaction_schema_merchant_ids`), mapped ISO2→ISO3, used Natural Earth admin-0 polygons to take a representative point per merchant country with 5% bbox jitter (fallback to centroid if jitter escapes), fixed RNG seed for determinism, overlaid the points on Timezone Boundary Builder 2024b polygons to populate `tzid_settlement`, left `evidence_url` blank, and marked `coord_source=synthetic-jittered-centroid` with notes explaining the derivation.

* 10:52am  **TODO - Merchant realism roadmap (documented for later execution)**
   - **Problem statement / current state**
       * `transaction_schema_merchant_ids`, `mcc_channel_rules`, `virtual_settlement_coords`, currency surfaces, etc. are entirely synthetic. They obey deterministic generation logic but are not anchored to any real-world brands, HQ coordinates, or channel mix data. As a result, SHAP/model explainers would reflect our synthetic assumptions rather than realistic distributions. We need a clear plan to swap these inputs for evidence-based equivalents once we restructure.
   - **Long-term remediation plan**
       1. **Source hierarchy** (highest trust first):
            - **GLEIF Golden Copy (LEI data)** – provides legal entity identifiers plus HQ and legal address coordinates. Links: [overview](https://www.gleif.org/en/lei-data/gleif-golden-copy), [downloads](https://www.gleif.org/en/lei-data/gleif-golden-copy/download-the-golden-copy), [concatenated files](https://www.gleif.org/en/lei-data/gleif-concatenated-files). Use LEI IDs to populate `merchant_id`→HQ lat/lon when available (`evidence_code=gleif_hq`, `source_id=LEI`).
            - **Wikidata** – query `P159` (headquarters) and `P625` (coordinates) via SPARQL for notable brands using the [Wikidata Query Service](https://query.wikidata.org) and the [headquarters property](https://www.wikidata.org/wiki/Property:P159). Use when no LEI exists (`evidence_code=wikidata_hq`, `source_id=Wikidata QID`).
            - **Overture Maps – Places theme** – download GeoParquet releases from [https://docs.overturemaps.org/guides/releases/](https://docs.overturemaps.org/guides/releases/) / [https://docs.overturemaps.org/guides/places/](https://docs.overturemaps.org/guides/places/) (e.g. `s3://overturemaps-us-west-2/release/2025-11-19.0/theme=places/type=place/`), filter to corporate/retail/office categories, and map by brand name + ISO country. Provides exact POI lat/lon (`evidence_code=overture_poi`, `source_id=place_id`).
            - **Foursquare OS Places** – open POI feed with `fsq_id`, brand metadata, and coordinates (see [landing](https://opensource.foursquare.com/os-places/), [schema](https://docs.foursquare.com/data-products/docs/places-os-data-schema), [access guide](https://docs.foursquare.com/data-products/docs/access-fsq-os-places)). Use for additional coverage or cross-validation (`evidence_code=fsq_os_place`, `source_id=fsq_id`).
            - **Government/company registries or BigPicture / SavvyIQ** – legal addresses to geocode when none of the above yield results (e.g. [Companies House](https://www.gov.uk/government/organisations/companies-house), [SEC EDGAR](https://www.sec.gov/edgar/search/), [BigPicture free dataset](https://docs.bigpicture.io/docs/free-datasets/companies)). (`evidence_code=geocoded_registry`).
            - **Synthetic fallback** – only if all sources miss: capital city or centroid with explicit `evidence_code=synthetic_capital` / `synthetic_centroid`.
       2. **Matching strategy**
            - Extend the merchant reference to include brand/name aliases (or maintain a lookup table). For each merchant, attempt matches in order: LEI → Wikidata → Overture → Foursquare → registry geocode.
            - Use deterministic matching rules (name + country, brand IDs) and record confidence scores. Prefer highest-trust evidence; fall back to lower-trust only when necessary.
       3. **Data pipeline updates**
            - Build ingestion scripts for each source (download, subset, normalize fields, compute tz via TBB polygons).
            - Produce a new `merchant_brand_profile` artefact capturing: merchant_id, brand name, MCC, channel, HQ lat/lon, tz, evidence_code, source IDs, and any metrics (e.g., number of POIs, traffic share).
            - Regenerate `virtual_settlement_coords` from the profile instead of country centroids.
            - Update MCC/channel distributions based on observed counts in the real sources (or policy heuristics derived from them).
            - Seal all governed artefacts via 3B S0 once the pipeline stabilizes.
       4. **Documentation & tooling**
            - Maintain provenance fields (`evidence_code`, `source_id`, `evidence_url`) so we can audit or replace entries later.
            - Keep a coverage report (percentage of merchants by evidence tier) so we know how much remains synthetic.
            - Clearly mark synthetic fallbacks in the dictionary and logbook until they are replaced.
   - This plan does **not** change current synthetic behaviour; it captures the agreed roadmap so future restructuring can swap synthetic inputs for evidence-backed ones without ambiguity.

* 11:00am
   - Updated `config/virtual/virtual_validation.yml` to a governed policy (`version: governed-v1`) with `ip_country_tolerance=0.02` and `cutoff_tolerance_seconds=5`, aligning tolerances to the spec’s ±2 pp IP-country envelope and ±5 s settlement window so S4/S5 can seal the pack.

* 11:29am
   - Updated the Makefile to add an `all` target that runs 1A → 2B → 3A → 3B (with 3B running S0-S3 by default, controlled via SEG3B_RUN_S* flags).

* 12:09pm
   - Fixed Segment3A S0 validation-bundle discovery: `_verify_upstream_bundles` now resolves either the root validation directory or its `/bundle` child before loading `index.json`, and the Makefile's `segment3a/3b` targets now point `VALIDATION_BUNDLE_2A` at the actual 2A validation directory (no synthetic `/bundle` suffix). This prevents `E_INDEX_MISSING` when 2A writes its bundle at the root.

* 12:15pm
   - Relaxed the 2A bundle loader so `index.json` may contain either `artifacts` or `files` arrays (2A writes the latter). Also taught the Makefile to read the 2A manifest fingerprint from `segment2a_result.json` before passing the validation bundle path, ensuring reruns point at the correct fingerprint when 1A≠2A.
