# Logbook
### Date: 27th July 2025
### Project: Fraud Detection System
### Issues Resolved: [SD-02](https://github.com/EsosaOrumwese/fraud-detection-system/issues/25) `in-progress`
### Reference commits: Check commits on this date (if any)

* 12:11am
  - Remove standalone `nb_mean_coefficients.yaml` references from `parameter_spec_1A.txt`
  - Extend `config/hurdle_coefficients.yaml` description to include both β (hurdle) and α (NB-mean) links
  - Update `ComputeNBParameters` `interface_contract_spec_1A.txt` to indicate `parsed_hurdle_coefficients` holds both coefficient sets
  - Amend `governing_artefacts_1A.txt`:
    • Note that `hurdle_coefficients.yaml` now contains NB-mean coefficients
    • Rename `stationarity_diagnostics.parquet` to `artefacts/diagnostics/hurdle_stationarity_tests_2024Q4.parquet`
  - Update `version_matrix_1A.toml.txt` to reflect the new path and version_tag for `hurdle_stationarity_tests`

* 1:03am
  * Align 1B specs with narrative: enrich site catalogue schema, propagate IDs/digests, and enforce jitter-off default
    - `interface_contract_spec_1B.txt`:
      - Propagate `merchant_id`, `site_id`, and `spatial_manifest_digest` through `AssignTimeZone` and `ComputeFootTraffic` stages
      - Expand `WriteSiteParquet` output schema to include `latitude`, `longitude`, `prior_tag`, `prior_weight_raw`, `prior_weight_norm`, `artefact_digest`, `spatial_manifest_digest`, `log_footfall_preclip`, `footfall_preclip`, `footfall_clipped`, `tzid`, `pixel_index`, `feature_index`, `d_haversine_km`, `d_road_km`
    - `parameter_spec_1B.txt`:
      - Ensure `jitter_enabled` has `default_value: false` and document that any override requires a semantic-version bump
    
* 1:33am
  * 2A spec alignment: correct TZ-path names, enrich audit schema, add site-catalogue writer
    * BuildSTRTreeIndex
      - Path pattern for `tz_index_digest` updated to `artefacts/priors/tz_world/2025a/tz_index_digest.txt`
    * DeriveTimeZone
      - `tz_assignment_audit` schema extended with `nudge_lat`, `nudge_lon`, `override_scope`, `error_type`
    * ApplyTimeZoneOverrides
      - Output renamed to `ci_override_validation.log`
      - Added `diff_count` field to schema
    * ExtractZoneTimeline
      - Cache-size record path changed to `output/tz_cache_bytes.txt`
      - Stored as `int` (byte count) not binary blob
    * Added new stage WriteUpdatedSiteCatalogue
      - Combines `site_coordinates` + audit logs
      - Writes final site catalogue with TZID, nudge coords, dst_adjusted, fold, spatial_manifest_digest

* 2:32am
  * Align 2B specs with narrative: fix catalogue and routing paths, enforce pweights binary format, add manifest and buffer stages, and enrich audit/validation schemas
    * `interface_contract_spec_2B.txt`:
      - `ComputeProbabilityVector` reads from `artefacts/catalogue/site_catalogue.parquet`
      - `merchant_pweights.bin` and `merchant_alias.npz` written at repo root with headerless little-endian `float64` binary schema and digest-based `SUCCESS_METRIC`
      - `RouteTransaction` `routing_audit_log` schema extended with `event_type`, `routing_manifest_digest`, `ip_country_code`, `error_type`, `error_details`
      - Rename validation stage to `routing_validation_log` at `logs/routing/validation.log`
      - Add `WriteRoutingManifest` stage emitting `artefacts/routing/routing_manifest.json`
      - Add `WriteOutputBuffer` stage writing `output/buffer/…` parquet with `gamma` and `ip_country_code` fields
    * `governing_artefacts_2B.txt`:
      - Declare `routing_validation_log` and `output_buffer` artefacts in the registry
    * `version_matrix_2B.toml.txt`:
      - Add stanzas for `routing_audit_log`, `validation.log`, and `output_buffer` with draft version tags and placeholder digests

* 3:10am
  * Align 3A specs with narrative: add missing artefact inputs, expand schemas, compute universe hash, and add gamma & validation stages
    * `interface_contract_spec_3A.txt`:
      - `FlagEscalationCountries`: added `zone_mixture_policy.yml` input, updated path to `artefacts/catalogue`, tightened `escalation_match_theta` metric
      - `ApplyDeterministicRounding`: added `rounding_spec` input (`round_ints.md`), updated paths to `artefacts/allocation`, replaced `row_count` with `rounding_integrity` metric
      - `ApplyBumpRule` & `EnforceZoneFloors`: updated paths to `artefacts/allocation`, added `residual` and `floor_applied` fields, tightened `bump_invariant` & `floor_invariant` metrics
      - `WriteZoneAllocParquet`: expanded schema with expectation, residual, rounded_count, bump_applied, adjusted_count, is_major_zone, universe_hash; replaced file_existence with universe_hash_match metric
      - Added `ComputeUniverseHash` stage to generate `universe_hash_digest.txt` with digest length check
      - Added `ComputeCorporateDayEffect` stage reading r`outing_day_effect.yml` & `rng_policy.yml`, writing `gamma_day_effect` parquet with variance check
      - Added `CrossZoneValidation` stage producing `barcode_slope_validation.log` and `share_convergence.log` with `validation_pass` metric
    * `governing_artefacts_3A.txt`:
      - Declared `universe_hash_digest`, `gamma_day_effect`, `barcode_slope_validation`, `share_convergence_log` artefacts
    * `version_matrix_3A.toml.txt`:
      - Added stanzas for `universe_hash_digest`, `gamma_day_effect`, `barcode_slope_validation`, and `share_convergence_log` with draft version tags

* 10:00pm
  * Spent the whole day trying to understand how to fit this large context of Layer 1 into the model/chat when fleshing out the design for Layer 2
    * I thought of and researched how to implement RAGs but they just seemed to complicated, at the moment, to implement seeing that it just doesn't make sense implementing a complex system to implement another complex system
    * Got the idea of using either OpenAI playground or Open WebUI which allow me to store the 9*8 files I have for Layer and continue building
    * But I think I went of the wrong tangent in tackling this by forgeting one thing. I am not "building" layer 2 with all these files, rather I am planning and designing layer 2.
    * Moreover, although all the information is relevant for Layer 1, not all is neccesary for building it or even planning out Layer 2 and the rest.
    * I need to find out what layer 2 needs for me to carry out planning successfully such that it rightly understands what Layer 1 has made available for it to use without making up its own
    * It was suggested I make a root folder `artefacts.yml` or so to contain the `Input Artefacts` section of the `Interface Contracts` but  for the whole layer 1. That way, any file or artefact that Layer 2 needs, it can crosscheck and reference it instead of parsing through the whole 9*8 files to see what's relevant to it.
    * This makes the use of Open AI playground or Open WebUI overkill or even not neccesary for this as I need the `o3` model for planning and that costs a lot in OpenAI's api

