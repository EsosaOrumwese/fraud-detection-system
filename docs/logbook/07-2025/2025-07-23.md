# Logbook
### Date: 23rd July 2025
### Project: Fraud Detection System
### Issues Resolved: [SD-02](https://github.com/EsosaOrumwese/fraud-detection-system/issues/25) `in-progress`
### Reference commits: Check commits on this date (if any)

* 6:17am
  * Consolidate reproducibility & configurability (subsegment 4a); close holes in manifest, RNG, CI, and validation harness
  * Summary:
    - Expanded `license_map` semantics with explicit artifact→license mapping and `validate_licences.py` enforcement.
    - Specified live manifest file path (`/tmp/build.manifest`) and exact line structure.
    - Clarified parameter‑set hash computation, dataset_root naming, and master_seed left‑shift XOR logic.
    - Formalized Philox 2¹²⁸ PRNG sub‑streams, `_jump` protocol, and comprehensive `rng_trace.log` events.
    - Inserted structural firewall checks (five vectorized validations) with reproducer script and CI abort.
    - Added geospatial conformance audit via beta‑posterior intervals and CI gating.
    - Introduced outlet‑count bootstrap procedure with 10 000 replicates, histogram envelopes, PNG diagnostics, and merge blocking.
    - Defined footfall–throughput Poisson‑GLM regression, dispersion thresholds, and CI flagging.
    - Documented multivariate indistinguishability test (XGBoost AUROC≤0.55) with fixed seeding.
    - Enumerated DST edge‑passer minute‑by‑minute validation around spring/fall transitions.
    - Integrated HashGate upload (`validation_passed=true`) and read‑only NFS dataset immutability.
    - Enforced artifact registry ordering, digest streaming, and Postgres collision‐prevention on `(parameter_hash, seed)`.
  * Breaking Changes:
    - `build.manifest` path and format updated; scripts must read `/tmp/build.manifest`.
    - New `rng_trace.log` event types and fields; audit parsers require update.
    - CI workflows now depend on new validation scripts: `firewall.py`, `geo_audit.py`, `bootstrap_validator.py`, `footfall_glm_validator.py`, `xgb_validator.py`, `dst_validator.py`, `upload_to_hashgate.py`, `validate_licences.py`.
    - Dataset directory naming embeds `parameter_hash`; old datasets will not mount under the new convention.

* 6:51am
  * Fixed some errors in 4A files. Specify manifest writers, artifact loader, and manifest structure.
  * Summary:
    - Named `pipeline_launcher.sh` as the orchestration script writing `/tmp/build.manifest`.
    - Named `artefact_loader.py` for enumeration of `artefact_registry.yaml` entries.
    - Fixed master seed placement to line 5 of the manifest in both narrative and assumptions.
    - Named `upload_to_hashgate.py` as the HashGate upload script.
    - Added Parquet `creator_param_hash` comment insertion and RNG constructor usage to assumptions.

* 5:20pm
  * Specify validation file names, upload protocol, and AUROC cadence
  * Summary:
    - Updated structural failure output to `structural_failure_<parameter_hash>.parquet` and raise `StructuralValidationError`.
    - Constrained licence validation to `validate_licences.py` against manifest’s `licence_digests` with `LicenceMismatchError`.
    - Defined HashGate upload: hash `validation/<parameter_hash>/`, POST to `/hashgate/<parameter_hash>/<master_seed>`, and log HTTP status.
    - Standardized PDF output as `glm_theta_violation.pdf` in `validation/<parameter_hash>/`.
    - Added `auroc_interval` field in `validation_conf.yml` for AUROC evaluation every 1,000,000 rows and `AurocThresholdExceeded` exception.

* 5:49pm
  * Run 1A-4B through Hole Finder and Fixer one more time. But this time, use updated files for context.
  * Aim:
    * Ensure specificity in the descriptions such that no assumptions will be left to the LLM when crafting specifications for each subsegment from the docs
    * This involves no holes in narrative and assumptions txt files
    * Governing artefacts and Maths Summary is also without holes and
    * The interface snapshot is comprehensive enough to capture the relationship between each layer.

* 9:02pm
  * Done with this merchant-location layer of the data engine but I need to make sure it is robust enough to begin crafting code from or does it need extra files to accompany the narrative and assumptions.

* 10:18pm
  * Here is the advice for the day. As I want to iron out Narrative prose -> Code generatable specification
    * Don’t build more narrative for its own sake. Build catalogue tables per layer. Fill each row deterministically. When the “hole rate” drops to zero, ship the code. 
    * This is as safe and error-proof as it gets—especially for solo work, since it prevents “forgotten unknowns.” 
    * No sycophancy, just fact: this approach, followed honestly, will produce a robust, reproducible, and auditable data generator. If you cut corners on the catalogue/fill process, you risk (1) silent spec leaks, (2) hidden performance cliffs, (3) audit hell, and (4) lost months in debugging.