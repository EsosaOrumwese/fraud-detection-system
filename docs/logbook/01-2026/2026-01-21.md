# Logbook
### Date: 21st January 2026
### Project: Fraud Detection System
### Reference commits: Check commits on this date (if any)

* 3:15am
  - Planned S4 shared-memory routing maps (memory-mapped NumPy arrays) to avoid
    per-worker dict duplication and `BrokenProcessPool` crashes.
  - Logged detailed plan in
    `docs/model_spec/data-engine/implementation_maps/segment_5B.impl_actual.md`
    (Entry: 2026-01-21 03:15).

* 3:42am
  - Appended a detailed plan to incrementally load year-scoped `s4_group_weights`
    into `group_weights_map` so S4 avoids full-memory loads and fixes the
    missing map regression. (Implementation map entry: 2026-01-21 03:41)

* 3:44am
  - Implemented year-scoped `s4_group_weights` loading for 5B.S4 so the map is
    built lazily per scenario and skipped entirely when the time-grid years don’t
    overlap. This prevents sending a large map to workers when unused and fixes
    the missing-map regression.

* 3:46am
  - Ran `make segment5b-s4`; run failed with `5B.S4.IO_WRITE_FAILED` and
    `BrokenProcessPool` (child process terminated abruptly). Run report:
    `runs/local_full_run-5/d61f08e2e45ef1bc28884034de4c1b68/reports/layer2/segment_state_runs/segment=5B/utc_day=2026-01-21/segment_state_runs.jsonl`.
  - Reduced S4 defaults in `makefile` (workers=8, inflight=8, output_buffer_rows=20000)
    to cut per-worker memory pressure before re-running.

* 3:55am
  - Planned a diagnostic wrapper for `_process_s4_batch` to capture worker
    exceptions and surface them in the parent process (to distinguish Python
    errors from native/OOM crashes). Logged plan in implementation map
    (Entry: 2026-01-21 03:54).

* 3:58am
  - Added `_process_s4_batch` wrapper to catch worker exceptions and return a
    structured error payload, plus parent-side handling to abort with context.
  - Imported `traceback` for worker error capture.

* 3:59am
  - Re-ran `make segment5b-s4` with updated defaults (workers=8); still failed
    with `5B.S4.IO_WRITE_FAILED` / BrokenProcessPool.
  - Ran `ENGINE_5B_S4_WORKERS=1` to force serial mode; hit
    `5B.S4.DOMAIN_ALIGN_FAILED` (bucket_missing, bucket_index=0).

* 4:05am
  - Restored `runner.py` to the last committed state after an indentation tool
    corrupted the file (syntax errors). Re-applied the worker error wrapper and
    incremental group-weights loading changes.

* 4:07am
  - Started `make segment5b-s4` with workers=8; after ~52s ETA was ~12–13
    minutes (above target). Terminated the running `s4_arrival_events_5b`
    python processes to avoid a long run.

* 4:10am
  - Tested workers=12/inflight=12; early ETA looked low but later stabilized at
    ~11–13 minutes. Killed the running `s4_arrival_events_5b` processes once ETA
    exceeded target.

* 4:17am
  - Reintroduced shared-memory lookup maps in `s4_arrival_events` (shared_maps
    build/load + worker usage) to cut per-event dict lookups and improve speed.

* 4:19am
  - Ran `make segment5b-s4` with shared maps enabled (workers=8). The run started
    fast but the PowerShell host hit an out-of-memory error; terminated the
    `s4_arrival_events_5b` python processes to recover.

* 4:35am
  - Confirmed no lingering `s4_arrival_events_5b` worker processes after the restart.
  - Logged a new S4-safe-defaults plan in `docs/model_spec/data-engine/implementation_maps/segment_5B.impl_actual.md` (Entry: 2026-01-21 04:34) to reduce workers/inflight/output buffer and re-test.

* 4:36am
  - Reduced 5B.S4 defaults in `makefile` (workers=4, inflight=4, output_buffer_rows=5000) to lower peak RAM before re-running S4 with shared maps.

* 4:38am
  - Ran `make segment5b-s4` with shared maps + new defaults (workers=4, inflight=4, output_buffer_rows=5000). Early ETA looked promising but after ~145s throughput collapsed (bucket ETA ~70+ min, arrivals ETA hours). Terminated the running `s4_arrival_events_5b` python processes to avoid a long run.

* 4:40am
  - Planned an S4 override test (workers=6, inflight=6, output_buffer_rows=10000) with shared maps to recover throughput without the 8/8/20000 OOM risk; logged plan in the implementation map (Entry: 2026-01-21 04:40).

* 4:42am
  - Ran S4 with overrides (workers=6, inflight=6, output_buffer_rows=10000). After ~115s the rates collapsed (ETA ~36 min+). Terminated the `s4_arrival_events_5b` processes to avoid a long run.

* 4:45am
  - Planned S4 worker optimizations to cut per-arrival overhead (skip bucket sorting when ordering checks are off, cache RNG prefix hashers, cache tzid index per worker). Logged full plan in the implementation map (Entry: 2026-01-21 04:44).

* 4:49am
  - Implemented S4 optimizations: cached RNG prefix hashers per worker, cached tzid index, and skipped per-bucket sorting when ordering stats are disabled (default). Prepared to re-run S4 to validate ETA and memory.

* 4:53am
  - Ran S4 after worker optimizations (workers=6, inflight=6, output_buffer_rows=10000). Throughput still collapsed after ~2 minutes (ETA ~38 min). Terminated the running `s4_arrival_events_5b` processes.

* 4:57am
  - Corrective note: added tuple-based event buffering in the S4 worker path (plus tuple-aware validation in `_write_events`) to reduce per-event dict overhead. Logged the corrective plan in the implementation map (Entry: 2026-01-21 04:56).

* 4:58am
  - Planned a validation run for the tuple-buffer change (workers=6, inflight=6, output_buffer_rows=10000) and will terminate early if ETA exceeds target.

* 5:00am
  - S4 tuple-buffer run failed schema validation (None values for optional fields). Adjusted tuple validation to omit None-valued keys to match previous dict-row behavior. Preparing to re-run.

* 5:02am
  - Re-ran S4 with tuple buffering (workers=6, inflight=6, output_buffer_rows=10000). Validation fix worked, but sustained ETA was still ~32 min (55k arrivals/sec). Terminated the run. Noted Polars DataOrientationWarning for tuple rows.

* 5:03am
  - Planned to add `orient="row"` for tuple DataFrame construction and test a larger output buffer (50000 rows) to reduce write overhead.

* 5:05am
  - Added `orient="row"` when building Polars DataFrames from tuple buffers to remove row-orientation warnings and reduce inference overhead.

* 5:05am
  - Ran S4 with tuple buffer + orient="row" and output_buffer_rows=50000 (workers=6). Sustained rate improved (~98k arrivals/sec) but ETA stayed ~18 min; terminated run.

* 5:06am
  - Planned a higher-parallelism test for S4 (workers=12, inflight=12, output_buffer_rows=50000) to see if throughput scales to the target window.

* 5:08am
  - Tested workers=12/inflight=12/output_buffer_rows=50000. Throughput improved (~118k arrivals/sec) but ETA still ~14-15 min; terminated run.

* 5:12am
  - Logged a detailed plan for the S4 compiled-kernel refactor (Numba-based SHA256 + Philox + routing fast path with fallback) in `docs/model_spec/data-engine/implementation_maps/segment_5B.impl_actual.md`.

* 5:13am
  - Added `numba` to `pyproject.toml` dependencies to support the compiled S4 kernel path.

* 5:16am
  - Moved the new `numba_kernel.py` into `packages/engine/src/engine/layers/l2/seg_5B/s4_arrival_events/` (initial path missed the `layers` segment).

* 5:24am
  - Implemented a Numba kernel module for S4 (SHA256+Philox+routing) and integrated a compiled-kernel fast path in `_process_s4_batch_impl` guarded by `ENGINE_5B_S4_COMPILED_KERNEL` and shared-maps availability.
  - Added shared-map edge alias arrays and per-worker tz cache/bytes preparation for the compiled path.

* 5:26am
  - Checked `numba_kernel` import with PYTHONPATH set; numba is not installed (`numba_available=False`), so compiled path will stay off until dependency is installed.

* 5:27am
  - Added `ENGINE_5B_S4_COMPILED_KERNEL` default and wiring in `makefile` so the compiled S4 path can be toggled via make.

* 5:28am
  - Verified `engine.layers.l2.seg_5B.s4_arrival_events.runner` imports cleanly with PYTHONPATH set; compiled-kernel module does not break imports even without numba installed.
