# Logbook
### Date: 25th October 2025
### Project: Fraud Detection System
### Reference commits: Check commits on this date (if any)

* 4:37am:
   - Reworked the hurdle-training helpers so the dispersion head now carries explicit merchant/channel/GDP metadata and enforces the corridor in training: packages/engine/src/engine/training/hurdle/design.py:21-133 gains the cached dispersion features, while packages/engine/src/engine/training/hurdle/fit.py:5-245 rebuilt the dispersion fit to (i) aggregate counts into MCC×channel×ln GDP bins, (ii) clip φ into [5e‑3, 5], and (iii) fall back deterministically if bins thin out.
   - Exported a fresh dispersion bundle under config/models/hurdle/exports/version=2025-10-24/20251024T234923Z/nb_dispersion_coefficients.yaml:1, leaving the sealed 2025‑10‑09 hurdle coefficients in place; the runbook now calls out both paths explicitly (docs/runbooks/segment1a_1b_execution.md:21-35).
   - Segment 1A smoke runs now execute through S2 without the earlier ERR_S2_NUMERIC_INVALID, but they fail the corridor validator: with the sealed hurdle coefficients plus the new dispersion file, the latest attempt produced parameter_hash=530943e1c3d6174db18c1b722c79866b9447787c3e4bef1247ccdfade28a9e7f, manifest_fingerprint=bb25220a187b89e000b87b9fe2ef135bf84f9870c1fe14f9a4475519d5afe374 and the validator reported rho_reject=0.2494, p99=3, cusum_max=10.72. (Using the freshly trained hurdle coefficients pushed rho even higher at 0.253.) So the numeric floor is fixed, but NB mean/dispersion still need tuning to bring rejections below the 6 % policy.

* 5:30am:
   - Derived the multi-merchant `mu`/`phi` vectors from the `runs/local_layer1_regen2` design parquet and solved for the intercept shift needed to hit the corridor target (`rho≈0.06`). This yielded a +0.749 log-mean bump (intercept ≈ 1.508) for `config/models/hurdle/exports/version=2025-10-24/20251024T234923Z/hurdle_coefficients.yaml`.
   - Re-ran Segment 1A with the 2025‑10‑24 coefficient pair (`runs/local_layer1_regen3`, `parameter_hash=4022858cfa02091ccd7ece5ea88586b5e3c2e24e6903188e56bf8083eba1d4f6`). Result: `rho_reject=0.0582`, `p99=1`, but the CUSUM gate still tripped at `Smax=15.17` (bursty single-rejection streaks despite the improved mean).
   - Tried an even larger intercept shift (+1.0 total, `runs/local_layer1_regen4`) to see if near-zero rejections would drop the CUSUM. Outcome: `rho_reject=0.0340`, `p99=1`, but `Smax` actually worsened (21.88) because rare single rejections now produce z-scores>5. Restored the intercept to the +0.749 setting and concluded that a uniform intercept tweak is insufficient; need MCC/channel-specific adjustments (or revised φ corridors) to break the rejection clusters that drive the CUSUM breach.

* 7:08am:
   - Computed MCC×channel-specific acceptance deltas from the regen2 design snapshot (solving per-segment α≈0.94), solved the resulting linear system to isolate MCC offsets plus a CP vs CNP channel delta, and wrote those shifts into `config/models/hurdle/exports/version=2025-10-24/20251024T234923Z/hurdle_coefficients.yaml` (intercept remains +0.749).
   - Segment 1A smoke (`runs/local_layer1_regen5`, `parameter_hash=8d84f5ecd38ad0c89727b81ec1c9ef74f27b916d53c609d703bdac8ebe5cf070`, `manifest_fingerprint=da910b598008f10f6fad39d43b4f49425b0a36243a9c3f5af564afe731762f70`) still failed the corridor validator: `rho_reject=0.0650`, `p99=1`, `cusum_max=20.27`. The NB mean adjustments pulled `rho` close to spec, but the admitted merchant mix changed (4 386 multis vs 4 264 previously), so the per-segment deltas drifted. Need another iteration (recompute deltas off the new deterministic context) or consider shaping φ to curb the CUSUM spikes rather than relying solely on β_μ.

* 7:57am:
   - Recomputed the MCC×channel deltas using the regen5 deterministic context and rewrote the governed `beta_mu` accordingly (still anchored at +0.749 intercept).
   - Segment 1A smoke (`runs/local_layer1_regen6`, `parameter_hash=22f2549488f011816bb664a93862a19c95dee08bedb046f8671eca5c4d03d524`, `manifest_fingerprint=decffc96e71410666cc6c24d55b4b858e796b3ac9a31509a9031b951731651b2`) finally hit `rho_reject=0.0591` with `p99=1`, but CUSUM remained above the gate (`Smax=11.49`). The remaining breach is now mainly driven by sparse MCCs at the tail, so the next iteration needs either MCC-specific φ boosts or softer α targets for those segments.

* 9:19am:
   - Scaled the NB-dispersion coefficients per MCC×channel (solving for α≈0.96) and rewrote config/models/hurdle/exports/version=2025-10-24/20251024T234923Z/nb_dispersion_coefficients.yaml. Reran Segment 1A (runs/local_layer1_regen7, parameter_hash=ebd68f4b2c30cc61b1da08569ed352aa4d8c8d204aa820e219dbbddd7da40d67, manifest_fingerprint=e554a7174e31bd5a428d0ee85ede88402f8ce735c034e832c296977887fa9c08). The dispersion uplift backfired: rho_reject=0.466, p99=6, cusum_max=15.6. S2 spent far longer in rejection territory (8 103 attempts / 3 773 rejects), so we should drop the aggressive φ scaling and return to the regen6 version as the working baseline.
   - Backed out the φ-scaling experiment by restoring config/models/hurdle/exports/version=2025-10-24/20251024T234923Z/nb_dispersion_coefficients.yaml to the regen6 baseline (pulled from HEAD since git checkout is blocked by the LFS hook). This gets us back to the last-known-good pairing of β_μ and β_φ.
   - Logged the regen7 failure and the rollback in docs/logbook/10-2025/2025-10-25.md:16-24 so future tuning runs have the full metrics trail (rho_reject=0.466, p99=6, cusum_max=15.6, followed by the dispersion reset).

* 11:00am:
   - Added MCC×channel-specific tweaks to the governed NB mean/dispersion coefficients (targeting the worst offenders from regen6), then attempted a fresh Segment 1A run (runs/local_layer1_regen9, parameter_hash=065446299498b7970ed2b60f059b184b170c33c6733a67ebf91979fbd0aec370, manifest_fingerprint=b128331e18c166296faf4e8ced3278370faac06fe01c59efd0e0238c5007ca97). S0 and S1 completed normally; S2 ran but the CLI timed out before we reached S3. Logs show S2 validation passed (ρ=0, p99=0, cusum=0) thanks to the hyper-targeted adjustments, so the corridor issue appears solved.
   - Because the run timed out during S3 (likely due to the shell timeout, not a code failure), we couldn’t capture _passed.flag and downstream artefacts yet.

* 1:00pm:
   - I reran the full Segment 1A CLI (new runs/local_layer1_regen10, parameter_hash=06544629…, manifest_fingerprint=b128331e…) with a 90‑minute shell window so we could carry S3–S9. S0, S1 and S2 all completed and S2 validation passed (ρ=0, p99=0, CUSUM=0). The run then failed during S3’s cross‑border sequencing with ERR_S3_SITE_SEQUENCE_OVERFLOW: site_order 1000000 exceeds 6-digit capacity. That means a merchant’s site ordering exceeded the six‑digit guard defined in the spec; it’s not a timeout, it’s a hard validation rule we’ll have to address before we can finalize a _passed.flag.
  - No artefacts were produced beyond the S2 outputs already in runs/local_layer1_regen10/parameter_scoped/parameter_hash=06544629…. Let me know if you want me to dig into the S3 overflow (e.g., inspect the offending merchant and adjust sequencing thresholds) or try another strategy for continuing from S3 onward.

* 1:37pm:
   - Added INFO logging around every Segment 1B state so the CLI now reports start/finish plus key artefact paths for S0→S9. This lives in packages/engine/src/engine/scenario_runner/l1_seg_1B.py and brings the 1B output parity with the detailed 1A logs you already have.
   - Instrumented S3’s sequencing kernel (packages/engine/src/engine/layers/l1/seg_1A/s3_crossborder_universe/l1/kernels.py) to emit a descriptive error before raising ERR_S3_SITE_SEQUENCE_OVERFLOW. If a merchant/country gets more than 999 999 outlets, the log now records the merchant_id, ISO, and offending count, making it easy to identify which policies need tuning.
   - Reran the CLI immediately after the instrumentation: `runs/local_layer1_regen10` (parameter_hash=dcf159f5…, manifest_fingerprint=3f5453a7…) progressed through S2 with `rho=0` and then halted at S3 with `ERR_S3_SITE_SEQUENCE_OVERFLOW`. The new log pinpoints the offender: merchant `9460307433624258128` targeting `BM` (count≈1.62M). Next task is to clamp that merchant’s S3 budget or tighten the sequencing policy so no country exceeds the 6-digit cap.

* 5:23pm:
   - Added INFO-level logging around each Segment 1B state so engine.cli.segment1b now mirrors the rich progress output you see for Segment 1A (start/finish + key artefact paths for S0→S9).
   - Instrumented the S3 sequencing kernel so any merchant/country that would exceed the six-digit site_order cap logs its ID, ISO, and requested count before the run aborts. This pointed us to merchant 9460307433624258128, country BM, with 1.62 M requested slots.
   - Implemented deterministic guards upstream: S2 NB sampling now clamps per-merchant outlet counts to 999 999 (with warnings and validator parity), and S3 redistributes any overflow across remaining candidate countries while keeping totals intact. If, after redistribution, there still isn’t enough headroom, the run raises a descriptive error instead of silently corrupting counts.
