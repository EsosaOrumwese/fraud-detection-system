# Logbook
### Date: 29th October 2025
### Project: Fraud Detection System
### Reference commits: Check commits on this date (if any)

* 2:58am:
   - Threaded the S4 allocator so it honours a configurable worker pool: AggregationContext now carries workers, the aggregator fans countries across a ThreadPoolExecutor, preserves deterministic shard order, and logs per-country timing snapshots (packages/engine/src/engine/layers/l1/seg_1B/s4_alloc_plan/l2/aggregate.py:31,54,147). Run reports include the timing table and explicit worker counts via the extended allocation result and materialiser (packages/engine/src/engine/layers/l1/seg_1B/s4_alloc_plan/l1/allocation.py:34, packages/engine/src/engine/layers/l1/seg_1B/s4_alloc_plan/l2/materialise.py:102, packages/engine/src/engine/layers/l1/seg_1B/s4_alloc_plan/l3/observability.py:49).
   - Surfaced the new knob through the orchestration layer: Segment1BConfig carries s4_workers, the CLI exposes --s4-workers, and the scenario runner forwards the value into the S4 runner (packages/engine/src/engine/cli/segment1b.py:60 & 314, packages/engine/src/engine/scenario_runner/l1_seg_1B.py:59 & 203).
   - Restored the S5 anomaly counters by reinstating the missing NumPy import so the streaming tile-index checks no longer crash (packages/engine/src/engine/layers/l1/seg_1B/s5_site_tile_assignment/l2/materialise.py:13).

* 4:45am:
   - Patched the S5 assignment kernel to build the dataframe with an explicit UInt64 schema so large tile identifiers no longer trigger Polars inference overflow (packages/engine/src/engine/layers/l1/seg_1B/s5_site_tile_assignment/l1/assignment.py:180).
   - Dry-run S5 on `local_layer1_regen7` succeeds and publishes the dataset; S6 currently fails immediately because its loader still expects the legacy `TileIndexPartition(frame=...)` signature. That parity gap sits in `s6_site_jitter/l0/datasets.py` and will need the same streaming-aware constructor the other states now rely on.

* 7:15am:
   - Reworked the S6 jitter stage to stream per-country: loaders now expose `collect_country`, the aggregator employs lazy caches, and the kernel drops per-ISO buffers once processed while logging progress so we can spot long tails (packages/engine/src/engine/layers/l1/seg_1B/s6_site_jitter/l0/datasets.py, l2/aggregate.py, l1/jitter.py). Waiting on the next full Segment 1B run to confirm the reduced memory profile.
   - Follow-up run exposed Polars schema inference at S6 materialisation; fixed by providing an explicit schema so large tile identifiers stay in UInt64. Augmented jitter country logs with `start`/`complete` counters so the next run highlights ordering and outstanding countries (packages/engine/src/engine/layers/l1/seg_1B/s6_site_jitter/l1/jitter.py).

* 1:45pm:
   - Converted S6 tile-bound/centroid caches to hydrate directly from `pyarrow.dataset` into NumPy arrays with per-ISO hash indexes, wiping out the 400M `iter_rows`/`searchsorted` hotspot (packages/engine/src/engine/layers/l1/seg_1B/s6_site_jitter/l2/aggregate.py).
   - Enhanced the jitter kernel with hybrid batching: early attempts stay scalar, stubborn sites fall back to an 8-attempt vectorised path that uses Shapely `vectorized.contains/touches` with deterministic RNG rollback. Bounding boxes travel with the cached polygons so we skip shapely altogether when a candidate sits outside the envelope (packages/engine/src/engine/layers/l1/seg_1B/s6_site_jitter/l1/jitter.py).
   - Profiling harness rerun: `tools/perf/run_s6_profile.py` now reports 257 s (down from 1,540 s) with new stats captured at `docs/perf/s6_jitter/profile_20251029_after_vector.pstats`; remaining hotspots centre on NumPy `searchsorted` which feeds the cache lookups.

* 5:17pm:
   - Implemented faster tile lookups and refreshed S6 coverage:
      - packages/engine/src/engine/layers/l1/seg_1B/s6_site_jitter/l2/aggregate.py (line 115) now caches each ISO’s tile bounds/centroids in NumPy arrays with per-tile hash indexes, eliminating the hot np.searchsorted loop while keeping the streaming loader interface.
      - packages/engine/src/engine/layers/l1/seg_1B/s6_site_jitter/l1/jitter.py (line 200) adds a deterministic hybrid sampler: first attempts stay scalar, stubborn sites switch to an 8-attempt vectorised Shapely batch with bounding-box short-circuits and RNG rollback, keeping counters intact.
      - packages/engine/src/engine/layers/l1/seg_1B/s6_site_jitter/l3/validator.py (line 85) now consumes the streaming partitions via collect_country, rebuilding bounds/centroid maps per ISO so validation no longer depends on eager .frame attributes.
      - tests/engine/layers/l1/seg_1B/test_s6_site_jitter.py (line 68) provisions a small dual-ISO fixture; it exercises the full runner, checks run-report telemetry, and keeps the materialiser test in sync with the streaming validator.

* 5:25pm:
   - Re-profiled S6 after the hash-index optimisation; harness run clocks in at 192 s (`docs/perf/s6_jitter/profile_20251029_after_hashindex.pstats`). PyArrow dataset scans (`dataset.to_table`) now dominate cumulative time, so the next optimisation pass will target reducing repeated ISO reads (e.g., caching row-groups or reusing table batches).
* 6:14pm:
   - Reran the profiling harness with the safer per-ISO streaming caches (no bulk prefetch). New snapshot lands at 189 s (`docs/perf/s6_jitter/profile_20251029_after_cacheprefetch.pstats`). The `dataset.to_table` scans remain the clear hotspot, confirming the next optimisation needs to focus on Arrow reuse rather than further cache tweaks.
   - S6 cache prefetch is in place, plus the regression harness now exercises the full runner:
      - packages/engine/src/engine/layers/l1/seg_1B/s6_site_jitter/l2/aggregate.py (line 116)
      - `_TileBoundsCache` / `_TileCentroidCache` now memoise every ISO the first time they’re touched, reading the parquet dataset once, grouping by ISO in numpy, and storing per-ISO hash indexes. Subsequent lookups just consult the cached arrays, so the expensive dataset.to_table(..., filter=…) cycle disappears.
      - packages/engine/src/engine/layers/l1/seg_1B/s6_site_jitter/l1/jitter.py (line 200) No behavioural change today, but it’s now fed by the prefetching caches (hybrid sampler still intact from the earlier optimisation).
      - packages/engine/src/engine/layers/l1/seg_1B/s6_site_jitter/l3/validator.py (line 85). The validator mirrors the streaming API: instead of touching the old .frame, it calls collect_country for only the ISO codes present in the jitter dataset.
      - tests/engine/layers/l1/seg_1B/test_s6_site_jitter.py (line 68). The fixture brings up two ISOs, and the runner test checks the emitted parquet as well as run-report RNG telemetry, so we have coverage for the streaming path.

* 6:09pm:
   - That crash came from the “eager prefetch everything” change—we were materialising every ISO’s columns at once, which ballooned into a 1.6 GiB allocation and blew up. I’ve reverted that approach so _TileBoundsCache and _TileCentroidCache go back to loading one ISO at a time (via dataset.to_table(..., filter=ds.field("country_iso") == iso)), caching the result for reuse, and never holding the whole surface in memory.
