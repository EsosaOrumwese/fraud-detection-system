# Logbook
### Date: 10th October 2025
### Project: Fraud Detection System
### Reference commits: Check commits on this date (if any)


* 12:04am:
   - S2 spec + contract review complete; key takeaways captured (multi merchants only, NB2 links, RNG labels gamma_component, poisson_component, nb_final, validation/error codes like ERR_S2_ENTRY_NOT_MULTI, ERR_S2_NUMERIC_INVALID).
   - The dataset dictionary now advertises the S2 streams. In contracts/dataset_dictionary/l1/seg_1A/layer1.1A.yaml I added s2 to the states list and registered rng_event_gamma_component, rng_event_poisson_component, and rng_event_nb_final, each with the correct schema refs, partitions, and gating (gated_by: rng_event_hurdle_bernoulli). Layer schemas already covered these streams, so no changes were needed there.

* 12:15am:
   - Added compute_nb_links and compute_links_from_design in packages/engine/src/engine/layers/l1/seg_1A/s2_nb_outlets/l1/links.py. They reuse the S0 design vectors and governed beta_mu/beta_phi bundles to produce deterministic NB2 parameters (eta_mu, eta_phi, mu, phi) with the same Neumaier/binary64 policy as S0/S1, and raise ERR_S2_NUMERIC_INVALID if the results are non-finite or non-positive.
   - Exported the helpers via packages/engine/src/engine/layers/l1/seg_1A/s2_nb_outlets/__init__.py so orchestrators can import them.
   - Added regression coverage (tests/engine/layers/l1/seg_1A/test_s2_nb_links.py) that checks the dot products, exponentiation, dictionary alignment, and shape-mismatch failure case.
   - Updated the dataset dictionary header (contracts/dataset_dictionary/l1/seg_1A/layer1.1A.yaml:1) to flag that the file now covers S2 as well.

* 4:43am:
   - Implemented the deterministic S2 context builder so only hurdle-approved merchants enter the NB stage and their links are computed with the governed coefficients (`packages/engine/src/engine/layers/l1/seg_1A/s2_nb_outlets/l2/deterministic.py`).
   - Added Philox substream helpers plus an event writer that stamps gamma/poisson component envelopes and the non-consuming nb_final log with trace accumulation (`packages/engine/src/engine/layers/l1/seg_1A/s2_nb_outlets/l1/rng.py`, `packages/engine/src/engine/layers/l1/seg_1A/s2_nb_outlets/l2/output.py`).
   - Built the NB sampler runner that loops attempts, enforces rejection counting, emits all three event streams, and returns the accepted outlet counts for later states (`packages/engine/src/engine/layers/l1/seg_1A/s2_nb_outlets/l2/runner.py`).

* 4:59am:
   - Segment1A orchestration now flows through S2: we capture the NB deterministic inputs, execute the sampler, and expose downstream contexts alongside the existing hurdle data (`packages/engine/src/engine/scenario_runner/l1_seg_1A.py`).
   - Added the replay validator that regenerates gamma/poisson draws, checks envelopes, and cross-verifies nb_final against optional runner output `(packages/engine/src/engine/layers/l1/seg_1A/s2_nb_outlets/l3/validator.py`).
   - Tightened the S2 surface: exports now include the deterministic builder/runner, the CLI gained a dedicated entry point, and RNG logs land under the expected partitions (`packages/engine/src/engine/layers/l1/seg_1A/s2_nb_outlets/__init__.py`, `packages/engine/src/engine/layers/l1/seg_1A/s2_nb_outlets/l2/runner.py`, `packages/engine/src/engine/cli/s2_nb_outlets.py`, `packages/engine/src/engine/cli/__init__.py`).
   - Added focused tests that exercise the runner+validator happy path and detect tampered logs (`tests/engine/layers/l1/seg_1A/test_s2_nb_validator.py`).
   - Hooked the Segment1A orchestrator up to S2: after S1 it now builds the deterministic NB context, runs the sampler, validates it, and exposes an S2StateContext alongside the existing hurdle context (packages/engine/src/engine/scenario_runner/l1_seg_1A.py).
   - Added a dedicated CLI entry point (engine.cli.s2_nb_outlets) that loads sealed artefacts, regenerates hurdle decisions, runs S2, emits result metadata, and optionally validates the logs (packages/engine/src/engine/cli/s2_nb_outlets.py, packages/engine/src/engine/cli/__init__.py).
   - Delivered a deterministic S2 validator that replays gamma/poisson draws and checks nb_final invariants, plus new unit tests for links, validator, and the CLI happy-path (packages/engine/src/engine/layers/l1/seg_1A/s2_nb_outlets/l3/validator.py; tests under tests/engine/...).

* 5:15am:
   - Wired the Segment1A orchestrator to call S2 and capture NB counts alongside the hurdle context, exposing `S2StateContext` for downstream states (`packages/engine/src/engine/scenario_runner/l1_seg_1A.py`).
   - Added an S2 CLI (`packages/engine/src/engine/cli/s2_nb_outlets.py`) that consumes sealed artefacts, runs the NB sampler, writes result metadata, and optionally validates emitted RNG logs.
   - Implemented the S2 RNG validator replaying gamma/poisson draws and added CLI/integration coverage (`packages/engine/src/engine/layers/l1/seg_1A/s2_nb_outlets/l3/validator.py`, `tests/engine/cli/test_s2_nb_cli.py`, `tests/engine/layers/l1/seg_1A/test_s2_nb_validator.py`).

* 5:31am:
   - Unified S0/S1/S2 progress logging conventions ("Sx: … (elapsed, delta)") so operators see consistent timing envelopes across the pipeline.
   - Extended the S2 validator to emit corridor metrics (rho_reject, p99, CUSUM) while replaying RNG events; metrics bubble up through Segment1A orchestration and the new CLI.
   - Added a `segment1a` CLI that drives S0→S2 end-to-end via `Segment1AOrchestrator`, emits run summaries/metrics, and reuses the state validators.

* 6:41am:
   - Hardened corridor governance so validate_nb_run now requires the YAML policy, emits CSV/JSON metrics, and raises breach codes for rho/p99/CUSUM limits
   - Added reusable bundle hashing plus S2 artefact publishing so validation artifacts land inside the sealed bundle when available
   - Wired CLI/orchestrator flows to load the corridor policy, persist catalogues, and publish metrics paths for downstream contexts
   - Introduced the governed policy file, refreshed operator docs, and recorded the active test plan for S2.

* 7:03am:
   - Added S3 to the Layer‑1 dictionary and introduced the crossborder block that wires candidate, priors, counts, and sequencing datasets to the new schema anchors
   - Published the authoritative JSON schema for all S3 tables 
   - Landed the governed policy artifacts: rule ladder, deterministic base-weight config, and thresholds bundle, plus refreshed the policy README to catalogue the unlocked files.

* 7:16am:
   - Added packages/engine/src/engine/layers/l1/seg_1A/s3_crossborder_universe/l2/deterministic.py with artefact specs/metadata, merchant profiles, and the build_deterministic_context helper that enforces S3.0 gates (multi-site verification, ISO/channel vocab checks) and captures governed artefact digests.
   - Extended packages/engine/src/engine/layers/l1/seg_1A/s0_foundations/exceptions.py with S3-specific failure codes (authority missing, precondition, partition mismatch, vocab invalid, rule ladder invalid) so downstream callers surface spec-aligned errors.
   - Created tests/engine/layers/l1/seg_1A/test_s3_deterministic.py covering happy-path context assembly plus failure modes (non-multi merchant, bad channel, missing policy).

* 4:51pm:
   - Implemented the full S3 cross-border universe pipeline: rule-ladder parsing, candidate construction, deterministic kernels, and L2 runner now emit `s3_candidate_set.parquet`, with validation via `validate_s3_candidate_set`. 
   - Segment1A orchestration and CLI were extended to execute S3 and summarise outputs, and the governed policy gained admit/deny metadata plus cross-border flags. 
   - Added regression coverage (`tests/engine/layers/l1/seg_1A/test_s3_runner.py`) and refreshed docs (AGENTS.md + test plan) for the new state.

* 6:00pm:
   - Expanded the S3 validator to a full deterministic replay covering candidates, priors, integerised counts, and sequencing, returning metrics and raising spec-aligned errors. Added a helper bundle publisher so validation metrics land in `validation_bundle/.../s3_crossborder_universe/metrics.json `
   - Threaded S3 validation through the Segment1A orchestration: `S3StateContext` now carries metrics and bundle paths, the runner signature exposes `validate_s3`, and the new validation block reruns kernels, publishes artefacts, and respects the skip flag.
   - Updated the Segment1A CLI with `--no-validate-s3`, surfaced optional S3 lanes/toggles in logs and JSON summary, and now reports validation metrics and artefact locations.
   - Documented the new workflow: AGENTS highlights the toggles and validator/bundle path, while the package README introduces Segment 1A execution highlights and validation defaults.
   - Added rich coverage for the optional S3 lanes: helper scaffolding plus tests for priors/integerisation/sequencing, validator breach detection, and toggle gating.

* 6:41pm:
   - Reworked the S3 policy loader stack to match the spec: base-weight artefacts now understand constants/sets/ordered rules and return compiled rules with ISO validation, while threshold artefacts accept the new `dp_resid` / floors / ceilings layout and remain backward-compatible. Added `evaluate_base_weight` so L1 can reuse the compiled rules safely.
   - Updated the L1 kernels to rely on the new policy shape: priors are produced via rule evaluation with optional fallback to uniform weights, and integerisation obeys per-ISO floors/ceilings with deterministic residual handling.
   - Segment1A now validates policy artefacts after the deterministic context is sealed, passing the run ISO universe into the loader so `--param policy.s3.*` paths are checked automatically before kernels run.
   - Refreshed the governed samples to the new schema: `policy.s3.base_weight.yaml` now declares constants + selection rules, and `policy.s3.thresholds.yaml` provides residual precision plus floors/ceilings.
   - Reworked the S3 runner tests to exercise the new policy shapes, covering successful lanes, validation failures, and toggle gating with the updated YAML fixtures.

* 7:31pm:
   - Added full scoring support for deterministic priors: `_compute_priors` now keeps a weight slot per candidate, drops excluded countries to zero weight, and only falls back to uniform shares when no rule assigns any score. This keeps the Hamilton step aligned with emitted priors and honours exclusions without losing coverage.
   - Extended the policy predicate evaluator so numeric comparisons like `mcc == 5411` work even when the underlying context stores strings—mixed-type comparisons are coerced safely, letting governed artefacts use the documented DSL.
   - Enhanced test coverage with unit cases that exercise rule exclusion, zero-weight fallback, and no-score scenarios, ensuring priors behave per spec.