# Logbook
### Date: 27th October 2025
### Project: Fraud Detection System
### Reference commits: Check commits on this date (if any)

* 1:19am:
   - packages/engine/src/engine/layers/l1/seg_1B/s1_tile_index/l2/runner.py now logs every expensive step: when ISO/polygon/raster artefacts load (with sizes), when tile enumeration starts/finishes, periodic progress every 25 countries, and when each parquet partition is written. These messages surface right in the Segment 1B CLI stdout thanks to the earlier logging.basicConfig change.
   - packages/engine/src/engine/layers/l1/seg_1B/s2_tile_weights/l2/runner.py emits INFO logs for ISO/tile-index loads, mass computation, quantisation, and final partition writes, so you can watch S2 chew through the large frames instead of wondering if it hung.

* 1:57am:
   - Added batch Parquet writers so the tile index and bounds are written incrementally to temporary directories before being atomically published. This avoids building huge in-memory lists and eliminates the MemoryError you hit once the raster windows expand.
   - Wrapped the entire enumeration flow with detailed logging plus temp-dir cleanup on failure, retaining the earlier progress logs but now reporting the actual tile count emitted from the streaming writer.
   - Updated the PAT/reporting bookkeeping to rely on the streaming counts, ensured schema-consistent empty shards are produced when needed, and removed the now-obsolete frame-building helpers.
   - Cleaned up imports and introduced the reusable _ParquetBatchWriter helper class.

* 4:30am:
   - Swapped the per-cell shapely loop for chunked raster windows driven by rasterio.features.rasterize. Each chunk (512×512) now produces inclusion masks entirely in C, so we only touch the handful of cells that survive the mask, and we compute centroid grids via affine math instead of millions of Point objects.
   - Holes are handled by rasterizing interior rings once per country; summary counts (visited/included/hole/outside) are updated in bulk, with only the tile‑ID min/max tracked per included cell.
   - Added batch Parquet writers that stream `tile_index`/`tile_bounds` rows to disk as soon as 200k records accumulate (with flush logs), keeping memory flat no matter how many tiles a country yields.
   - Instrumented the inner loops so you now see per-country window info and batch flushes in segment1b_regen6.log.
   - Removed the old `_point_in_hole` code and unused data structures; all stats now come from the same raster masks we use for inclusion.
   - `packages/engine/src/engine/layers/l1/seg_1B/s1_tile_index/l2/runner.py` now rasterizes each country polygon in 512×512 windows using `rasterio.features.rasterize` instead of iterating cell-by-cell through shapely. We compute centroids via affine math, only touch cells flagged by the mask, and update per-country stats in batches. Hole accounting is done via a second rasterized mask of polygon interiors, so we no longer need the old `_point_in_hole` logic.
   - Outputs no longer buffer millions of dicts. A new `_ParquetBatchWriter` streams `tile_index` and `tile_bounds` rows to per-partition temp directories as soon as batches hit 200k rows; every flush is logged so you can see progress, and empty shards are handled cleanly.
   - Logging now reports each country window and every parquet flush, giving live visibility instead of long silent gaps.

* 4:58am:
   - Track 1 coding work is now in place:
      - _raster_window_for_geometry uses true polygon bounds (via a helper that walks every exterior vertex) with a small fallback to keep coverage safe. That means each country now only scans the cells it actually occupies, instead of a loose corner-based box.
      - Chunk processing logs the country/row/col window, tiles visited/included, and chunk duration, so the run log shows exactly how the enumerator is progressing.
      - The chunk loop iterates in fixed windows (default 512×512) with minimal overhead, and every flush is still streamed through the batch parquet writer we added earlier.
     This checks off Track 1.A and Track 1.B from the engineering decision doc. Next step is Track 1.C (parity harness) before we tackle parallel execution.
   - Track 1.C is now in place: I added tests/engine/layers/l1/seg_1B/s1_tile_index/test_windowing_parity.py, which builds a synthetic raster + sample polygons and compares the new chunked enumerator directly against a brute-force reference for both inclusion rules (center/any overlap). It also asserts the tightened window stays bounded so we know the new row/col math isn’t over-scanning. We can run this regression any time to confirm future tweaks don’t drift.
    No runtime tests were executed yet (the new pytest covers the parity check). Once you’re ready, we can move on to Track 3 (parallel execution) with confidence the tighter windowing is still emitting the exact same tiles.

* 5:31am:
   - Track 2 polish is in. We now cache the entire country mask once per window (and slice it per chunk) instead of re-rasterizing each chunk, cache row-based geodesic areas so we don’t recompute them per cell, and log every chunk’s progress (visited/included/duration). Holes are rasterized once per country, too. To preserve the “any overlap” semantics, we buffer polygons by half a pixel when InclusionRule.ANY_OVERLAP is active—matching the legacy intersection logic—and the parity tests confirm we’re still enumerating exactly the same tile set as the brute-force reference.

* 5:49am:
   - Step 3.1 is wired up: 
     - Segment1BConfig and the CLI now accept --s1-workers (default 1). When you pass --s1-workers=N, the value flows through Segment1BOrchestrator into S1RunnerConfig.
     - S1RunnerConfig gained a workers field, and S1TileIndexRunner reads it. For now we still execute single-threaded; if N>1 we log that multiprocess mode isn’t live yet but record the requested count in the PAT metrics (workers_used) so telemetry matches the intent.
     - No behavioral changes yet beyond the plumbing, so existing runs behave exactly as before unless you override the flag.
   - Tests: py -3.12 -m pytest tests/engine/layers/l1/seg_1B/s1_tile_index/test_windowing_parity.py

- 4:39pm:
   - Added worker-aware orchestration so the runner picks single- vs multi-process execution, emits per-worker telemetry, and persists enhanced PAT metrics including tiles/sec stats.
   - Implemented dedicated single-worker and multi-worker execution flows with temp-shard management, deterministic merges, and shared summary aggregation.
   - Introduced helpers for partitioning country batches, running a ProcessPoolExecutor, rebuilding geometries from WKB, and merging worker shards into canonical order.

- 10:41pm:
   - Current Baseline:
      - Latest run still spends ~51½ minutes in S1 before the segfault hits (runs/segment1b_regen6.log:4, runs/segment1b_regen6.log:9793, runs/segment1b_regen6.log:9806), so we need roughly a 10× speed-up to hit a 5-minute target.
   - Acceleration Plan:
      - Profile First: Capture a short representative run under py-spy/cProfile to quantify where the remaining 51 minutes land (mask generation vs. Python loops vs. parquet writes). That gives us hard numbers before touching code.
      - Vectorise the Chunk Hot Loop: The inner per-cell loop at packages/engine/src/engine/layers/l1/seg_1B/s1_tile_index/l2/runner.py:729 is still pure Python. Rework it so each chunk produces NumPy arrays (rows, cols, tile_id, centroid_lon/lat, bounds) and appends whole batches instead of iterating per tile. Tile IDs are linear (row * ncols + col), bounds can be derived from the affine once per chunk, and pixel area can come from a precomputed per-row lookup instead of tile_bounds()/tile_area() calls.
      - Bulk Writers, Fewer Flushes: Extend _ParquetBatchWriter (…/runner.py:808) so it accepts columnar batches (or replace it with a streaming pyarrow.ParquetWriter). Larger row groups (>1 M) combined with a lighter codec (Snappy/LZ4) will cut the amortised cost of writing 221 M rows.
      - Smarter Worker Scheduling: _partition_countries still slices the country list evenly by count (…/runner.py:974). Switch to cost-aware scheduling (queue countries by window pixel count, let workers fetch the next heaviest job) so we eliminate the long tail where worker‑9 handles 50 M tiles alone.
      - Warm Caches & Shared Metadata: Precompute per-row geodesic areas and lon/lat edge arrays once per process and reuse them across chunks; keep the PopulationRaster metadata in shared memory so workers don’t reload affine/geod repeatedly.
      - Iterate with Measurement: After each optimisation pass, rerun Segment 1B with --s1-workers equal to core count and log tiles/sec per worker until we consistently hit the few-minute envelope. Once stable, fold the improvements into the Track 3/4 regression harness to guard determinism.