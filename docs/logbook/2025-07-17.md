# Logbook
### Date: 17th July 2025
### Project: Fraud Detection System
### Issues Resolved: [SD-02](https://github.com/EsosaOrumwese/fraud-detection-system/issues/25) `in-progress`
### Reference commits: Check commits on this date (if any)

* 9:00am
  * Detailed out the logic/specifications for the revamp v2 (although there are still holes to be filled). This is more than a revamp as we're scrapping the old one and starting again with realism baked in from the get go.
  * To achieve SD-02
* 
* 9:42am
  * One of the most important skills I've learnt working on this project is AI Engineering. In trying to get the model to produce code artifacts that I want, I have been able to really think about what problem I want to solve rather solve rather than the code itself
  * For I got to see that the reason the model produced rubbish code was because I didn't clearly understand what problem I was trying to solve and didn't have a way of communicating it clearly to the model
  * In building the first synthetic generator, I start with:
    * Just getting generated code based of vague issue descriptions and checklists. That worked but it produced flimsy code
    * Next was to try to take the issue as the end goal and break down the development into phases, as one would naturally do. This time, it generates a scaffold or backbone in the first phase and then builds upon it till it gets to the last stage.
      * This was a good method, but it exposed the hole in my thinking. As I didn't understand the problem well enough to convey to the model what I wanted.
      * Since it had no clear and detailed specifications communicated to it, it ended up producing its own thing which didn't match with what I wanted (to be fair, I didn't even know exactly what I wanted)
    * The next method I'm on is one based off Sean Grove, talk ["The New Code"](https://www.youtube.com/watch?v=8rABwKRsec4) given at AI Engineer World's Fair. And it can be summarized as;
      1. Start a specification for every AI feature _(What do you expect to happen? What does success criteria look like?)_
      2. Debate clauses, attach examples _(Is it clearly written down and communicated)_
      3. Make the spec executable
      4. Feed the spec to the model 
      5. Test against your spec
    * This was something I had begun to try before coming across his presentation
      * I broke down the synthetic generator with realism baked into it into a feature described in narrative technical details,
        * For each feature, it got further broken down into segments and described in technical narrative details
        * For each segment, it got described, once again in narrative technical details. With assumptions produced by the model which made sure it aligned with the initial feature description
      * For the first feature description, I "brutally" tested it using the `o3` model against my goal of `a realistic synthetic fraud transaction generator fit for production` and scored it. 
        * Getting an unbiased score of 9+/10 led to it being the specification for my goal
      * Next each feature, broken down, got its description tested against the main spec and scored. Once it passed with ~10/10, it formed the specification for that feature.
        * And so on, until we got down to the subsegment description.
    * Next thing to do is to mirror OpenAI's [`model_spec`](https://github.com/openai/model_spec/blob/main/model_spec.md) and fully 
        