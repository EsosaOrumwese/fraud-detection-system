# Descriptive Narrative of Flow w/regards to Platform
_NOTE to AGENT: You are only allowed to change the doc under the EXPLICIT instruction from the USER. And if done, ensure edit is woven into the narrative flow of the platform_

## Local-Parity SR->WSP->IG->EB Narrative (`platform_20260201T224449Z`) — v0 pins surfaced

In local parity, the platform stack is brought up with LocalStack Kinesis, MinIO (object store), and the supporting services used by Control and Ingress. Kinesis exposes the control and event-bus streams, and MinIO exposes the fraud-platform bucket used for run artifacts and receipts. The run begins by generating a new **`platform_run_id`** and writing it as the active platform run (e.g., `runs/fraud-platform/ACTIVE_RUN_ID`) so downstream components resolve **platform-run-scoped** paths deterministically.

Scenario Runner (SR) executes against the Oracle Store roots (`ORACLE_ROOT`, `ORACLE_ENGINE_RUN_ROOT`, `ORACLE_STREAM_VIEW_ROOT`) and the interface pack contract. SR resolves a deterministic **`scenario_run_id`** (from the run equivalence key), validates the gate pass, assembles a `run_facts_view` for the selected scenario, and writes that view into the object store at a platform-run-scoped path (write-once; drift is rejected). SR then emits a READY control event into the control bus (topic `fp.bus.control.v1` on the Kinesis control stream) containing **both** `platform_run_id` and `scenario_run_id`, plus `manifest_fingerprint`, `parameter_hash`, `bundle_hash` (or plan hash), and other required pins. The READY event is idempotent: its `message_id` is derived from `platform_run_id + scenario_run_id + bundle_hash/plan_hash`, and consumers treat duplicates as no-ops.

World Streamer Producer (WSP) runs as the ready consumer. It listens to the control bus and, when it receives a READY event, dedupes by READY `message_id`, loads the `run_facts_view` from MinIO, and validates that the `scenario_run_id` in READY matches the facts view it loads. It builds its output plan from the `local_parity` profile: traffic output ids from `config/platform/wsp/traffic_outputs_v0.yaml` (fraud stream by default) and context output ids from `config/platform/wsp/context_fraud_outputs_v0.yaml` (`arrival_events_5B`, `s1_arrival_entities_6B`, `s3_flow_anchor_with_fraud_6B`). The run applies a stream speedup of `600x` (_this is a variable, where `0x` causes the events to flow according to the realistic `ts_utc` time_) and sets output concurrency to `4`, so all four outputs start together. Each output worker iterates its view, constructs the canonical event envelope with `ContextPins` (including `platform_run_id` and `scenario_run_id`) and payload, derives a deterministic `event_id`, and POSTs the event to the IG ingest endpoint. Under v0 pins, transient 429/5xx/timeouts are retried with bounded exponential backoff using the **same** `event_id`; schema/policy 4xx are treated as non-retryable and stop the stream with an explicit surfaced reason. The run caps emission at `200` events per output, so each worker emits `200` events before stopping (_this is a variable and can be adjusted or ignored to allow a full run_).

Ingestion Gate (IG) receives each POST and performs the admission pipeline. It validates required pins by class (`traffic_fraud`, `context_arrival`, `context_arrival_entities`, `context_flow_fraud`) as defined in `config/platform/ig/class_map_v0.yaml`, applies schema policy, and resolves the canonical `event_class` (aligned to `class_map_v0.yaml` and stable across topic/version changes). IG computes the dedupe tuple **`(platform_run_id, event_class, event_id)`** and persists `payload_hash` for anomaly detection (`eb_topic` is recorded as metadata, not part of the dedupe key). If the same dedupe tuple is seen again with the same `payload_hash`, IG logs a DUPLICATE and drops the event; if the dedupe tuple matches but `payload_hash` differs, IG raises an anomaly and quarantines (never silent replace). For new events, IG records an admission row in state `PUBLISH_IN_FLIGHT`, then selects the partitioning profile and stream for the event class (for example, `ig.partitioning.v0.traffic.fraud` routes to `fp.bus.traffic.fraud.v1`; `ig.partitioning.v0.context.arrival_events` routes to `fp.bus.context.arrival_events.v1`; `ig.partitioning.v0.context.arrival_entities` routes to `fp.bus.context.arrival_entities.v1`; `ig.partitioning.v0.context.flow_anchor.fraud` routes to `fp.bus.context.flow_anchor.fraud.v1`). The partition key is computed from the `key_precedence` rules in `config/platform/ig/partitioning_profiles_v0.yaml` (v0 claim: context locality is by `merchant_id`; `arrival_seq` participates in JoinFrameKey downstream, not partitioning). IG publishes to LocalStack Kinesis and updates the admission row to `ADMITTED` with `eb_ref` on success; on timeout/unknown publish outcome it records `PUBLISH_AMBIGUOUS` and does not blindly re-publish without reconciliation. IG writes an admission receipt JSON into MinIO under `s3://fraud-platform/{platform_run_id}/ig/receipts/` for ADMIT/DUPLICATE/QUARANTINE. ADMIT receipts are written after publish success and include `platform_run_id`, `scenario_run_id`, `event_class`, `payload_hash`, `admitted_at_utc`, and `eb_ref`; if receipt writing fails after publish, IG preserves `eb_ref + payload_hash` in the admission DB and marks `receipt_write_failed` for backfill so EB evidence is not lost.

Event Bus (EB) in local parity is Kinesis. For this run, four EB topics carry the admitted stream: `fp.bus.traffic.fraud.v1`, `fp.bus.context.arrival_events.v1`, `fp.bus.context.arrival_entities.v1`, and `fp.bus.context.flow_anchor.fraud.v1`. Because the delivery model is at-least-once, a small number of early retries occurred; IG detected these as duplicates (same dedupe tuple `(platform_run_id, event_class, event_id)` re-sent shortly after) and dropped them. As a result, EB admitted counts reflect unique events, not raw WSP sends. In this run WSP emitted `200` per output, while EB admitted `194` (traffic), `191` (arrival_events), `193` (arrival_entities), and `196` (flow_anchor). This is expected: IG de-duplication protects the EB from replays so downstream consumers see one canonical copy per dedupe tuple.

Operationally, IG health remained AMBER with `BUS_HEALTH_UNKNOWN` during the run, yet admission and publishing proceeded without interruption. The flow observed in logs is continuous and concurrent: WSP outputs start together, events stream into IG in parallel, and EB receives the four topics without sequential gating between streams. At the end of the run, WSP stops each output after its `200`-event cap, while IG completes any in-flight admissions and final receipts/backfill markers.

This is the implemented Control and Ingress posture that RTDL will consume: SR provides run facts plus an idempotent READY signal carrying both `platform_run_id` and `scenario_run_id`; WSP produces traffic and context streams concurrently with deterministic `event_id`s and bounded retries; IG validates, deduplicates using `(platform_run_id, event_class, event_id)` with `payload_hash` anomaly detection, routes by partitioning profile, and preserves EB evidence even under receipt/write faults; and EB provides the four admitted topics with deterministic keys derived from IG partitioning profiles. Context topics are merchant-local by partitioning, while traffic does not carry `(merchant_id, arrival_seq)` yet, so RTDL resolves joins via its FlowBinding bridge until traffic is enriched.

## Planned RTDL Flow (from EB topics, local-parity upstream)

The RTDL plane begins where Control & Ingress ends: the Event Bus holds four admitted topics for a fraud-mode run — `fp.bus.traffic.fraud.v1`, `fp.bus.context.arrival_events.v1`, `fp.bus.context.arrival_entities.v1`, and `fp.bus.context.flow_anchor.fraud.v1`. RTDL treats the bus as the sole source of truth for **admitted online events**, and it does so under at-least-once delivery with run-scoped pins. Each consumer group maintains its own per-partition checkpoints, never mutates the bus, and only advances offsets once its downstream side effects are safely committed.

Durability is not assumed to be “only the bus.” The Event Bus is a durable log, but retention is finite and replay windows are bounded. RTDL therefore plans for **explicit archival surfaces**: an **event archive writer** that copies admitted EB events (plus origin_offset metadata and `ContextPins`) into object storage for long-horizon replay, and a **decision/audit store** that writes its own append-only records and payload references into object storage as the authoritative history of decisions and outcomes. This avoids the failure mode where an event ages out of EB retention and becomes unrecoverable. For online processing, EB origin offsets are the evidence boundary; for replay, archive-stored origin offsets and payloads are the evidence boundary. Any mismatch is an audit anomaly and replay fails closed. In local parity this is still a planned capability, but it is treated as necessary for production RTDL, where replay, audit, and training depend on immutable records beyond the bus retention window.

RTDL also depends on **stateful stores** that are distinct from the EB itself: a Context Store for join frames, an Online Feature Store for low-latency aggregates, and (optionally) a Graph projection store. These stores do not replace the EB; they are derived, idempotent projections that can be rebuilt from the archived events when necessary. The key architectural rule is that **the durable truth lives in the bus + archive**, while RTDL state stores are rebuildable and versioned by EB offsets.

At the intake edge, RTDL runs a **bus inlet** per topic. The inlet validates the canonical envelope, enforces `ContextPins` (`run_id`, `scenario_id`, `manifest_fingerprint`, `parameter_hash`; `seed` is required for synthetic runs and optional otherwise), and re-checks basic schema invariants. The inlet also maintains **idempotency guards** keyed by `(run_id, event_class, event_id)` and persists `payload_hash` so replays do not double-apply state and collisions are flagged as audit anomalies. The `eb_topic` is recorded as metadata for audit/replay but is not part of the dedupe key. Every event that passes inlet checks is stamped with the EB **origin_offset** metadata (stream, partition, sequence) and handed to the appropriate RTDL sub-plane.

Three of the four topics are **context streams**, and they are always treated as **state builders**, not decision triggers. The arrival event stream (`arrival_events_5B`) establishes the “skeleton” of a flow: the arrival sequence, event time, merchant identity, and any canonical IDs from the payload. Each arrival event opens or updates a join frame in the **Context Store**, keyed by `(run_id, merchant_id, arrival_seq)`. The arrival entity stream (`s1_arrival_entities_6B`) attaches entity references to that frame (`party_id`, `account_id`, `instrument_id`, `device_id`, IP, etc.). The flow anchor stream (`s3_flow_anchor_with_fraud_6B`) provides the binding from arrival sequence to a `flow_id` and cements the ordering context; the Context Store uses it to complete the frame and mark it “join-ready.” These context updates are append-only, idempotent, and safe under replay because they are keyed by a stable `event_id` and pinned to run scope.

Because traffic does not include `(merchant_id, arrival_seq)`, RTDL maintains a **FlowBinding Index** that maps `flow_id -> (run_id, merchant_id, arrival_seq)` and is run-scoped by `run_id` (lookups are effectively `(run_id, flow_id)`). Only flow_anchor is allowed to create/update bindings. If a different binding appears for the same `(run_id, flow_id)`, RTDL emits an audit anomaly and quarantines the event. FlowBinding writes are atomic with JoinFrame updates; only after both are committed (WAL flushed) does the flow_anchor checkpoint advance.

The traffic stream (`s3_event_stream_with_fraud_6B`) is the **decision trigger**. For each traffic event, RTDL resolves its join context by first looking up the FlowBinding Index with `(run_id, flow_id)`, then fetching the JoinFrame by `(run_id, merchant_id, arrival_seq)`. If the join frame is complete, the event advances immediately. If it is incomplete, RTDL does not guess: it either (a) waits in a bounded **join-wait buffer** for up to `join_wait_budget_ms` (600-900 ms, always <= `decision_deadline_ms=1500`) or (b) escalates to a **degrade policy** that explicitly records missing context as either `context_missing: flow_binding_missing` or `context_missing: join_frame_incomplete`, and produces a safe, explainable action (for example, STEP-UP or QUEUE). In all cases, the decision record preserves the evidence: which context pieces were present, which were missing, and the **origin_offset** values that define the evidence boundary.

In parallel, two RTDL support planes are hydrated **from the same EB**:

1. **Identity & Entity Graph (IEG)** consumes EB context and traffic to project a run-scoped graph view (entities, edges, identifiers). It provides stable, read-only graph queries to the decision fabric. `graph_version` is derived from EB offsets, so any decision can be replayed against the same graph state.

2. **Online Feature Plane (OFP)** consumes EB streams and updates low-latency feature aggregates (counts, windows, last-seen timestamps, distincts) keyed by `ContextPins` + entity keys. OFP does not attempt to interpret decisions; it only maintains memory that can be deterministically recomputed from the bus.

Once a traffic event is join-ready, the **Decision Fabric** executes its stages. Guardrails run first (cheap, deterministic checks), followed by primary model scoring, with optional second-stage enrichment if latency permits. The fabric uses: (a) the joined context frame, (b) OFP features, and (c) IEG queries. It produces a decision package containing the action, reasons, model/policy versions, and a feature snapshot hash. That decision package is written to the **Decision Log & Audit Store (DLA)** as an append-only record tied to EB **origin_offset** values (traffic + flow_anchor + arrival events/entities used), preserving full provenance for replay.

The **Actions Layer** consumes decision packages and applies idempotent side effects (approve, step-up, decline, queue). Each action is keyed by a stable `decision_id` (derived from `event_id + bundle_ref + traffic origin_offset` — i.e., the `origin_offset` of the traffic evidence event) so that retries do not duplicate external effects. Outcomes (success/failure, error codes, timestamps) are recorded as action result events and/or appended back into the audit stream for later labeling and learning.

Throughout the RTDL plane, **availability and correctness are bounded by the EB evidence**. Duplicates arriving at EB do not cause double processing because RTDL applies idempotency at the inlet and in each state updater. If a duplicate is a true replay, RTDL’s state remains unchanged. If an `event_id` collision were to occur (same `event_id`, different payload), RTDL detects a payload hash mismatch and emits an audit anomaly; the policy response is to quarantine or degrade rather than silently proceed.

The net effect is a deterministic, replayable decision loop: context events build join state, traffic events trigger decisions, decisions produce actions, and every step is pinned to EB offsets and `ContextPins` so the system can be replayed or audited end-to-end. This planned flow is the direct continuation of the local-parity Control & Ingress posture and establishes the exact upstream contract RTDL must honor before we finalize its detailed design.

## Planned Case + Labels Flow (human truth loop)

The Case + Labels plane begins once RTDL has produced **decisions** and the Actions Layer (AL) has produced **immutable outcomes**. The authoritative evidence inputs are **DLA audit records**, **AL outcome records**, and the EB **origin_offset evidence basis**—carried **by reference**, not copied payloads. Every item entering this plane remains pinned to ContextPins, including `platform_run_id` and `scenario_run_id`, so downstream truth never leaks across runs.

A thin **CaseTrigger writer** (or RTDL/AL directly) emits explicit `CaseTrigger` events for review-worthy situations: `DECISION_ESCALATION`, `ACTION_FAILURE`, `ANOMALY`, `EXTERNAL_SIGNAL`, or `MANUAL_ASSERTION`. Each CaseTrigger carries ContextPins, a canonical **CaseSubjectKey** of `(platform_run_id, event_class, event_id)`, plus evidence refs such as `decision_id`, `action_outcome_id`, and `audit_record_id`. The trigger is idempotent under at-least-once delivery: `case_id = hash(CaseSubjectKey)` and `case_trigger_id = hash(case_id + trigger_type + source_ref_id)` ensure duplicates attach cleanly without creating duplicate cases.

**Case Management (CM)** consumes CaseTriggers and builds the **case timeline** as the primary case truth object. A case is created once per `case_id` (no merges in v0), and everything thereafter is **append-only timeline events**. Timeline appends are idempotent: each timeline event uses a stable key `(case_id, timeline_event_type, source_ref_id)` and a deterministic `case_timeline_event_id = hash(case_id + timeline_event_type + source_ref_id)`. If a duplicate arrives, CM no-ops; if the same key arrives with different content, CM flags an **anomaly** rather than silently overwriting. CM stores **refs + minimal metadata only**; it does not become a second evidence store and does not reinterpret upstream payload truth.

Investigators work inside CM and **append assertions** (e.g., “confirmed fraud”, “confirmed legitimate”, “needs follow-up”) as new timeline events with explicit `actor_id`, `source_type=HUMAN`, and `observed_time`. Corrections are new assertions—no destructive edits. CM may support assignment/ownership as timeline-derived state, but v0 remains lock-free: concurrent edits are naturally represented as ordered, append-only events.

When a human workflow requires an operational side effect (block, release, notify, queue), CM does not execute it directly. Instead, it emits an **ActionIntent** to AL with ContextPins, subject identifiers, reasons, and evidence refs, using an idempotency key such as `hash(case_id + source_case_event_id + intent_type + subject_key)`. AL executes the intent idempotently and produces an immutable outcome; CM later attaches the resulting `action_outcome_id` to the timeline **by reference**, closing the loop without CM owning side effects.

Label truth is emitted through the **Label Store (LS)**, not inside CM. When an investigation yields a label, CM creates a **LabelAssertion** whose subject is execution-scoped: **LabelSubjectKey = `(platform_run_id, event_id)`**, where `event_id` is the **traffic/decision-trigger** event (not context event ids). The assertion includes `label_type` (v0 controlled vocabulary such as `fraud_disposition`, `chargeback_status`, `account_takeover`), `label_value`, and both **effective_time** (when it was true) and **observed_time** (when it was learned). The assertion id is stable across retries: `label_assertion_id` is derived from `case_timeline_event_id + LabelSubjectKey + label_type`, and `observed_time` is fixed at first creation.

LS owns its own **writer boundary** (IG-equivalent ingress). CM submits the LabelAssertion to LS and records `LABEL_PENDING` on the case timeline. LS enforces idempotency and integrity at the boundary using the dedupe tuple `(LabelSubjectKey, label_type, label_assertion_id)` plus `payload_hash` anomaly detection (same tuple + different hash ⇒ **ANOMALY**). For hashing, LS uses a canonical field set (subject, type, value, effective/observed time, source_type, actor_id if HUMAN, evidence refs) and a canonical ordering of `evidence_refs` (sorted by `(ref_type, ref_id)`). Only after LS durably commits the append (WAL-flushed) does it return an ack/ref; CM then appends `LABEL_ACCEPTED` (or `LABEL_REJECTED`) to the case timeline. If LS is unavailable, CM remains pending and retries idempotently; CM never claims label truth until LS ack succeeds.

LS maintains **append-only label timelines** and provides a resolved view per `(LabelSubjectKey, label_type)` using explicit precedence rules (human > external feed > automated, then observed_time, then assertion_id). Late-arriving truth (e.g., chargebacks weeks later) is accepted as new assertions with preserved effective vs observed time, so historical “what did we know then?” views remain reconstructible.

External truth signals (chargebacks, disputes, confirmed fraud feeds) enter v0 through the **CM workflow first**, then become LabelAssertions under the same contract. A future direct external ingest can write to LS, but must follow the same idempotency and provenance rules.

This closes the human truth loop cleanly: **RTDL evidence → CaseTriggers → CM timelines → LabelAssertions → Label Store timelines → training consumption**, with run-scoped pins, append-only history, and explicit commit/ack semantics preventing leakage or silent reinterpretation.

## Planned Learning + Evolution Flow (offline model loop)

The Learning + Evolution plane begins once the platform has two durable, pinned truths available: **admitted event history** (EB within retention, Archive beyond) and **label truth timelines** (Label Store, with effective_time vs observed_time). It does not invent new truth; it deterministically **rebuilds** and **packages** learning artifacts from pinned inputs. This plane is explicitly offline and job-driven, but it preserves the same rails as upstream: by-ref inputs, ContextPins everywhere, no-PASS-no-read, and immutable artifacts with audit visibility.

The entry vertex is the **Offline Feature Shadow (OFS)**, which runs as an on-demand/scheduled job. OFS is triggered by a build intent (not “always on”), and it resolves only **pinned inputs**: it reads admitted events from EB/Archive using an explicit **replay basis** (offset ranges or time windows anchored to offsets), reads labels from Label Store using an explicit **as-of boundary** (observed_time cutoff), and loads **feature definition versions** from the same authority OFP uses. If OFS needs any world or scenario context, it only obtains those surfaces via SR’s `run_facts_view` and its by-ref locators with PASS evidence; it never scans “latest.” The result of an OFS build is **DatasetManifests** plus materialized artifacts under `ofs/...`, where each manifest pins the replay basis, label as-of boundary, join keys/entity scope, feature version set, and provenance (including any parity anchors if requested).

OFS is allowed to perform **parity rebuilds** against online evidence when asked, but parity is optional and explicit: a parity run starts from an online snapshot anchor or audit provenance (e.g., feature snapshot hash + input basis) and produces a match/mismatch evidence artifact. Even in parity mode, OFS remains deterministic and never rewrites history; any rebuild is a new derived artifact, not a mutation of prior manifests.

The **Model Factory (MF)** consumes OFS outputs via **DatasetManifests only**. MF is a job, triggered by Run/Operate with a training intent and fully pinned inputs (manifest refs + config/profile revisions). MF refuses “latest dataset” or unpinned data. It produces a training run record (train_run_id), evaluation evidence, and a **bundle** if gates PASS. MF is authoritative for its own run evidence and bundle packaging, but it is explicitly **not** an activation authority; it publishes candidates to Registry and can emit governance facts (training started/completed, bundle published) to `fp.bus.control.v1` for auditability.

The **Model/Policy Registry (MPR)** is the controlled source of deployable truth. It receives bundles + evidence from MF, enforces immutability and compatibility metadata, and governs lifecycle actions (approve, promote, rollback, retire). Only MPR determines **ACTIVE** for a given scope; promotion is auditable and produces registry events. When Decision Fabric resolves what to run, it does so deterministically through MPR, which **fails closed** if a candidate is incompatible with required feature versions or the current degrade capabilities mask.

This plane therefore closes the loop without breaking upstream guarantees: **admitted events + label timelines → deterministic offline reconstruction → manifested datasets → reproducible training → governed bundles → deterministic activation**. Every join remains pinned, every artifact is by-ref and immutable, and every promotion is explicit and auditable, so the learning loop can evolve without introducing drift into the real-time plane.
