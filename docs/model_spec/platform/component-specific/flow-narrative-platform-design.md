# Descriptive Narrative of Flow w/regards to Platform
_NOTE: You are only allowed to change the doc under the EXPLICIT instruction from the USER. And if done, ensure edit is woven into the narrative flow of the platform_

## Local-Parity SR->WSP->IG->EB Narrative (`platform_20260201T224449Z`)

In local parity, the platform stack is brought up with LocalStack Kinesis, MinIO (object store), and the supporting services used by Control and Ingress. Kinesis exposes the control and event-bus streams, and MinIO exposes the fraud-platform bucket used for run artifacts and receipts. The run is started by creating a new platform run id and setting it as the active run so downstream components can resolve run-scoped paths.

Scenario Runner (SR) executes against the Oracle Store roots (`ORACLE_ROOT`, `ORACLE_ENGINE_RUN_ROOT`, `ORACLE_STREAM_VIEW_ROOT`) and the interface pack contract. It validates the gate pass, assembles a `run_facts_view` for the selected scenario, and writes that view into the object store at a run-scoped path. SR then emits a READY control event into the control bus (topic `fp.bus.control.v1` on the Kinesis control stream) containing the `run_id`, `manifest_fingerprint`, `parameter_hash`, and other required pins that identify the run and seed the downstream flow.

World Streamer Producer (WSP) runs as the ready consumer. It listens to the control bus and, when it receives the READY event, loads the `run_facts_view` from MinIO and resolves the scenario and stream views. It builds its output plan from the `local_parity` profile: traffic output ids from `config/platform/wsp/traffic_outputs_v0.yaml` (fraud stream by default) and context output ids from `config/platform/wsp/context_fraud_outputs_v0.yaml` (`arrival_events_5B`, `s1_arrival_entities_6B`, `s3_flow_anchor_with_fraud_6B`). The run applies a stream speedup of `600x` and sets output concurrency to `4`, so all four outputs start together. Each output worker iterates its view, constructs the canonical event envelope with `ContextPins` and payload, and POSTs the event to the IG ingest endpoint. The run caps emission at `200` events per output, so each worker emits `200` events before stopping.

Ingestion Gate (IG) receives each POST and performs the admission pipeline. It checks the dedupe key against the admission database to enforce idempotency (`(run_id, topic_or_class, event_id)` with `payload_hash` anomaly detection), validates required pins by class (`traffic_fraud`, `context_arrival`, `context_arrival_entities`, `context_flow_fraud`) as defined in `config/platform/ig/class_map_v0.yaml`, and applies schema policy. If the dedupe key has already been seen, IG logs an IG duplicate and drops the event. If the event is admitted, IG selects the partitioning profile and stream for the event class (for example, `ig.partitioning.v0.traffic.fraud` routes to `fp.bus.traffic.fraud.v1`; `ig.partitioning.v0.context.arrival_events` routes to `fp.bus.context.arrival_events.v1`; `ig.partitioning.v0.context.arrival_entities` routes to `fp.bus.context.arrival_entities.v1`; `ig.partitioning.v0.context.flow_anchor.fraud` routes to `fp.bus.context.flow_anchor.fraud.v1`). The partition key is computed from the `key_precedence` rules in `config/platform/ig/partitioning_profiles_v0.yaml`, which is the source of determinism for context join locality; traffic joins are resolved via FlowBinding until traffic is enriched. IG publishes to LocalStack Kinesis, logs the sequence offset, and writes an admission receipt JSON into MinIO under `s3://fraud-platform/{run_id}/ig/receipts/`. The platform log mirrors the IG publish lines for traceability.

Event Bus (EB) in local parity is Kinesis. For this run, four EB topics carry the admitted stream: `fp.bus.traffic.fraud.v1`, `fp.bus.context.arrival_events.v1`, `fp.bus.context.arrival_entities.v1`, and `fp.bus.context.flow_anchor.fraud.v1`. Because the delivery model is at-least-once, a small number of early retries occurred; IG detected these as duplicates (same `event_id` re-sent `1-2` seconds later) and dropped them. As a result, EB admitted counts reflect unique events, not raw WSP sends. In this run WSP emitted `200` per output, while EB admitted `194` (traffic), `191` (arrival_events), `193` (arrival_entities), and `196` (flow_anchor). This is expected: IG de-duplication protects the EB from replays so downstream consumers see one canonical copy per `event_id`.

Operationally, IG health remained AMBER with `BUS_HEALTH_UNKNOWN` during the run, yet admission and publishing proceeded without interruption. The flow observed in logs is continuous and concurrent: WSP outputs start together, events stream into IG in parallel, and EB receives the four topics without sequential gating between streams. At the end of the run, WSP stops each output after its `200`-event cap, while IG completes any in-flight admissions and final receipts.

This is the implemented Control and Ingress posture that RTDL will consume: SR provides the run facts and READY signal; WSP produces traffic and context streams concurrently; IG validates, deduplicates, and routes by partitioning profile; and EB provides the four admitted topics with deterministic keys derived from the IG partitioning profiles. Context topics are join-local on `(merchant_id, arrival_seq)`; traffic does not carry those fields yet, so RTDL uses a FlowBinding bridge for joins.

---

## Planned RTDL Flow (from EB topics, local-parity upstream)

The RTDL plane begins where Control & Ingress ends: the Event Bus holds four admitted topics for a fraud-mode run — `fp.bus.traffic.fraud.v1`, `fp.bus.context.arrival_events.v1`, `fp.bus.context.arrival_entities.v1`, and `fp.bus.context.flow_anchor.fraud.v1`. RTDL treats the bus as the sole source of truth for **admitted online events**, and it does so under at-least-once delivery with run-scoped pins. Each consumer group maintains its own per-partition checkpoints, never mutates the bus, and only advances offsets once its downstream side effects are safely committed.

Durability is not assumed to be “only the bus.” The Event Bus is a durable log, but retention is finite and replay windows are bounded. RTDL therefore plans for **explicit archival surfaces**: an **event archive writer** that copies admitted EB events (plus origin_offset metadata and `ContextPins`) into object storage for long-horizon replay, and a **decision/audit store** that writes its own append-only records and payload references into object storage as the authoritative history of decisions and outcomes. This avoids the failure mode where an event ages out of EB retention and becomes unrecoverable. For online processing, EB origin offsets are the evidence boundary; for replay, archive-stored origin offsets and payloads are the evidence boundary. Any mismatch is an audit anomaly and replay fails closed. In local parity this is still a planned capability, but it is treated as necessary for production RTDL, where replay, audit, and training depend on immutable records beyond the bus retention window.

RTDL also depends on **stateful stores** that are distinct from the EB itself: a Context Store for join frames, an Online Feature Store for low-latency aggregates, and (optionally) a Graph projection store. These stores do not replace the EB; they are derived, idempotent projections that can be rebuilt from the archived events when necessary. The key architectural rule is that **the durable truth lives in the bus + archive**, while RTDL state stores are rebuildable and versioned by EB offsets.

At the intake edge, RTDL runs a **bus inlet** per topic. The inlet validates the canonical envelope, enforces `ContextPins` (`run_id`, `scenario_id`, `manifest_fingerprint`, `parameter_hash`; `seed` is required for synthetic runs and optional otherwise), and re-checks basic schema invariants. The inlet also maintains **idempotency guards** keyed by `(run_id, topic_or_class, event_id)` and persists `payload_hash` so replays do not double-apply state and collisions are flagged as audit anomalies. Every event that passes inlet checks is stamped with the EB **origin_offset** metadata (stream, partition, sequence) and handed to the appropriate RTDL sub-plane.

Three of the four topics are **context streams**, and they are always treated as **state builders**, not decision triggers. The arrival event stream (`arrival_events_5B`) establishes the “skeleton” of a flow: the arrival sequence, event time, merchant identity, and any canonical IDs from the payload. Each arrival event opens or updates a join frame in the **Context Store**, keyed by `(run_id, merchant_id, arrival_seq)`. The arrival entity stream (`s1_arrival_entities_6B`) attaches entity references to that frame (`party_id`, `account_id`, `instrument_id`, `device_id`, IP, etc.). The flow anchor stream (`s3_flow_anchor_with_fraud_6B`) provides the binding from arrival sequence to a `flow_id` and cements the ordering context; the Context Store uses it to complete the frame and mark it “join-ready.” These context updates are append-only, idempotent, and safe under replay because they are keyed by a stable `event_id` and pinned to run scope.

Because traffic does not include `(merchant_id, arrival_seq)`, RTDL maintains a **FlowBinding Index** that maps `flow_id -> (run_id, merchant_id, arrival_seq)` and is run-scoped by `run_id` (lookups are effectively `(run_id, flow_id)`). Only flow_anchor is allowed to create/update bindings. If a different binding appears for the same `(run_id, flow_id)`, RTDL emits an audit anomaly and quarantines the event. FlowBinding writes are atomic with JoinFrame updates; only after both are committed (WAL flushed) does the flow_anchor checkpoint advance.

The traffic stream (`s3_event_stream_with_fraud_6B`) is the **decision trigger**. For each traffic event, RTDL resolves its join context by first looking up the FlowBinding Index with `flow_id`, then fetching the JoinFrame by `(run_id, merchant_id, arrival_seq)`. If the join frame is complete, the event advances immediately. If it is incomplete, RTDL does not guess: it either (a) waits in a bounded **join-wait buffer** for up to `join_wait_budget_ms` (600-900 ms, always <= `decision_deadline_ms=1500`) or (b) escalates to a **degrade policy** that explicitly records missing context as either `context_missing: flow_binding_missing` or `context_missing: join_frame_incomplete`, and produces a safe, explainable action (for example, STEP-UP or QUEUE). In all cases, the decision record preserves the evidence: which context pieces were present, which were missing, and the **origin_offset** values that define the evidence boundary.

In parallel, two RTDL support planes are hydrated **from the same EB**:

1. **Identity & Entity Graph (IEG)** consumes EB context and traffic to project a run-scoped graph view (entities, edges, identifiers). It provides stable, read-only graph queries to the decision fabric. `graph_version` is derived from EB offsets, so any decision can be replayed against the same graph state.

2. **Online Feature Plane (OFP)** consumes EB streams and updates low-latency feature aggregates (counts, windows, last-seen timestamps, distincts) keyed by `ContextPins` + entity keys. OFP does not attempt to interpret decisions; it only maintains memory that can be deterministically recomputed from the bus.

Once a traffic event is join-ready, the **Decision Fabric** executes its stages. Guardrails run first (cheap, deterministic checks), followed by primary model scoring, with optional second-stage enrichment if latency permits. The fabric uses: (a) the joined context frame, (b) OFP features, and (c) IEG queries. It produces a decision package containing the action, reasons, model/policy versions, and a feature snapshot hash. That decision package is written to the **Decision Log & Audit Store (DLA)** as an append-only record tied to EB **origin_offset** values (traffic + flow_anchor + arrival events/entities used), preserving full provenance for replay.

The **Actions Layer** consumes decision packages and applies idempotent side effects (approve, step-up, decline, queue). Each action is keyed by a stable `decision_id` (derived from `event_id + bundle_ref + origin_offset`) so that retries do not duplicate external effects. Outcomes (success/failure, error codes, timestamps) are recorded as action result events and/or appended back into the audit stream for later labeling and learning.

Throughout the RTDL plane, **availability and correctness are bounded by the EB evidence**. Duplicates arriving at EB do not cause double processing because RTDL applies idempotency at the inlet and in each state updater. If a duplicate is a true replay, RTDL’s state remains unchanged. If an `event_id` collision were to occur (same `event_id`, different payload), RTDL detects a payload hash mismatch and emits an audit anomaly; the policy response is to quarantine or degrade rather than silently proceed.

The net effect is a deterministic, replayable decision loop: context events build join state, traffic events trigger decisions, decisions produce actions, and every step is pinned to EB offsets and `ContextPins` so the system can be replayed or audited end-to-end. This planned flow is the direct continuation of the local-parity Control & Ingress posture and establishes the exact upstream contract RTDL must honor before we finalize its detailed design.
