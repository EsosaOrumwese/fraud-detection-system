# Descriptive Narrative of Flow w/regards to Platform
_NOTE to AGENT: You are only allowed to change the doc under the EXPLICIT instruction from the USER. And if done, ensure edit is woven into the narrative flow of the platform_

## Local-Parity SR->WSP->IG->EB Narrative (`platform_20260201T224449Z`) — v0 pins surfaced

In local parity, the platform stack is brought up with LocalStack Kinesis, MinIO (object store), and the supporting services used by Control and Ingress. Kinesis exposes the control and event-bus streams, and MinIO exposes the fraud-platform bucket used for run artifacts and receipts. The run begins by generating a new **`platform_run_id`** and writing it as the active platform run (e.g., `runs/fraud-platform/ACTIVE_RUN_ID`) so downstream components resolve **platform-run-scoped** paths deterministically.

Scenario Runner (SR) executes against the Oracle Store roots (`ORACLE_ROOT`, `ORACLE_ENGINE_RUN_ROOT`, `ORACLE_STREAM_VIEW_ROOT`) and the interface pack contract. SR resolves a deterministic **`scenario_run_id`** (from the run equivalence key), validates the gate pass, assembles a `run_facts_view` for the selected scenario, and writes that view into the object store at a platform-run-scoped path (write-once; drift is rejected). SR then emits a READY control event into the control bus (topic `fp.bus.control.v1` on the Kinesis control stream) containing **both** `platform_run_id` and `scenario_run_id`, plus `manifest_fingerprint`, `parameter_hash`, `bundle_hash` (or plan hash), and other required pins. The READY event is idempotent: its `message_id` is derived from `platform_run_id + scenario_run_id + bundle_hash/plan_hash`, and consumers treat duplicates as no-ops.

World Streamer Producer (WSP) runs as the ready consumer. It listens to the control bus and, when it receives a READY event, dedupes by READY `message_id`, loads the `run_facts_view` from MinIO, and validates that the `scenario_run_id` in READY matches the facts view it loads. It builds its output plan from the `local_parity` profile: traffic output ids from `config/platform/wsp/traffic_outputs_v0.yaml` (fraud stream by default) and context output ids from `config/platform/wsp/context_fraud_outputs_v0.yaml` (`arrival_events_5B`, `s1_arrival_entities_6B`, `s3_flow_anchor_with_fraud_6B`). The run applies a stream speedup of `600x` (_this is a variable, where `0x` causes the events to flow according to the realistic `ts_utc` time_) and sets output concurrency to `4`, so all four outputs start together. Each output worker iterates its view, constructs the canonical event envelope with `ContextPins` (including `platform_run_id` and `scenario_run_id`) and payload, derives a deterministic `event_id`, and POSTs the event to the IG ingest endpoint. Under v0 pins, transient 429/5xx/timeouts are retried with bounded exponential backoff using the **same** `event_id`; schema/policy 4xx are treated as non-retryable and stop the stream with an explicit surfaced reason. The run caps emission at `200` events per output, so each worker emits `200` events before stopping (_this is a variable and can be adjusted or ignored to allow a full run_).

Ingestion Gate (IG) receives each POST and performs the admission pipeline. It validates required pins by class (`traffic_fraud`, `context_arrival`, `context_arrival_entities`, `context_flow_fraud`) as defined in `config/platform/ig/class_map_v0.yaml`, applies schema policy, and resolves the canonical `event_class` (aligned to `class_map_v0.yaml` and stable across topic/version changes). IG computes the dedupe tuple **`(platform_run_id, event_class, event_id)`** and persists `payload_hash` for anomaly detection (`eb_topic` is recorded as metadata, not part of the dedupe key). If the same dedupe tuple is seen again with the same `payload_hash`, IG logs a DUPLICATE and drops the event; if the dedupe tuple matches but `payload_hash` differs, IG raises an anomaly and quarantines (never silent replace). For new events, IG records an admission row in state `PUBLISH_IN_FLIGHT`, then selects the partitioning profile and stream for the event class (for example, `ig.partitioning.v0.traffic.fraud` routes to `fp.bus.traffic.fraud.v1`; `ig.partitioning.v0.context.arrival_events` routes to `fp.bus.context.arrival_events.v1`; `ig.partitioning.v0.context.arrival_entities` routes to `fp.bus.context.arrival_entities.v1`; `ig.partitioning.v0.context.flow_anchor.fraud` routes to `fp.bus.context.flow_anchor.fraud.v1`). The partition key is computed from the `key_precedence` rules in `config/platform/ig/partitioning_profiles_v0.yaml` (v0 claim: context locality is by `merchant_id`; `arrival_seq` participates in JoinFrameKey downstream, not partitioning). IG publishes to LocalStack Kinesis and updates the admission row to `ADMITTED` with `eb_ref` on success; on timeout/unknown publish outcome it records `PUBLISH_AMBIGUOUS` and does not blindly re-publish without reconciliation. IG writes an admission receipt JSON into MinIO under `s3://fraud-platform/{platform_run_id}/ig/receipts/` for ADMIT/DUPLICATE/QUARANTINE. ADMIT receipts are written after publish success and include `platform_run_id`, `scenario_run_id`, `event_class`, `payload_hash`, `admitted_at_utc`, and `eb_ref`; if receipt writing fails after publish, IG preserves `eb_ref + payload_hash` in the admission DB and marks `receipt_write_failed` for backfill so EB evidence is not lost.

Event Bus (EB) in local parity is Kinesis. For this run, four EB topics carry the admitted stream: `fp.bus.traffic.fraud.v1`, `fp.bus.context.arrival_events.v1`, `fp.bus.context.arrival_entities.v1`, and `fp.bus.context.flow_anchor.fraud.v1`. Because the delivery model is at-least-once, a small number of early retries occurred; IG detected these as duplicates (same dedupe tuple `(platform_run_id, event_class, event_id)` re-sent shortly after) and dropped them. As a result, EB admitted counts reflect unique events, not raw WSP sends. In this run WSP emitted `200` per output, while EB admitted `194` (traffic), `191` (arrival_events), `193` (arrival_entities), and `196` (flow_anchor). This is expected: IG de-duplication protects the EB from replays so downstream consumers see one canonical copy per dedupe tuple.

Operationally, IG health remained AMBER with `BUS_HEALTH_UNKNOWN` during the run, yet admission and publishing proceeded without interruption. The flow observed in logs is continuous and concurrent: WSP outputs start together, events stream into IG in parallel, and EB receives the four topics without sequential gating between streams. At the end of the run, WSP stops each output after its `200`-event cap, while IG completes any in-flight admissions and final receipts/backfill markers.

This is the implemented Control and Ingress posture that RTDL will consume: SR provides run facts plus an idempotent READY signal carrying both `platform_run_id` and `scenario_run_id`; WSP produces traffic and context streams concurrently with deterministic `event_id`s and bounded retries; IG validates, deduplicates using `(platform_run_id, event_class, event_id)` with `payload_hash` anomaly detection, routes by partitioning profile, and preserves EB evidence even under receipt/write faults; and EB provides the four admitted topics with deterministic keys derived from IG partitioning profiles. Context topics are merchant-local by partitioning, while traffic does not carry `(merchant_id, arrival_seq)` yet, so RTDL resolves joins via its FlowBinding bridge until traffic is enriched.

## Planned RTDL Flow (from EB topics, local-parity upstream)

The RTDL plane begins where Control & Ingress ends: the Event Bus holds four admitted topics for a fraud-mode run — `fp.bus.traffic.fraud.v1`, `fp.bus.context.arrival_events.v1`, `fp.bus.context.arrival_entities.v1`, and `fp.bus.context.flow_anchor.fraud.v1`. RTDL treats the bus as the sole source of truth for **admitted online events**, and it does so under at-least-once delivery with run-scoped pins. Each consumer group maintains its own per-partition checkpoints, never mutates the bus, and only advances offsets once its downstream side effects are safely committed.

Durability is not assumed to be “only the bus.” The Event Bus is a durable log, but retention is finite and replay windows are bounded. RTDL therefore plans for **explicit archival surfaces**: an **event archive writer** that copies admitted EB events (plus origin_offset metadata and `ContextPins`) into object storage for long-horizon replay, and a **decision/audit store** that writes its own append-only records and payload references into object storage as the authoritative history of decisions and outcomes. This avoids the failure mode where an event ages out of EB retention and becomes unrecoverable. For online processing, EB origin offsets are the evidence boundary; for replay, archive-stored origin offsets and payloads are the evidence boundary. Any mismatch is an audit anomaly and replay fails closed. In local parity this is still a planned capability, but it is treated as necessary for production RTDL, where replay, audit, and training depend on immutable records beyond the bus retention window.

RTDL also depends on **stateful stores** that are distinct from the EB itself: a Context Store for join frames, an Online Feature Store for low-latency aggregates, and (optionally) a Graph projection store. These stores do not replace the EB; they are derived, idempotent projections that can be rebuilt from the archived events when necessary. The key architectural rule is that **the durable truth lives in the bus + archive**, while RTDL state stores are rebuildable and versioned by EB offsets.

At the intake edge, RTDL runs a **bus inlet** per topic. The inlet validates the canonical envelope, enforces `ContextPins` (`run_id`, `scenario_id`, `manifest_fingerprint`, `parameter_hash`; `seed` is required for synthetic runs and optional otherwise), and re-checks basic schema invariants. The inlet also maintains **idempotency guards** keyed by `(run_id, event_class, event_id)` and persists `payload_hash` so replays do not double-apply state and collisions are flagged as audit anomalies. The `eb_topic` is recorded as metadata for audit/replay but is not part of the dedupe key. Every event that passes inlet checks is stamped with the EB **origin_offset** metadata (stream, partition, sequence) and handed to the appropriate RTDL sub-plane.

Three of the four topics are **context streams**, and they are always treated as **state builders**, not decision triggers. The arrival event stream (`arrival_events_5B`) establishes the “skeleton” of a flow: the arrival sequence, event time, merchant identity, and any canonical IDs from the payload. Each arrival event opens or updates a join frame in the **Context Store**, keyed by `(run_id, merchant_id, arrival_seq)`. The arrival entity stream (`s1_arrival_entities_6B`) attaches entity references to that frame (`party_id`, `account_id`, `instrument_id`, `device_id`, IP, etc.). The flow anchor stream (`s3_flow_anchor_with_fraud_6B`) provides the binding from arrival sequence to a `flow_id` and cements the ordering context; the Context Store uses it to complete the frame and mark it “join-ready.” These context updates are append-only, idempotent, and safe under replay because they are keyed by a stable `event_id` and pinned to run scope.

Because traffic does not include `(merchant_id, arrival_seq)`, RTDL maintains a **FlowBinding Index** that maps `flow_id -> (run_id, merchant_id, arrival_seq)` and is run-scoped by `run_id` (lookups are effectively `(run_id, flow_id)`). Only flow_anchor is allowed to create/update bindings. If a different binding appears for the same `(run_id, flow_id)`, RTDL emits an audit anomaly and quarantines the event. FlowBinding writes are atomic with JoinFrame updates; only after both are committed (WAL flushed) does the flow_anchor checkpoint advance.

The traffic stream (`s3_event_stream_with_fraud_6B`) is the **decision trigger**. For each traffic event, RTDL resolves its join context by first looking up the FlowBinding Index with `(run_id, flow_id)`, then fetching the JoinFrame by `(run_id, merchant_id, arrival_seq)`. If the join frame is complete, the event advances immediately. If it is incomplete, RTDL does not guess: it either (a) waits in a bounded **join-wait buffer** for up to `join_wait_budget_ms` (600-900 ms, always <= `decision_deadline_ms=1500`) or (b) escalates to a **degrade policy** that explicitly records missing context as either `context_missing: flow_binding_missing` or `context_missing: join_frame_incomplete`, and produces a safe, explainable action (for example, STEP-UP or QUEUE). In all cases, the decision record preserves the evidence: which context pieces were present, which were missing, and the **origin_offset** values that define the evidence boundary.

In parallel, two RTDL support planes are hydrated **from the same EB**:

1. **Identity & Entity Graph (IEG)** consumes EB context and traffic to project a run-scoped graph view (entities, edges, identifiers). It provides stable, read-only graph queries to the decision fabric. `graph_version` is derived from EB offsets, so any decision can be replayed against the same graph state.

2. **Online Feature Plane (OFP)** consumes EB streams and updates low-latency feature aggregates (counts, windows, last-seen timestamps, distincts) keyed by `ContextPins` + entity keys. OFP does not attempt to interpret decisions; it only maintains memory that can be deterministically recomputed from the bus.

Once a traffic event is join-ready, the **Decision Fabric** executes its stages. Guardrails run first (cheap, deterministic checks), followed by primary model scoring, with optional second-stage enrichment if latency permits. The fabric uses: (a) the joined context frame, (b) OFP features, and (c) IEG queries. It produces a decision package containing the action, reasons, model/policy versions, and a feature snapshot hash. That decision package is written to the **Decision Log & Audit Store (DLA)** as an append-only record tied to EB **origin_offset** values (traffic + flow_anchor + arrival events/entities used), preserving full provenance for replay.

The **Actions Layer** consumes decision packages and applies idempotent side effects (approve, step-up, decline, queue). Each action is keyed by a stable `decision_id` (derived from `event_id + bundle_ref + traffic origin_offset` — i.e., the `origin_offset` of the traffic evidence event) so that retries do not duplicate external effects. Outcomes (success/failure, error codes, timestamps) are recorded as action result events and/or appended back into the audit stream for later labeling and learning.

Throughout the RTDL plane, **availability and correctness are bounded by the EB evidence**. Duplicates arriving at EB do not cause double processing because RTDL applies idempotency at the inlet and in each state updater. If a duplicate is a true replay, RTDL’s state remains unchanged. If an `event_id` collision were to occur (same `event_id`, different payload), RTDL detects a payload hash mismatch and emits an audit anomaly; the policy response is to quarantine or degrade rather than silently proceed.

The net effect is a deterministic, replayable decision loop: context events build join state, traffic events trigger decisions, decisions produce actions, and every step is pinned to EB offsets and `ContextPins` so the system can be replayed or audited end-to-end. This planned flow is the direct continuation of the local-parity Control & Ingress posture and establishes the exact upstream contract RTDL must honor before we finalize its detailed design.
