Reproducibility in this pipeline is not a decorative appendix—it is the load‑bearing spine that lets every auditor trace a single row of synthetic data all the way back to the exact bytes of code, configuration and third‑party artefacts that shaped it. The sub‑segment called **“Reproducibility and configurability”** therefore sits at a deeper stratum than the geography or temporal engines: it binds those engines inside a cryptographically signed time‑capsule, while still giving developers the freedom to adjust parameters through governed YAML sheets. What follows is the uncompressed technical anatomy of that capsule, explained in the same vein as the earlier site‑routing narrative—no shorthand, no step omitted.&#x20;

---

The build process launches inside a container whose base image has a **sha256 digest pinned in `Dockerfile.lock`**; the `pipeline_launcher.sh` script reads that digest, the container’s hostname, and the UTC start timestamp, and writes the trio as the first three lines of a *live manifest* file in `/tmp/build.manifest`. The very next action is to fingerprint the generator’s source code: `git rev-parse --verify HEAD` yields a forty‑character SHA‑1 tree hash. That hash goes to the fourth line of the manifest; from here on any uncommitted change to the repository would alter the hash and break the reproducibility invariant.

With source code frozen, `artefact_loader.py` enumerates **every artefact path** declared in `artefact_registry.yaml`. This registry contains a top‑level list of absolute POSIX paths and a `license_map` section pointing each artefact to the full‑text licence file copied into the `LICENSES/` directory. The traversal order is deterministic—lexicographic on path—and each file is streamed into `sha256sum`. The output line `digest  path` is appended to the manifest, but the real purpose is to feed the digest bytes into an **incremental SHA‑256 accumulator**. After the final artefact is processed the accumulator yields a 256‑bit value: the **parameter‑set hash**. That hash is then hex‑encoded and used in three places: (1) as a suffix of the dataset’s root directory name (`synthetic_v1_<hash>`), (2) as the comment string in every Parquet schema (`creator_param_hash=<hash>`), and (3) as a positional argument to the random‑seed generator.

The generator’s random numbers come from NumPy’s **Philox 2¹²⁸ + AES‑round** counter PRNG. The *master seed* is a 128‑bit integer formed by left‑shifting the high‑resolution timestamp in nanoseconds by 64 bits and then XOR‑ing that result with the low 128 bits of the parameter‑set hash. That seed is written into line 5 of the manifest before any stochastic step occurs. Each module—hurdle site counts, coordinate sampler, Dirichlet splitter, LGCP arrival engine—declares a canonical *stream ID* equal to the SHA‑1 of its fully qualified module name. When a module begins it calls

```python
rng_state = np.random.Philox(master_seed)
rng_state._jump(stream_id)
```

moving its counter forward by `stream_id` blocks; since the counter space is 2¹²⁸, collision is practically impossible. Modules that require many logically separate sub‑streams (e.g. one per merchant) call `rng_state._jump(merchant_id_hash)` inside their loop; each jump is recorded in `rng_trace.log` with the tuple `(module_name, merchant_id, jump_offset)`. If a reviewer wants to reproduce exactly the coordinate draw for merchant 42, they can read `rng_trace.log`, create a Philox RNG seeded with `master_seed`, jump directly to that offset, and start sampling.

Configuration flexibility is channelled exclusively through YAML—there are **no numeric constants** hard‑coded in Python except for physical constants such as 60 seconds per minute. Each YAML begins with a three‑header block:

```yaml
schema: "jp.fraudsim.hurdle_coefficients@2"
version: "3.1.4"
released: "2025-06-30"
```

The `schema` string identifies which JSON Schema the loader must validate against; the integer after the `@` represents the major revision. The build refuses to load a YAML whose schema major exceeds the generator’s expected value, thereby coercing maintainers to upgrade code before ingesting breaking changes. Every YAML entry that represents a statistical estimate carries three additional keys: `mean`, `ci_lower`, `ci_upper`—the 90 % confidence interval harvested from the notebook that fit the model. The validation harness later draws 100 bootstrap replicates per coefficient from a truncated normal anchored at `mean` and clips simulated counts; if the realised synthetic histogram ever falls outside the 95 % predictive envelope, the bootstrap fails and CI marks the coefficient YAML as needing retune.

Before any heavy work starts the build performs a **collision audit**. It queries the internal dataset registry (a Postgres catalog with table `datasets(id, parameter_hash, seed, path)`). If the registry already holds a row with the same `(seed, parameter_hash)` but whose `path` differs from the one the build intends to create, an exception is thrown: someone is attempting to remint a dataset under identical randomness but new semantics. The operator must explicitly increment the semantic version field of at least one YAML file to proceed, ensuring downstream teams notice the change.

As records stream out of the generator they first pass a **structural firewall**. Every row must satisfy: finite latitude or `ip_latitude` depending on `is_virtual`; `tzid` string present in zoneinfo version pinned by manifest; consistency between `event_time_utc` and `local_time_offset` as computed by `zoneinfo.ZoneInfo`; and correct `dst_adjusted` and `fold` flags at DST transitions. The firewall is wired using Pandas vectorised checks; the first row that fails is written to `/tmp/failure_reproducer.py` together with the RNG jump offset that produced it. CI stops immediately and surfaces the reproducer script in its artefacts tab.

Upon structural pass, the **geospatial conformance audit** aggregates the site table by `(merchant_country, tzid)`, derives empirical zone shares, and overlays them on beta‑posterior mean ribbons computed from `country_zone_alphas.yaml`. Wilson score intervals form grey ribbons; the empirical line must stay inside every ribbon with margin 0.1 percentage‑points. Because Dirichlet prior → multinomial posterior → beta marginal is exact conjugacy, the theoretical bound is sharp; a breach means the zone sampler or bump rule drifted.

An **outlet‑count bootstrap** reconstructs the two‑stage hurdle model from `hurdle_coefficients.yaml`. Ten thousand bootstrap replicates of the model produce a predictive envelope for chain‑size histogram; the synthetic counts go on top. If any bucket sits more than one replicate outside 95 % limits, the build fails and plots the offending histogram into the CI artefact bundle.

The **footfall–throughput regression** operates site‑wise. Hourly legitimate transaction counts are regressed on the natural log of the footfall scalar using a Poisson GLM with spline basis for hour‑of‑day and random intercepts for `(merchant_id, date)`. Over‑dispersion parameter θ must land in \[1,2] for card‑present channels and \[2,4] for CNP. If θ misses the band, LGCP variance σ² is flagged via a YAML “tune me” label.

For **temporal multivariate realism** the harness embeds each transaction into ℝ⁶: sine/cosine of hour, sine/cosine of day‑of‑week, latitude and longitude (unit sphere). A 200 000‑row sample—half real, half synthetic—feeds into XGBoost’s binary classifier seeded from a Philox branch. If AUROC exceeds 0.55, the dataset is rejected on the grounds it is too easy to distinguish from reality. Because the XGBoost seed is inside the same RNG tree, rerunning the test with identical seed reproduces the AUROC exactly.

Daylight‑saving fidelity is tested by the **DST edge passer**, which enumerates each site subject to DST, constructs a 48‑hour synthetic wafer around both spring leap and autumn fold for each simulation year, and walks minute‑by‑minute expectations: no missing legal minutes, no present illegal minutes, both folds appear. Failure prints `site_id`, `tzid`, offending minute and a pointer to RNG offset that built the record.

All validation artefacts—raw CSVs, PNGs, GLM coefficient tables—are written under `validation/<parameter_hash>/`. Finally, `upload_to_hashgate.py` posts the manifest JSON, the `validation_passed=true` flag, and the artefacts URL to the internal Flask service **HashGate**. Pull‑requests that ship a new synthetic dataset must include the HashGate URI; the model‑risk board’s GitHub Action polls the URI and refuses merge until the flag reads **true**.

Once the merge occurs, the dataset’s directory—whose name already embeds `parameter_hash`—is mounted read‑only on the shared NFS. Attempting to regenerate the same seed with a different parameter hash yields a directory‑name collision, forcing the developer to bump semantic versions, thereby broadcasting change.

Through parameter‑set hashing, stream‑jump determinism, versioned YAML guarded by schema checks, bootstrap envelopes that keep statistical promises honest, adversarial AUROC tests that probe joint distribution realism, and automation that refuses silent collisions, the *Reproducibility and configurability* layer ensures the synthetic catalogue is never ambiguous, never silently mutating, and always escorted by the forensic paperwork required for JP Morgan’s most hostile audits.
