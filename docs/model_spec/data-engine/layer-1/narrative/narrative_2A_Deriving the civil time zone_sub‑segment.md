The material that follows is not a synopsis; it is a verbatim exposure of every premise, data‑source dependency, numerical convention and protective rail that underpins the **“Deriving the civil time zone”** stage. The language is continuous and discursive so that the implementer can translate it line‑for‑line into code without needing to infer anything. All references to artefacts—whether shapefiles, YAML registries or version strings—are the literal filenames used in the build tree, and every safeguard is identified by the exact condition that triggers it.

---

The stage begins by opening the **tz‑world shapefile** whose basename is `tz_world_2025a.shp`. **The shapefile and its companion resources reside under `artefacts/priors/tz_world/2025a/` including `tz_world_2025a.shp`, `tz_world_2025a.shx`, `tz_world_2025a.dbf`, `tz_world_2025a.prj`, and `tz_world_2025a.cpg`; each file’s SHA‑256 digest (and optional semver) is declared in `spatial_manifest.json` under keys `tz_world_shp_digest`, `tz_world_shx_digest`, etc., ensuring complete provenance for the entire layer.** Its SHA‑256 digest is embedded in the catalogue manifest, under the key `tz_polygon_digest`. The file’s coordinate reference system is EPSG:4326; the build refuses to continue if `fiona.open()` reports anything else. Every polygon record’s attribute table contains the column `TZID`. Immediately after loading, the code populates an **STR‑tree spatial index** with each polygon’s bounding box; the index’s construction is deterministic because the polygons are inserted in lexicographic order of `TZID`. Determinism of the index matters because the order in which bounding‐box enlargement occurs inside the STR‑tree algorithm affects its shape and therefore its traversal path later. **An audit routine serialises the constructed STR‑tree using Python 3.10’s pickle protocol 5, computes a SHA‑256 digest of the resulting byte stream, and stores it in the manifest under the key `tz_index_digest`, ensuring consistent hashing across environments and mitigating MD5 collision risks.** A re‑run with the same shapefile and seed produces the identical digest, proving that the spatial index itself is reproducible.

When an outlet coordinate is fed into the engine, the STR‑tree is queried for candidate polygons. The shortlist is traversed in the order returned by the index; for each polygon the engine calls `prepared_polygon.contains(point)` from the *Shapely* library. If exactly one polygon claims the point the `TZID` from that polygon becomes the site’s provisional civil time zone. If zero polygons claim the point, the engine raises `TimeZoneLookupError` with the offending latitude, longitude and the `prior_tag` that generated the coordinate; the CI harness intercepts the exception, marks the artefact as defective and halts the build. If two polygons claim the point—a circumstance that occurs along borders where vertex precision is lower than floating‑point representation—the engine performs a deterministic tie‑break. Let the two polygons be $P_1$ and $P_2$ with areas $A_1$ and $A_2$. The algorithm computes the vector from the point $x$ to the centroid of the smaller polygon, $c = \operatorname{centroid}(\arg\min\{A_1, A_2\})$. It normalises this vector to unit length, multiplies it by a scalar ε where **ε = 0.0001 degrees**, then adds the displacement to $x$ producing $x' = x + ε\frac{c-x}{\|c-x\|}$. **The value ε is defined in `config/timezone/tz_nudge.yml`, which carries fields `semver`, `sha256_digest`, and `nudge_distance_degrees: 0.0001`; its digest is recorded in the manifest as `tz_nudge_digest`, and any change to ε requires a semver bump.** Because the smallest strip of land in tz‑world exceeds one kilometre in width, a 0.0001° nudge (\~11 m at the equator) almost always yields a single owner. In the rare event that two polygons still claim the nudged point, the engine raises `DSTLookupTieError` (site\_id, x, x', TZIDs) and aborts, ensuring implementers handle this boundary case explicitly. The updated coordinate $x'$ becomes the query point, and the lookup repeats; by construction it yields a single owner. The nudge vector is stored in the site catalogue columns `nudge_lat` and `nudge_lon` so that a forensic examiner can replicate the calculation with independent software.

Tz‑world polygons, however comprehensive, lag behind political decrees. All deviations are centralised in the file **`config/timezone/tz_overrides.yaml`**. **The override registry `config/timezone/tz_overrides.yaml` is a governed artefact with metadata fields `semver` and `sha256_digest`; it must list each override entry (scope, tzid, evidence\_url, expiry\_yyyy\_mm\_dd), and any modification triggers an update to `tz_overrides_digest` in the manifest.** Each override item is a mapping with fields: `scope`, `tzid`, `evidence_url`, and `expiry_yyyy_mm_dd`. The scope can be one of three forms: the string `"country:CA"` meaning the entire ISO country, the pattern `"mcc:5411"` meaning every grocery outlet, or the tuple `("merchant_id", "site_id")` targeting an individual site. A `pre‑commit` Git hook verifies that `evidence_url` is a valid URL and that `expiry_yyyy_mm_dd` is parsable. During the lookup phase the engine checks overrides in precedence order: site‑specific first, then MCC‑wide, then country‑wide. If an override matches, its `TZID` supersedes the polygon result. Nightly CI opens the fresh site catalogue, walks through every row, reapplies the override logic and asserts that at least one override location would change if the shapefile alone were used; a count of zero triggers a rejection because it proves the override is obsolete.

With a final `TZID` attached, the engine consults the **IANA zoneinfo database** version `"tzdata2025a"`. **The tzdata archive `artefacts/priors/tzdata/tzdata2025a.tar.gz` is declared with a `semver` field in `zoneinfo_version.yml` and its SHA‑256 recorded as `tzdata_archive_digest` in the manifest, guaranteeing the exact rule set used for timeline extraction.** Zoneinfo is queried through the standard `zoneinfo.ZoneInfo` API. For each distinct `TZID` present in the catalogue the engine enumerates its `_utc_transition_times` list, intersects it with the simulation horizon and writes a compact two‑column timetable `(transition_epoch, offset_minutes)`. The timetable is stored in memory as two NumPy arrays of dtype `int64` and `int16` respectively, then compressed with run‑length encoding before being attached to the `tz_cache` singleton as Python bytes. Run‑length encoding is adopted because zones that abolished DST decades ago consist chiefly of repeated offsets, and encoding each repeated segment as `(count, value)` pairs reduces average size to about 1 kB per zone. A memory gauge after construction writes the exact byte size of the entire cache into the manifest; if a future upgrade to the IANA file inadvertently triples memory, CI surfaces the expansion where a silent leak could otherwise go unnoticed.

When the temporal engine later generates a **local epoch second** $t_{\text{local}}$, the civil‑time module must map it to UTC. The timetable for the site’s zone is bisection‑searched (via `numpy.searchsorted`) to locate the most recent transition epoch not later than $t_{\text{local}}$. The associated offset $o$ in minutes is read; the provisional UTC epoch is `event_time_utc` = `floor((t_local - 60*o) * 1000)`  (`int64`, milliseconds since epoch)


If $t_{\text{local}}$ lies strictly between a transition epoch and that epoch plus the forward gap length, it is illegal. **Here, $\Delta$ is defined as the difference between the post‑transition and pre‑transition UTC offsets in seconds (i.e. $\Delta = (o_{i+1} - o_i)\times60$), computed directly from the tzdata rule set for that transition.** The engine replaces it by $t_{\text{legal}} = t_i + \Delta$, sets `dst_adjusted=True` and writes `gap_seconds = t_{\text{legal}} - t_{\text{local}}`. That surplus interval is returned to the arrival engine so that the waiting‑time distribution remains statistically faithful. If instead $t_{\text{local}}$ matches two legal instants in the fall‑back fold, **the engine determines the correct fold by computing `h = SHA-256(global_seed ‖ site_id ‖ t_local)` and setting `fold = h[0] mod 2`**, assigning `fold=0` for the first hour or `fold=1` for the second, and moves on. This parity rule ensures identical behaviour across re‑runs even though CPython’s hash randomisation changes between interpreter invocations.

The module then writes the integer **local‑time offset** $o$ to the transaction buffer. The row now contains (`event_time_utc`, `local_time_offset`, `dst_adjusted`, `fold`); these four fields are sufficient for any consumer to reconstruct unambiguously the civil time of the transaction. **Failure of the nightly validation (triggering `ZoneMismatchError` or `TimeTableCoverageError`) raises a controlled exception that aborts the build process, flushes all pending logs, and prevents any partial or corrupted output files from being committed, guaranteeing atomic rollback to a clean state.**

The random‑number generator does not influence the civil‑time mapping directly, but the deterministic parity hash for fold assignment uses SHA‑256 with the global seed concatenated to `(site_id, t_local)`, ensuring that fold disambiguation remains stable under seed changes but will differ if `site_id` mutates, as required for reproducibility.

Every constant—ε for the border nudge, the IANA version string, the maximum allowed memory for the timetable cache—is stored in a YAML artefact whose digest is printed inside the site catalogue manifest. Every external shapefile, every override line and every computed STR‑tree digest is likewise captured. Therefore, if a reviewer alters *any* of these inputs, downstream consumers will detect the manifest drift and insist on a complete regeneration, maintaining the chain of reproducibility.