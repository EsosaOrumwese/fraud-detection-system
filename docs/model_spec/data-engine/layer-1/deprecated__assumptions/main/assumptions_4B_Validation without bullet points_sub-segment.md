## Assumptions

The validation layer rests on a lattice of explicit premises, each tied to a concrete artefact, a deterministic code path, a manifest fingerprint, and an automated alarm in CI. The lattice begins with structural integrity. The validator opens every Parquet partition in round‑robin order and, for each row, feeds the geographic coordinates—`latitude, longitude` for physical merchants or the `ip_latitude, ip_longitude` pair for virtual ones—into the same tz‑world spatial index whose shapefile digest (`tz_polygon_digest`) was sealed earlier in the manifest. The point‑in‑polygon query must echo back the row’s `tzid_operational`; disagreement triggers `StructuralError`, writes the offending row plus its Philox jump offset to `structural_failure_<parameter_hash>.parquet`, and stops the build. Because the index digest is fixed, the validator cannot accidentally consult a different map.

Immediately after the coordinate round‑trip, the timestamp legality check recomputes local civil time as $event_time_utc + 60 * local_time_offset$ converts it through the zoneinfo release pinned by `zoneinfo_digest`, and demands bit‑level equality with the original local time stored in the row buffer. A mismatch raises `DstIllegalTimeError` and emits a reproducer script. Daylight‑saving consistency is verified by comparing each candidate local epoch second to the zone’s DST transition table; any second that lies in a spring gap or fails to carry a correct `fold` bit in the autumn fold also raises `DstIllegalTimeError`. Concurrently, the schema firewall asserts that nullable columns obey the merchant’s `is_virtual` flag and that every required field is finite under Fastavro’s runtime schema compiled from `transaction_schema.json`—the schema’s digest (`schema_digest`) prevents silent swaps.

Provided every row survives structural scrutiny, the validator shifts into adversarial indistinguishability mode. Every transaction streams through `adv_embed.embed_6d` (source digest `adv_embed_digest`), projecting it into a six‑dimensional vector of sine/cosine of local hour, sine/cosine of day‑of‑week, and Mercator‑projected latitude/longitude. A window of 200 000 such vectors—half synthetic, half drawn from the GDPR‑sanitised real reference slice—is fed into the XGBoost classifier whose hyper‑parameters (`adv_conf_digest`) are locked in `validation_conf.yml`. The validator computes AUROC every `auroc_interval` rows (configured in `validation_conf.yml`); if AUROC exceeds the cut‑line (`auroc_cut = 0.55`), it halts, dumps model artefacts and misclassified indices to `/tmp/auroc_failure`, and raises `DistributionDriftDetected`, ensuring reproducibility via the recorded RNG jump in `rng_trace.log`.

With adversarial drift defeated, the narrative moves to semantic congruence. Hourly legitimate transaction counts per site are joined to the immutable foot‑traffic scalars in `site_catalog.parquet`; the Poisson GLM in `semantic_glm.py` regresses counts on a cubic spline for hour‑of‑day plus merchant‑day random intercepts. The dispersion estimate θ must reside within the corridor specified in `footfall_coefficients.yaml`—1 to 2 for card‑present channels, 2 to 4 for CNP. If θ escapes this corridor, the validator labels the YAML with a Git attribute `needs_recalibration`, emits `glm_theta_violation.pdf`, and raises `ThetaOutOfRange`, preventing silent variance drift.

The fourth strand is the offset‑barcode examination. Transactions are binned into a matrix of UTC hour versus `local_time_offset`, and a Hough transform in `barcode.py` (digest pinned) extracts the dominant line, translating accumulator space back into a slope in minutes per hour. The allowable band, recorded in `barcode_bounds.yml`, is \[–1,–0.5]. A slope outside this range triggers `BarcodeSlopeError`, draws a red overlay on the heat‑map stored as `barcode_failure_<merchant_id>.png`, and archives it in CI.

Every artefact used above maps to a licence file defined in `artefact_registry.yaml`. During validation `validate_licences.py` recomputes SHA‑1 digests for each licence and compares them to the `licence_digests` field in the manifest; any mismatch raises `LicenceMismatchError`, preventing datasets whose legal pedigree has drifted.

When all structural, adversarial, semantic, and barcode passes return clean, the validator appends `validation_passed=true` to the manifest, hashes the entire `validation/<parameter_hash>/` directory (storing section‑level digests `structural_sha256`, `adv_sha256`, `semantic_sha256`, `barcode_sha256`), and uploads the bundle to HashGate at `/hashgate/<parameter_hash>/<master_seed>`. The GitHub pull‑request action then polls this URI, retrieves the manifest, recomputes its SHA‑256, and blocks merge on any byte mismatch. Once merge proceeds, the dataset directory—its name containing the `parameter_hash`—mounts read‑only on NFS, and the Postgres registry enforces uniqueness of `(parameter_hash, seed, path)`, forbidding any silent regeneration with altered contents.

Because every premise—coordinate validity against tz‑world, civil‑time reconstruction via zoneinfo, adversarial hyper‑parameters, dispersion corridors, physical‑laws‑based barcode slopes, and licence integrity—resolves to a manifest digest and an automated CI guard, any deviation surfaces immediately. The validator thus fulfills the contract begun in the reproducibility layer, delivering a synthetic ledger whose provenance, correctness, and configurability are exhaustively documented and machine‑verified.