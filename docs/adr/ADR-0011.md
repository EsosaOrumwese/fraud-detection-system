# ADR-0011 · *daily_synthetic* Airflow DAG (ORCH-02)

*Status — Proposed*  
*Created — 2025-06-17* 
*Edited — 2025-06-20*
*Author — Esosa Orumwese*

---

## 1 Context  

Sprint-02 must automate daily creation of the synthetic payments feed that downstream
Feast materialisation and model-retrain jobs will consume.  
The generator delivered in DAT-02 already produces a 1 M-row Parquet in ≈ 22 s on a
developer laptop.  
ORCH-02 turns that CLI into a **first-class Airflow 3 DAG** running inside the Celery
stack introduced in ADR-0010.   

Key forces  

| Force                    | Why it matters                                              |
|--------------------------|-------------------------------------------------------------|
| **Reproducibility**      | Same binary & params in dev-laptop, CI, MWAA later.         |
| **Idempotent S3 layout** | Athena/Hive-style `year=/month=` prefixes.                  |
| **Fast local feedback**  | `airflow dags test …` must finish in < 1 min.               |
| **Low ops overhead**     | No extra Airflow providers; pure TaskFlow + boto3.          |
| **Cost guard-rails**     | Temp files cleaned; Parquet compressed; CI only parses DAG. |

---

## 2 Decision  

### 2.1 DAG definition

* **File** `orchestration/airflow/dags/daily_synthetic.py`  
* **Schedule** : `0 2 * * *` (02:00 UTC daily).  
* **Catch-up** : `True` with `max_active_runs = 1` → backfills without overlap.  
* **Params** : `rows` (int, default 1 000 000)
* **Retries** : `2`, `retry_delay = 5 min`.  
* **Tags** : `["data-gen"]`.  
* **Owner / email_on_failure** disabled (Budget alarm already covers spend spikes).

### 2.2 Task graph (TaskFlow API)

| Task id    | Function                                                                                        | Notes                         |
|------------|-------------------------------------------------------------------------------------------------|-------------------------------|
| `generate` | Calls `fraud_detection.simulator.generate_dataset()` — writes Parquet into `tempfile.mkdtemp()` | Returns local path            |
| `upload`   | Uses `boto3.upload_file()` to `s3://$RAW_BUCKET/year=YYYY/month=MM/…`                           | Multipart handles 50-MB file  |
| `cleanup`  | Deletes local file; `trigger_rule=ALL_DONE` so it runs even on failure                          | Keeps container disk < 200 MB |

### 2.3 Environment & secrets

* **`.env`** injects `RAW_BUCKET`, `AWS_DEFAULT_REGION`, and developer credentials.  
* Celery workers inherit these via Compose; MWAA will supply them via Secrets Manager.

### 2.4 Image dependencies

Pinned in **`orchestration/airflow/requirements.txt`** (baked into custom image):



```{text}
polars==1.31.0
faker==TBC
boto3==1.38.27
mimesis==TBC
```

### 2.5 Developer UX

| Make target            | Action                                             |
|------------------------|-----------------------------------------------------|
| `airflow-up`           | Start full stack (web, scheduler, worker, broker). |
| `airflow-test-dag`     | `airflow dags test daily_synthetic 2025-06-12` in container. |
| `airflow-down`         | Graceful stop.                                     |

### 2.6 CI hooks

* **DAG import test** installs slim Airflow wheel, parses DagBag, asserts zero `import_errors`.  
* Runs in < 10 s; no containers spun up in CI.

---

## 3 Consequences  

### Positive

* **End-to-end demo**: one click in UI produces Parquet in raw bucket; screenshot added to Sprint-02 review.  
* **Parameter-driven backfills**: ops can replay past days with different `rows`.  
* **Disk hygiene**: temp deleted even on failure; no Docker layer bloat.  
* **CI safe**: DAG always import-checked before merge; prevents broken deploys.

### Negative / Trade-offs

* Celery stack now needs +1 GB RAM for worker; mitigated by `make airflow-up-minimal` profile.  
* Upload step relies on developer AWS creds in `.env`; MWAA transition will move to IAM role.  
* Storing only Parquet (no CSV sample) means MLflow run artefacts depend on generator code for raw inspection.

---

## 4 Alternatives considered  

| Option                              | Reason rejected                               |
|-------------------------------------|-----------------------------------------------|
| **BashOperator** running CLI script | Harder param handling, brittle quoting.       |
| **Prefect 2 flow**                  | Team aligned on Airflow for recruiter signal. |
| **Fargate task via ECSOperator**    | Extra AWS infra; overkill for 5-min job.      |

---

## 5 Validation checklist  

* `make airflow-up` then trigger DAG → three green tasks; S3 prefix exists, file ~52 MB.  
* `docker exec worker du -sh /tmp/*` → temp file ≤ 1 MB (only logs).  
* `pytest -q tests/unit/test_dag_import.py` passes in CI.  
* Backfill 2025-06-11 succeeds in < 8 min on laptop stack.

---

## 6 Future improvements  

1. **S3 object metadata** : add `Content-Encoding: gzip` once Parquet compression switched to `GZIP`.  
2. **Great Expectations data test task** before upload.  
3. Push row count & S3 URI as **XCom** to downstream Feast materialisation DAG.  
4. Swap boto3 creds for **ECS task role / MWAA execution role** when migrating off laptop stack.

---

## 7 Change history  

| Date       | Author      | Note                            |
|------------|-------------|---------------------------------|
| 2025-06-17 | E. Orumwese | Initial ADR-0011 accepted.      |
| 2025-06-20 | E. Orumwese | Removed expected `realism` flag |

---
