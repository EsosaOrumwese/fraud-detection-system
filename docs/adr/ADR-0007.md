# ADR-0007 · Baseline Fraud Model (ML-01)

*Status — Accepted*  
*Created — 2025-06-05*  
*Author — Esosa Orumwese*

---

## 1 Context  

Sprint-01 must demonstrate an **end-to-end “data → metric” loop** on the synthetic payments set produced in DAT-02.  
Key forces:

* **Imbalanced data** – fraud ≈ 0.3 % prevalence.  
* **Run-anywhere** – local laptops now; Airflow-scheduled retrain in Sprint-02.  
* **Recruiter signal** – industry-standard libraries (scikit-learn API, XGBoost, MLflow).  
* **Reproducibility** – every experiment must be CLI-invocable and CI-testable. :contentReference[oaicite:0]{index=0}  

---

## 2 Decision  

| Layer                   | Decision                                                                                                                                                      |
|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Algorithm**           | `xgboost.XGBClassifier` (`tree_method="hist"`, `n_estimators=300`, `max_depth=6`) – balances speed and baseline accuracy.                                     |
| **Class imbalance**     | `scale_pos_weight = n_negative / n_positive` (≈ 330) – faster than SMOTE for a first cut and preserves recall.                                                |
| **Feature pipeline**    | `ColumnTransformer`  ▶︎ `OneHotEncoder(max_categories=100, handle_unknown="ignore", sparse_output=True)` for categoricals; log-transform on `amount` numeric. |
| **Metric of record**    | **Average Precision (PR-AUC)** – preferred for rare-event data ≤ 1 % fraud.                                                                                   |
| **Experiment tracking** | **MLflow 2.22** local tracking URI `file:./mlruns`; model registered as `fraud_xgb` v1.                                                                       |
| **CLI wrapper**         | `train_baseline.py` with `argparse`; exposed via `make train`.                                                                                                |
| **Governance hooks**    | Two fast unit tests (`pytest`) assert AUC-PR > prevalence baseline and model artefact presence; run in CI < 30 s.                                             |
| **Artefacts**           | • Parquet training sample version hash<br>• model.pkl<br>• feature-encoder.pkl<br>• SHAP summary (optional if time).                                          |

---

## 3 Consequences  

### Positive  
* **>0.70 PR-AUC** on 20 % hold-out meets sprint goal.  
* MLflow UI + registry create an auditable trail the moment code is merged.  
* Training fits in **< 7 min / < 2 GB RAM** on a typical laptop, so dev loop is tight.  
* Pipeline re-usable in Airflow DAG by simply calling `make train` in a Docker task.

### Negative / Trade-offs  
* One-hot encoding explodes dimensionality; CSR matrix mitigates RAM but complicates SHAP visuals.  
* Local MLflow store means no team-wide visibility until we deploy a remote server in Sprint-02.  
* `scale_pos_weight` may under-weight borderline positives; later sweeps will explore SMOTE-Tomek and focal-loss variants.

---

## 4 Alternatives considered  

| Alternative                            | Why rejected (baseline phase)                                                                           |
|----------------------------------------|---------------------------------------------------------------------------------------------------------|
| **Logistic regression + class_weight** | 6-point drop in PR-AUC during spike test; chosen XGB gives interpretable trees with better recall.      |
| **CatBoost**                           | Superior to XGB on categoricals but GPU licence friction for some devs; revisit once GPU budget okayed. |
| **Prefect flow**                       | Team pivoted to Airflow to align with recruiter expectations and larger community.                      |
| **Neptune.ai tracking**                | Free tier good but we want a de-facto-standard OSS tool (MLflow) to showcase.                           |

---

## 5 Implementation sketch (reference)  

```bash
poetry add xgboost scikit-learn imbalanced-learn mlflow polars shap
make train           # trains on 500 k rows, logs run
make mlflow-ui       # open http://localhost:5000
pytest -q            # CI smoke tests
```

Run targets appear in **`.github/workflows/ci.yml`** right after linting;

---

## 6 Future improvements (backlog → Sprint-02)

1. **Airflow DAG** – daily retrain with synthetic delta load; push model to remote MLflow server.
2. **Optuna sweep** – 20-trial hyper-opt, tracked as child runs under the same MLflow experiment.
3. **Great Expectations** on training data to block schema drift.
4. **SageMaker Training Job** wrapper to benchmark cloud parity cost.
5. **SHAP explainability report** auto-exported to S3 and surfaced in PR comment.

---

## 7 Validation checklist

* `make train` finishes in < 7 min; model registered as `fraud_xgb` v1.
* `mlflow ui` shows params, metrics, and artefacts.
* `pytest` passes in CI (< 30 s).
* Hold-out PR-AUC ≥ 0.70 (printed and logged).
* ADR-0007 merged before code PR.

---

## 8 Change history

| Date       | Author         | Notes                                                        |
| ---------- | -------------- | ------------------------------------------------------------ |
| 2025-06-05 | Esosa Orumwese | Initial ADR accepted. Future Airflow/MLflow server outlined. |

---



