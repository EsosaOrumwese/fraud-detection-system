The validation layer rests on a lattice of explicit premises, each tied to a concrete artefact, a deterministic code path, a manifest fingerprint, and an automated alarm in CI. The lattice begins with structural integrity. The validator opens every parquet partition in round‑robin order and for each row feeds the geographic coordinates—`latitude, longitude` for physical merchants or the `ip_latitude, ip_longitude` pair for virtual ones—into the same tz‑world spatial index whose shapefile digest (`tz_polygon_digest`) was sealed earlier in the manifest. The point‑in‑polygon query must echo back the row’s `tzid_operational`; disagreement triggers `StructuralError`, writes the offending row plus its Philox jump offset to `structural_failure_<parameter_hash>.parquet`, and stops the build. Because the index digest is fixed, the validator cannot accidentally consult a different map.

Immediately after the coordinate round‑trip, the timestamp legality check recomputes local civil time as `event_time_utc + 60 × local_time_offset`, converts it through the zoneinfo release pinned by `zoneinfo_digest`, and demands bit‑level equality with the original local time kept in the row buffer. A mismatch would indicate that some upstream routine changed offsets without updating the stored value. Daylight‑saving consistency is verified by comparing each candidate local epoch second to the zone’s DST transition table; any second that lies in a spring gap or fails to carry a correct `fold` bit in the autumn fold raises `DstIllegalTimeError` and emits a reproducer script. At the same moment the schema firewall asserts that nullable columns obey the merchant’s `is_virtual` flag and that every required field is finite under Fastavro’s runtime schema compiled from `transaction_schema.json`—the schema’s digest (`schema_digest`) captured during reproducibility stage prevents silent swaps.

Provided every row survives structural scrutiny, the validator shifts into adversarial indistinguishability mode. Every transaction streams through the function `adv_embed.embed_6d`, whose source digest is locked in the manifest as `adv_embed_digest`. That function deterministically projects the record to a six‑dimensional vector comprising sine and cosine of local hour, sine and cosine of day‑of‑week, and the two‑component Mercator projection of its spatial coordinate. A window of 200 000 such vectors—half synthetic, half drawn from the real reference slice shipped under GDPR‑sanitised licence—is fed into the XGBoost classifier whose hyper‑parameter file `validation_conf.yml` bears digest `adv_conf_digest`. The classifier is seeded from the Philox stream jump labelled `"validator.adversarial"`; the stream position is recorded in `rng_trace.log`. If at any evaluation checkpoint the AUROC crosses the cut‑line in the same YAML (`auroc_cut = 0.55`), the validator halts, dumps the model artefacts and mis‑classified indices to `/tmp/auroc_failure` and raises `DistributionDriftDetected`. Because the RNG jump and the classifier dump appear in CI artefacts, an auditor can recreate the exact training set and confirm the AUROC reproducibly.

With adversarial drift defeated, the narrative moves to semantic congruence. Hourly legitimate transaction counts per site are joined to the immutable foot‑traffic scalars in `site_catalog.parquet`; the Poisson GLM in `semantic_glm.py` regresses counts on a cubic‑spline basis for hour‑of‑day plus the natural log of footfall, including merchant‑day random intercepts. The dispersion estimate θ must reside in the corridor specified in `footfall_coefficients.yaml`: 1 to 2 for card‑present channels, 2 to 4 for card‑not‑present. The YAML digest is already present in the manifest; if θ escapes the corridor the validator labels the YAML with a “needs‑recalibration” Git attribute, emits `glm_theta_violation.pdf` and raises `ThetaOutOfRange`, ensuring the variance promise made in the LGCP calibration cannot silently rot.

The fourth strand in the lattice is the offset‑barcode examination. Counts are binned into a matrix of UTC hour versus `local_time_offset`, and a Hough transform—parameterised inside `barcode.py` whose digest is pinned—extracts the dominant line, translating accumulator space back into a slope measured in offsets per hour. The allowable band, recorded in `barcode_bounds.yml`, is between −1 and −0.5. A slope outside that interval means physical impossibility under Earth’s rotation; the validator draws a red overlay on the heat‑map, stores `barcode_failure_<merchant_id>.png`, and throws `BarcodeSlopeError`. Because the PNG is archived in CI, the development team sees the deviation visually instead of parsing numeric logs.

Every artefact used above maps to a licence file; `artefact_registry.yaml` couples each path to its licence. During validation the script `validate_licences.py` recomputes SHA‑1 digests for those licence texts and compares them with the `licence_digests` field in the manifest; a mismatch raises `LicenceMismatchError`, preventing datasets whose legal pedigree has drifted.

When all structural, adversarial, semantic and barcode passes return clean, the validator appends `validation_passed=true` to the manifest, hashes the entire validation artefact directory and stores that digest alongside section digests (`structural_sha256`, `adv_sha256`, `semantic_sha256`, `barcode_sha256`). It uploads the bundle to HashGate under the composite URI `/hashgate/<parameter_hash>/<master_seed>`. The GitHub pull‑request action polls that URI, retrieves the manifest and recomputes its SHA‑256; if any byte differs, merge is blocked. Once merge proceeds, the dataset directory—its name irrevocably containing the parameter‑set hash—is mounted immutable on NFS, and the Postgres registry records `(parameter_hash, seed, path)` as unique, forbidding any future regeneration with identical hash but altered contents.

Because every premise—coordinate validity against tz‑world, offset arithmetic under zoneinfo, hyper‑parameters in YAML, dispersion corridors tied to LGCP variance, physics‑bound barcode slopes, licence integrity—resolves to a digest in the manifest and an automated CI guard, any deviation surfaces immediately. The validator thus completes the contract begun in the reproducibility layer: the synthetic ledger leaves the pipeline only when every statistical and legal promise has survived a continuous, inter‑locked gauntlet, providing reviewers with an artefact whose provenance, correctness and configurability are exhaustively documented and machine‑verified.
