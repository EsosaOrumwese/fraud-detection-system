<<<IC-FIX id=1>
Stage: ValidateStructuralIntegrity
INPUT_ARTIFACT:
name: transactions_data
path_pattern: s3://data-engine/datasets/{parameter_hash}_{master_seed_hex}/transactions/*.parquet
sha256: <to-be-populated-on-build>
schema: |
{
  "name": "transactions_data",
  "type": "record",
  "fields": [
    {"name":"transaction_id","type":"string","nullable":false},
    {"name":"amount","type":"double","nullable":false},
    {"name":"site_id","type":"string","nullable":false},
    {"name":"event_time","type":"long","nullable":false}
  ]
}
INPUT_ARTIFACT:
name: timezone_shapefile
path_pattern: s3://data-engine/artefacts/priors/tz_world/2025a/tz_world_2025a.shp  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"binary" }
INPUT_ARTIFACT:
name: zoneinfo_version
path_pattern: s3://data-engine/config/zoneinfo_version.yml  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{
  "name": "zoneinfo_version",
  "type": "record",
  "fields": [
    {"name":"version","type":"string","nullable":false}
  ]
}
INPUT_ARTIFACT:
name: transaction_schema
path_pattern: s3://data-engine/schemas/transaction_schema.json  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"record" }
OUTPUT_ARTIFACT:
name: structural_failure
path_pattern: s3://data-engine/validation/structural_failure_{parameter_hash}.parquet
sha256: <to-be-populated-on-build>
schema: |
{ "type":"record" }
OUTPUT_ARTIFACT:
name: validator_log
path_pattern: s3://data-engine/validation/{parameter_hash}/validator.log
sha256: <to-be-populated-on-build>
PARTITIONING:
keys: []
order_by: []
SUCCESS_METRIC:
metric_name: defect_count_zero          # row-count of failure parquet == 0
expected_range: [1,1]
ERROR_POLICY:
error_code: StructuralIntegrityError
retry_max: 0
retry_backoff_sec: 0
idempotent: true
SCHEMA_VERSION:
version: v1.0.0-draft
ACCESS_POLICY:
read_policy: validation-team-read
write_policy: validation-team-write
CONSUMED_BY:
module: validate_structural_integrity.py
function: validateStructuralIntegrity
description: Checks each row for schema completeness and writes failures
TEST_PATHWAY:
test_type: integration
tool: pytest
script: tests/test_validate_structural_integrity.py
assertion: failure file exists with offending rows
Confidence=MEDIUM
<<<END IC-FIX>>}

<<<IC-FIX id=2>
Stage: ValidateAdversarialIndistinguishability
INPUT_ARTIFACT:
name: validation_conf
path_pattern: s3://data-engine/config/validation_conf.yml  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{
  "name": "validation_conf",
  "type": "record",
  "fields": [
    {"name":"auroc_threshold","type":"double","nullable":false},
    {"name":"eval_interval_rows","type":"int","nullable":false}
  ]
}
OUTPUT_ARTIFACT:
name: auroc_model_dump
path_pattern: s3://data-engine/validation/auroc_model_{parameter_hash}.parquet  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"binary" }
OUTPUT_ARTIFACT:
name: misclassified_examples
path_pattern: s3://data-engine/validation/misclassified_{parameter_hash}.csv  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"record" }
OUTPUT_ARTIFACT:
name: rng_trace_log
path_pattern: s3://data-engine/datasets/{parameter_hash}_{master_seed_hex}/logs/rng_trace.log
sha256: <to-be-populated-on-build>
schema: |
{ "type":"record" }
PARTITIONING:
keys: []
order_by: []
SUCCESS_METRIC:
metric_name: auroc_under_threshold       # AUROC < 0.55 AND rng_trace present
expected_range: [1,1]
ERROR_POLICY:
error_code: DistributionDriftError
retry_max: 0
retry_backoff_sec: 0
idempotent: true
SCHEMA_VERSION:
version: v1.0.0-draft
ACCESS_POLICY:
read_policy: validation-team-read
write_policy: validation-team-write
CONSUMED_BY:
module: validate_adversarial_indistinguishability.py
function: validateAdversarialIndistinguishability
description: Evaluates classifier AUROC and dumps on failure
TEST_PATHWAY:
test_type: integration
tool: pytest
script: tests/test_validate_adversarial_indistinguishability.py
assertion: model dump and examples exist on drift
Confidence=MEDIUM
<<<END IC-FIX>>}

<<<IC-FIX id=3>
Stage: ValidateSemanticCongruence
INPUT_ARTIFACT:
name: footfall_coefficients
path_pattern: s3://data-engine/config/footfall_coefficients.yaml  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"record" }
INPUT_ARTIFACT:
name: site_catalogue
path_pattern: s3://data-engine/datasets/{parameter_hash}_{master_seed_hex}/site_catalogue/*.parquet
sha256: <to-be-populated-on-build>
schema: |
{ "type":"record" }
OUTPUT_ARTIFACT:
name: theta_violation_pdf
path_pattern: s3://data-engine/validation/theta_violation_{parameter_hash}.pdf  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"binary" }
PARTITIONING:
keys: []
order_by: []
SUCCESS_METRIC:
metric_name: dispersion_bounds_pass      # CP∈[1,2], CNP∈[2,4]
expected_range: [1,1]
ERROR_POLICY:
error_code: SemanticCongruenceError
retry_max: 0
retry_backoff_sec: 0
idempotent: true
SCHEMA_VERSION:
version: v1.0.0-draft
ACCESS_POLICY:
read_policy: validation-team-read
write_policy: validation-team-write
CONSUMED_BY:
module: validate_semantic_congruence.py
function: validateSemanticCongruence
description: Checks GLM theta against configured corridors
TEST_PATHWAY:
test_type: integration
tool: pytest
script: tests/test_validate_semantic_congruence.py
assertion: PDF generated on violation
Confidence=MEDIUM
<<<END IC-FIX>>}

<<<IC-FIX id=4>
Stage: ValidateOffsetBarcode
INPUT_ARTIFACT:
name: transactions_data
path_pattern: s3://data-engine/datasets/{parameter_hash}_{master_seed_hex}/transactions/*.parquet
sha256: <to-be-populated-on-build>
schema: |
{ "type":"record" }
INPUT_ARTIFACT:
name: barcode_bounds
path_pattern: s3://data-engine/config/barcode_bounds.yml  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"record" }
OUTPUT_ARTIFACT:
name: barcode_failure_png
path_pattern: s3://data-engine/validation/barcode_failure_{merchant_id}.png
sha256: <to-be-populated-on-build>
schema: |
{ "type":"binary" }
PARTITIONING:
keys: []
order_by: []
SUCCESS_METRIC:
metric_name: barcode_slope_bounds_pass   # slope ∈ [-1,-0.5]
expected_range: [1,1]
ERROR_POLICY:
error_code: OffsetBarcodeError
retry_max: 0
retry_backoff_sec: 0
idempotent: true
SCHEMA_VERSION:
version: v1.0.0-draft
ACCESS_POLICY:
read_policy: validation-team-read
write_policy: validation-team-write
CONSUMED_BY:
module: validate_offset_barcode.py
function: validateOffsetBarcode
description: Renders barcode png when slope out of bounds
TEST_PATHWAY:
test_type: integration
tool: pytest
script: tests/test_validate_offset_barcode.py
assertion: PNG exists on failure
Confidence=MEDIUM
<<<END IC-FIX>>}

<<<IC-FIX id=5>
Stage: ValidateLicences
INPUT_ARTIFACT:
name: artefact_registry
path_pattern: s3://data-engine/config/reproducibility/artefact_registry.yaml  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"record" }
OUTPUT_ARTIFACT:
name: licence_check_log
path_pattern: s3://data-engine/validation/licence_check.log  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"record" }
PARTITIONING:
keys: []
order_by: []
SUCCESS_METRIC:
metric_name: licence_mismatch_count_zero
expected_range: [1,1]
ERROR_POLICY:
error_code: LicenceMismatchError
retry_max: 0
retry_backoff_sec: 0
idempotent: true
SCHEMA_VERSION:
version: v1.0.0-draft
ACCESS_POLICY:
read_policy: validation-team-read
write_policy: validation-team-write
CONSUMED_BY:
module: validate_licences.py
function: validateLicences
description: Verifies licence file digests against manifest
TEST_PATHWAY:
test_type: integration
tool: pytest
script: tests/test_validate_licences.py
assertion: log lists all licences verified
Confidence=MEDIUM
<<<END IC-FIX>>}

<<<IC-FIX id=Flag>>
Stage: AppendValidationPassFlag
INPUT_ARTIFACT:
  name: structural_failure
  path_pattern: s3://data-engine/validation/structural_failure_{parameter_hash}.parquet
  sha256: <to-be-populated-on-build>
INPUT_ARTIFACT:
  name: validator_log
  path_pattern: s3://data-engine/validation/{parameter_hash}/validator.log
  sha256: <to-be-populated-on-build>
OUTPUT_ARTIFACT:
  name: ci_validation_passed_flag
  path_pattern: s3://data-engine/validation/{parameter_hash}/ci_validation_passed.flag
  sha256: <to-be-populated-on-build>
OUTPUT_ARTIFACT:
  name: readonly_flag
  path_pattern: s3://data-engine/datasets/{parameter_hash}_{master_seed_hex}/readonly.flag
  sha256: <to-be-populated-on-build>
SUCCESS_METRIC:
  metric_name: validation_pass_flag_set
  expected_range: [1,1]
<<<END IC-FIX>>

<<<IC-FIX id=6>
Stage: UploadToHashGate
INPUT_ARTIFACT:
name: validation_bundle
path_pattern: s3://data-engine/validation/{parameter_hash}/**  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"binary" }
OUTPUT_ARTIFACT:
name: hashgate_uri
path_pattern: s3://data-engine/validation/hashgate_uri.txt  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"string" }
PARTITIONING:
keys: []
order_by: []
SUCCESS_METRIC:
metric_name: file_existence
expected_range: [1,1]
ERROR_POLICY:
error_code: HashGateUploadError
retry_max: 3
retry_backoff_sec: 60
idempotent: true
SCHEMA_VERSION:
version: v1.0.0-draft
ACCESS_POLICY:
read_policy: validation-team-read
write_policy: ci-team-write
CONSUMED_BY:
module: upload_to_hashgate.py
function: uploadToHashGate
description: Uploads validation bundle to HashGate service
TEST_PATHWAY:
test_type: integration
tool: pytest
script: tests/test_upload_to_hashgate.py
assertion: URI file exists
Confidence=MEDIUM
<<<END IC-FIX>>}

<<<IC-FIX id=7>
Stage: ValidateMergeCondition
INPUT_ARTIFACT:
name: hashgate_uri
path_pattern: s3://data-engine/validation/hashgate_uri.txt  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{ "type":"string" }
OUTPUT_ARTIFACT:
name: merge_status_flag
path_pattern: s3://data-engine/validation/merge_status_{parameter_hash}.flag  # path reserved by spec.
sha256: <to-be-populated-on-build>
schema: |
{
  "name": "merge_status_flag",
  "type": "record",
  "fields": [
    {"name":"merge_status","type":"string","nullable":false}
  ]
}
PARTITIONING:
keys: []
order_by: []
SUCCESS_METRIC:
metric_name: manifest_hash_match         # HashGate manifest == local manifest
expected_range: [1,1]
ERROR_POLICY:
error_code: MergeConditionError
retry_max: 0
retry_backoff_sec: 0
idempotent: true
SCHEMA_VERSION:
version: v1.0.0-draft
ACCESS_POLICY:
read_policy: ci-team-read
write_policy: ci-team-write
CONSUMED_BY:
module: validate_merge_condition.py
function: validateMergeCondition
description: Checks manifest match before merging dataset
TEST_PATHWAY:
test_type: integration
tool: pytest
script: tests/test_validate_merge_condition.py
assertion: merge_status_flag indicates success
Confidence=MEDIUM
<<<END IC-FIX>>}

##### END INTERFACE_CONTRACT_SPEC #####

id=1 | gaps_closed=input|output|schema|metric|error | notes=Defined structural integrity I/O, schemas, metric, and error policy  
id=2 | gaps_closed=input|output|schema|metric|error | notes=Completed adversarial AUROC validation I/O and error policy  
id=3 | gaps_closed=input|output|schema|metric|error | notes=Specified semantic congruence I/O, PDF output, and error policy  
id=4 | gaps_closed=input|output|schema|metric|error | notes=Added barcode bounds input, PNG output, and policy  
id=5 | gaps_closed=input|output|schema|metric|error | notes=Defined licence registry I/O, log output, and error policy  
id=6 | gaps_closed=input|output|schema|metric|error | notes=Set validation bundle I/O, URI output, and upload policy  
id=7 | gaps_closed=input|output|schema|metric|error | notes=Added merge status flag output and merge check logic  
<<IS-END>>