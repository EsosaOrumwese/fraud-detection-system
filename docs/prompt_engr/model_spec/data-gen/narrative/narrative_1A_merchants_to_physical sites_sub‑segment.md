The generator’s opening manoeuvre is to transform the symbolic `merchant_id` rows in the bare `transaction_schema` into a concrete list of outlet stubs—still placeless, still timezone‑agnostic, but already carrying every decision about *how many* outlets the merchant operates and *in which countries* the chain will legally trade. The reviewer must be able to rerun this stage, byte‑for‑byte, months later and see identical results, so the narrative below treats determinism and auditability as first‑class requirements.

---

The build process begins with a tight handshake between code and data: the pipeline enumerates every artefact bundle that lives under *git LFS* and is tagged by semantic version—`hurdle_coefficients.yaml`, `nb_dispersion_coefficients.yaml`, `crossborder_hyperparams.yaml`, the entire hierarchy of spatial priors in `spatial_prior_bundle/`, the GDP bucket mapping file, the currency→country split table, and the network‑share vectors directory. For each individual file the loader computes a SHA‑256 digest; it then XOR‑reduces all those digests together with the git commit hash and a combined parameter hash (XOR of coefficient/hyperparameter YAML digests then SHA‑256) to yield a single 256‑bit manifest fingerprint. That fingerprint is written to (a) the top‑level `_manifest.json`, (b) every Parquet file’s `comment` field, and (c) the RNG master‑seed derivation string, so that any artefact changed without a version bump becomes instantly visible—downstream readers reject mismatching manifests at open time.

With the governing artefacts frozen, the code constructs the merchant design matrix. For each merchant the hurdle logistic uses intercept, MCC dummies, channel dummies, and a developmental (GDP bucket) categorical derived from the merchant’s registered home country GDP per capita. GDP per capita is looked up in the World‑Bank data vintage released on 2025‑04‑15 and frozen by SHA‑256; subsequent updates never alter historical runs because the artefact path embeds that release date. It is discretised into five buckets using Jenks natural breaks; the mapping table lives at `artefacts/gdp/gdp_bucket_map_2024.parquet` with its own digest (`gdp_bucket_map_digest`) and semantic version so reviewers can rerun the binning step on a later vintage. All bucket assignments are therefore deterministic functions of ISO‑3166 code and the versioned GDP data‑set.

The single‑versus‑multi decision is the first random action. The generator multiplies the design matrix row by the logistic‑regression coefficient vector stored in `hurdle_coefficients.yaml` (fields: `semver`, `sha256_digest`, `estimation_window`, `stationarity_test_digest`), applies the logistic function, and obtains a probability π that this merchant is multi‑site. **For avoidance of doubt, this single vector already contains a coefficient for every predictor in the hurdle design matrix—including the five GDP‑development bucket dummies—so no separate parameter symbol (e.g. a standalone `γ_{dev}` file or term) exists; all hurdle logistic coefficients are co‑located in `hurdle_coefficients.yaml` and loaded atomically.** Philox 2¹²⁸ supplies a single uniform deviate from the sub‑stream whose jump stride is the first 64 little‑endian bits of SHA‑256("hurdle_bernoulli"); if it is less than π the merchant advances to the multi‑site branch, otherwise it is locked to exactly one outlet. Because the logistic coefficients were estimated on a proprietary acquirer sample and released only as YAML, the open‑source code never sees raw data yet reproduces the same probabilities. The Bernoulli outcome is persisted to the row; the RNG audit log also records event\_type=`hurdle_bernoulli`, pre/post counters, the uniform, π, and the stride for full replay.

For every merchant declared multi‑site the pipeline computes the mean μ and dispersion φ of a negative binomial. **The mean log‑link intentionally *excludes* the developmental (GDP bucket) factor present in the hurdle logistic** (parsimony and to avoid collinearity with dispersion), using only intercept + MCC + channel; the dispersion log‑link reuses those categorical components and appends a continuous natural log(GDP per capita in constant 2015 USD) term with positive coefficient η constrained during estimation. When GDP is low the term lifts log φ, swelling the variance and stretching the right tail governing very large outlet counts; when GDP is high it contracts log φ. A deviate N ∼ NB(μ, φ) is then sampled from the merchant‑specific Philox sub‑stream via the Poisson–Gamma mixture (Gamma(r=φ, scale=μ/φ) then Poisson). Parameter equivalents (r=φ, p=φ/(φ+μ)) and both draws are logged. If the variate equals 0 or 1 the algorithm rejects it, increments the “NB‑rejection” counter, and draws again from the same stream state; the loop terminates only once N ≥ 2, guaranteeing that the multi‑site branch cannot regress to single‑site logic. Every rejection *and* the final accepted draw write full pre/post counters plus cumulative rejection count to `rng_audit.log`. Nightly CI monitors `nb_rejection_rate_overall` (target ≤ 0.06), the p99 of rejection counts (≤ 3), and a one‑sided CUSUM against the baseline to surface latent drift.

With the domestic count N pinned, the engine next decides how far the chain spreads beyond its home borders; “geographic sprawl” is the random variable K (count of foreign countries). For merchants entering the cross‑border branch it samples a **true zero‑truncated Poisson**, K ∼ ZTPoisson(λ\_extra) with λ\_extra = θ₀ + θ₁ log N, θ‑hyperparameters from `crossborder_hyperparams.yaml` (digest recorded, `theta1_stats` includes Wald p‑value < 1e‑5 and confidence interval establishing sub‑linearity). Classical rejection draws from Poisson(λ\_extra) until K ≥ 1; the number of rejections is logged (event\_type=`ztp_rejection`). A hard cap of 64 rejections aborts the build (`ztp_retry_exhausted`) to prevent pathological hangs; CI asserts mean rejections < 0.05 and p99.9 < 3.

When K ≥ 1 the pipeline assembles a candidate list of foreign jurisdictions using a currency→country expansion. It begins with a *settlement share vector* at the currency level from `artefacts/network_share_vectors/settlement_shares_2024Q4.parquet` (`settlement_shares_digest`). Multi‑country currencies are deterministically expanded using proportional intra‑currency country weights from `artefacts/currency_country_split/ccy_country_shares_2024Q4.parquet`; if any destination weight is sparse (observation count < 30) additive Dirichlet smoothing (α=0.5) is applied. If the entire currency row remains sparse the expansion falls back to equal split across member countries and flags `sparse_flag=true` in the audit log. The resulting country weights are renormalised. Weighted sampling without replacement of K foreign countries uses the Gumbel‑top‑k method: for each candidate i one uniform u\_i produces key = log w\_i − log(−log u\_i); the K largest keys determine selection, with ties broken lexicographically by ISO code. Keys, selected ISO codes (in selection order), and ordering are logged (event\_type=`gumbel_key`).

The ordered `country_set` is thus length K+1: the home ISO first, followed by the K foreign ISO codes in the exact order selected. The code loads the α‑vector for `(home_country, mcc_code, channel)` from `crossborder_hyperparams.yaml`; α has length K+1. A single Dirichlet(α) draw is produced via independent Gamma(α\_i,1) (Marsaglia–Tsang) in IEEE‑754 binary64; raw gamma deviates and the normalised weights w\_i (rounded to 8 decimal places) are logged (event\_type=`dirichlet_gamma_vector`). Multiplying w by N gives real‑valued allocations; largest‑remainder rounding is fully deterministic: floor each component, compute deficit d = N − Σ floor, sort residual fractions (quantised to 8 dp) descending with ISO secondary key, and add +1 to the first d entries. Because residuals are quantised and ties resolved by ISO code, two runs with identical inputs yield identical integer allocations. A rounding justification metric (max |n\_i − w\_i N|, p99.9, and relative error) is emitted; worst‑case relative deviation is ≤ 1/N and empirically < 0.3% even for small N. Residual ordering and the resulting positional index become a persisted `tie_break_rank` aiding forensic replay.

At this point the algorithm assigns monotonically increasing `site_id`s. It sorts the provisional outlet catalogue first by `country_iso` ascending and then by `tie_break_rank`. Inside each `(merchant_id, country_iso)` block it assigns a fixed 6‑digit zero‑padded sequence starting at `000001`; overflow beyond 999999 outlets in a block raises a `site_sequence_overflow` abort (none expected under current parameters). The composite `site_id` = `merchant_id` ∥ sequence ensures consistent width for byte‑level diffs and stable regex validation. Each stub row now contains these non‑nullable typed columns: `merchant_id` (int64), `site_id` (string `%06d`), `home_country_iso` (char(2)), `legal_country_iso` (char(2)), `single_vs_multi_flag` (bool), `raw_nb_outlet_draw` (int32), `final_country_outlet_count` (int32), `tie_break_rank` (int32), `manifest_fingerprint` (hex64), `global_seed` (uint64). Integers are stored little‑endian, strings UTF‑8, ISO codes dictionary‑encoded; compression is ZSTD level 3 (codec choice hashed into the manifest). The table is written under a path embedding both seed and fingerprint, e.g., `…/outlet_catalogue/seed={seed}/fingerprint={fingerprint}/part-*.parquet`.

Immediately after write, a validation routine re‑reads the Parquet and recomputes μ, φ, K, Dirichlet weights, residual ordering, integer allocations, tie\_break\_rank sequence, and generated site\_ids from the stored inputs. Any mismatch aborts the build before downstream consumption. Stream jump events (e.g., the module‑specific `"cross-border-allocation"` jump) are logged with stride (`stream_jump`) so complete Philox counter evolution can be reconstructed even if no rejections occurred. Accepted final NB draws, ZTP outcomes, Dirichlet/gamma vectors, Gumbel keys, and sequence finalisation each produce explicit audit events; absence of any required event constitutes a structural validation failure.

The narrative halts here intentionally. Every outlet is counted and legally domiciled, but no latitude, longitude or timezone exists yet. The coordinate sampling logic belongs to the next sub‑segment, and because the manifest fingerprint already reflects every parameter and artefact that influenced the decisions above, the downstream geographic sampler cannot retroactively change outlet counts or country assignments without emitting a new fingerprint and thereby an explicit lineage break. This strict layering—abstract counts and jurisdictional allocation first, concrete coordinates and temporal attributes later—enables the entire merchant‑location pipeline to be audited in orthogonal chunks, a structural property that underpins high‑confidence reproducibility during rigorous review.
