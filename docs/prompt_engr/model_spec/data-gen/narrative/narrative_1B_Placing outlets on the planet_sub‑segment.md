The moment the catalogue says “merchant X operates **N** sites in country Y,” a second engine has to materialise those sites on the globe with enough fidelity that a geospatial analyst could overlay them on open data and nod that the pattern looks real. That engine is the sub‑segment we labelled **“Placing outlets on the planet.”** Its job is to turn an unlocated site identifier into a latitude, longitude, elevation proxy, prior weight and provenance trail, all recorded with cryptographic hashes that prove which spatial artefacts were involved. What follows is the full narrative of how I would build that engine so that every stochastic decision is traceable, every edge‑case is handled deterministically, and every modelling assumption can be overturned by editing a YAML file rather than touching code.

---

The first step is to prepare a **library of spatial priors** keyed by `(mcc_code, channel)`. Each prior is either a raster (GeoTIFF) or a vector layer (ESRI shapefile or GeoPackage) that assigns a non‑negative weight to every location within a country. For high‑street retail the prior is the 100‑metre resolution population density raster published by Facebook’s High Resolution Settlement Layer; for vehicle‑oriented merchants the prior is a clipped Open‑Street‑Map primary‑road network whose line segments carry an AADT attribute scraped from government traffic counts; for duty‑free and travel‑retail MCCs the prior is the IATA polygon of commercial‑airport boundaries; for big‑box categories the prior is a blended raster formed by taking a convex combination of suburban population density and road traffic density. The blend coefficients live in `spatial_blend.yaml`; a reviewer can change a coefficient and regenerate a new catalogue to test sensitivity. Every file in the prior library is accompanied by a checksum file; the build concatenates those checksums and records a grand digest in the site catalogue so that anyone can reproduce exactly which map was used.

With priors ready, the engine enters an **importance‑sampling loop** that is deterministic given the global RNG stream. For a given `site_id` in country `c` the engine first checks whether a category‑specific prior exists for that country. If it does, the engine opens the raster header, reads the grid of weights, and constructs a cumulative‑distribution function over pixel indices. Because the raster may contain millions of pixels, the CDF is stored in a succinct Fenwick tree that allows O(log n) prefix‑sum queries but only O(n) memory once across the whole build. The engine samples a pixel index proportional to its weight, then converts the grid coordinate to a geographic bounding box. If the prior is a vector layer instead of a raster, the engine first samples a feature with probability proportional to its associated weight (segment length times AADT for roads, polygon area for airports), then chooses a random point uniformly along the feature’s geometry; if the geometry is a polyline the point is uniform in arc length, if a polygon it is uniform in surface area via rejection sampling on an enclosing rectangle.

Once a coordinate candidate has been drawn, a **land‑water sanity pass** runs. The candidate is tested against the 1:10 m Natural‑Earth land polygon via point‑in‑polygon; if it is in water, or if it lies more than fifty metres from the parent road when the prior is a road network, the point is rejected and the sampling loop repeats. This ensures that all sites end up on dry land or precisely on the intended transport corridor. Because the spatial‑prior weights can occasionally put mass directly on the shoreline, the expected number of iterations is still low; the loop terminates almost always on the first or second try.

Every accepted coordinate is tagged with a **`prior_tag`** string that records the exact prior artefact (for example, `pop_density_hrsl_2024q2`, `osm_primary_road_aadt_202503`) and the numeric weight value that was used in the CDF. The tag is written into the site catalogue so that a GIS audit can sample rows stratified by tag and overlay them on the underlying map to verify plausibility.

Elevation is not needed for fraud‑detection models, but some downstream anomaly features rely on approximate travel time. Rather than query a digital elevation model for every point, the engine stores the **Haversine distance to the country’s capital** and, when the prior is a road network, the shortest on‑network driving distance to the capital as well. The former is computed analytically, the latter by invoking a pre‑built contraction‑hierarchies routing graph loaded into RAM. Both numbers are recorded in the catalogue and serve as cheap proxies for remoteness without bloating the temporal generator.

Edge cases arise when a country is so small—or an MCC so specialised—that the chosen prior has zero support inside its borders. One example is a land‑locked micro‑state with no roads that meet the primary‑road tag. To maintain determinism the build falls back to a **country‑level population raster** in those cases. The fallback is not silent: the catalogue writes `prior_tag='FALLBACK_POP'` and CI checks that the fraction of fallbacks remains below one percent globally. If a reviewer considers the fallback rate too high they can add a bespoke prior for the missing category.

After the coordinate is finalised, the engine calls `tz_world.lookup(lat, lon)` to retrieve the **civil time zone**, but only to verify that the coordinate lies in the expected country; full time‑zone assignment is deferred to the next sub‑segment. If the lookup returns a time‑zone whose ISO‑alpha‑2 country code does not match the country assigned at the allocation stage, that is evidence of a border overlap or a data‑error in the prior and the point is resampled. Because border overlaps are rare and the engine logs each failure with the offending geometry ID, a reviewer can isolate and inspect problematic priors.

Finally, the **foot‑traffic scalar** is computed. The engine multiplies a category‑specific load factor κ (read from `footfall_coefficients.yaml`) with the numeric weight of the raster pixel or vector feature and then with a Log‑Normal residual. The residual’s variance parameter σ² sits in the same YAML artefact and is tuned so that, in a dry run of the temporal engine, the Fano factor of hourly legitimate counts lies inside the 1.3–1.7 corridor. If κ or σ² change, the catalogue’s manifest hash changes, forcing regeneration downstream; this prevents silent drift in temporal dispersion.

Each completed site row is written immediately to a Parquet partition keyed by `merchant_id` and `site_id` to make the build crash‑tolerant; a re‑run with the same seed picks up only missing partitions. When all merchants are processed the build writes a **manifest JSON** that records the seed, the composite hash of every spatial artefact, and the wall‑clock build time. That manifest is itself checksummed and appended to the top‑level catalogue directory. Any downstream simulation step verifies the checksum before proceeding so that an out‑of‑date catalogue cannot be paired with newer artefacts by accident.

Because every coordinate arises from a publicly documentable prior, every randomness comes from a known seed in an isolated Philox stream, every fallback is logged, every tag is retained, and every artefact is hashed into the manifest, the “Placing outlets on the planet” engine withstands not only statistical checks but forensic audits of the entire spatial pipeline.
