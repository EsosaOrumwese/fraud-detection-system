The generator’s opening manoeuvre is to transform the symbolic `merchant_id` rows in the bare `transaction_schema` into a concrete list of outlet stubs—still placeless, still timezone‑agnostic, but already carrying every decision about *how many* outlets the merchant operates and *in which countries* the chain will legally trade. The reviewer must be able to rerun this stage, byte‑for‑byte, months later and see identical results, so the narrative below treats determinism and auditability as first‑class requirements.

---

The build process begins with a tight handshake between code and data: the pipeline enumerates every artefact bundle that lives under *git LFS* and is tagged by semantic version—`hurdle_coefficients.yaml`, `crossborder_hyperparams.yaml`, **the entire hierarchy of spatial priors in `spatial_prior_bundle/`**, and the *network‑share vectors* directory. For each individual file the loader computes a SHA‑256 digest; it then **XOR‑reduces** all those digests together with the git commit hash to yield a single 256‑bit manifest fingerprint. That fingerprint is written to (a) the top‑level `_manifest.json`, (b) every Parquet file’s `comment` field, and (c) the RNG master‑seed derivation string, so that any artefact changed without a version bump becomes instantly visible—downstream readers reject mismatching manifests at open time.


With the governing artefacts frozen, the code constructs the *merchant design matrix*. For each merchant the matrix holds three fixed‑effect columns—MCC one‑hot, channel one‑hot, and an ordinal development bucket derived from the merchant’s registered country GDP per capita—and an intercept. GDP per capita is looked up in the World‑Bank data vintage released on 2025‑04‑15 and frozen by SHA‑256; subsequent World‑Bank updates never alter historical runs because the artefact path embeds that release date. It is discretised into five buckets using Jenks natural breaks; the mapping table is also stamped with its own SHA‑256 so reviewers can rerun the binning step on a later vintage. All bucket assignments are therefore deterministic functions of ISO‑3166 code and the versioned GDP data‑set.

The *single‑versus‑multi* decision is the first random action. The generator multiplies the design matrix row by the logistic‑regression coefficient vector stored in `hurdle_coefficients.yaml`, applies the logistic function, and obtains a probability π that this merchant is multi‑site. Philox 2¹²⁸ supplies a single uniform deviate; if it is less than π the merchant advances to the multi‑site branch, otherwise it is locked to exactly one outlet. Because the logistic coefficients were estimated on a proprietary acquirer sample and released only as YAML, the open‑source code never sees raw data yet reproduces the same probabilities. The Bernoulli outcome is persisted to the row so that auditors can stratify later statistics by single‑versus‑multi without re‑deriving the decision.


For every merchant declared multi‑site the pipeline first computes the mean μ and dispersion φ of a negative‑binomial whose log‑link uses exactly the same design matrix that powered the logistic, but with one extra twist: the dispersion equation carries a continuous GDP‑per‑capita covariate in log φ. When GDP is low the term lifts log φ, swelling the variance and stretching the right tail that governs very large outlet counts; when GDP is high the term suppresses log φ, shrinking the variance and mirroring the more disciplined rollout cadence seen in affluent markets. A deviate **N ∼ NB(μ, φ)** is then sampled from the merchant‑specific Philox sub‑stream. If that variate equals **0 or 1** the algorithm rejects it, increments the “NB‑rejection” counter, and draws again from the *same* stream state; the loop terminates only once N ≥ 2, guaranteeing that the *multi‑site* branch cannot regress to single‑site logic. Every rejection writes the pre‑draw counter, the discarded value, and the post‑draw counter to `rng_audit.log`, deposited next to the final Parquet. Nightly CI scrapes that log and surfaces the total rejection count so that any upward drift signals a latent misspecification long before it can bleed into geographic dispersion or LGCP calibration.

With the domestic count N pinned, the engine next decides how far the chain spreads beyond its home borders. It samples a **zero‑truncated Poisson**, K ∼ ZTPoisson($\lambda_{extra}$), where $\lambda_{extra}=\theta_0+\theta_1\log{N}$ and the θ‑hyperparameters come from `crossborder_hyperparams.yaml`. A classical rejection sampler repeats draws from Poisson($\lambda_{extra}$) until the outcome is strictly positive (K ≥ 1), thereby guaranteeing that any merchant flagged “cross‑border” actually spans at least one foreign jurisdiction. The probability mass excluded at zero is renormalised inside the rejection logic, and the number of rejections is appended to `rng_audit.log` under a dedicated `ztp_rejections` field so analysts can track the shape of the tail over time.

When K is non‑zero, the pipeline assembles a candidate list of foreign jurisdictions. The list is sampled without replacement from the ISO‑3166 universe by probabilities proportional to a pre‑computed *network share vector*. That vector captures the fraction of the home currency’s card spend that settles abroad. The vector lives at `artefacts/network_share_vectors/settlement_shares_2024Q4.parquet`; its SHA‑256 digest is folded into the manifest fingerprint, and a quarterly release cadence bumps both file name and semantic version (e.g., `…_2025Q1.parquet`) so old manifests remain resolvable and no ambiguity arises about which vintage drove a particular synthetic cohort. With K extras selected and the home country appended, the stage is set for a Dirichlet allocation.

The code loads the α‑vector applicable to the tuple `(home_country, mcc_code, channel)` from `crossborder_hyperparams.yaml`; the vector has length K+1 and has been calibrated so that the expected outbound ratio for each originating country reproduces historical cross‑border settlement. A single draw from Dirichlet(α) produces fractional weights w. Multiplying w by the integer outlet count N gives real‑valued allocations; largest‑remainder rounding converts them to integers that still sum to N. The rounding procedure is deterministic: floor every weight, compute the deficit d = N − ∑ floor, order the residual fractions descending and give one extra outlet to the first d entries. Because fractions sort stably, two runs with the same floating‑point output yield identical integer allocations—an important subtlety when floating‑point tie‑breakers can otherwise spoil reproducibility.

At this point the algorithm assigns monotonically increasing `site_id`s. It sorts the provisional outlet catalogue first by `country_iso` ascending and then by the **stable tie‑break rank** that emerged from largest‑remainder rounding. Inside each `(merchant_id, country_iso)` block it assigns a zero‑padded, four‑digit sequence number beginning at `'0001'`. Thus, the composite `site_id` = `merchant_id` ∥ `sequence` is length‑invariant across all markets, making byte‑level catalogue diffs clean and ensuring that downstream schema validators—many of which apply fixed‑width regular expressions—operate without brittle special‑case logic. Each stub row now contains `merchant_id`, `site_id`, `country_iso`, the single‑versus‑multi flag, the raw draw N, the final integer outlet count per country, and the manifest fingerprint. The PRNG stream is then jumped forward by a *module‑specific stride* derived by hashing the string `"cross-border-allocation"`; subsequent modules therefore consume untouched entropy.

The narrative halts here intentionally. Every outlet is counted and legally domiciled, but no latitude, longitude or timezone exists yet. The coordinate sampling logic belongs to the next sub‑segment, and because the manifest fingerprint already reflects every parameter that influenced the decisions above, the downstream geographic sampler can never retroactively change outlet counts or country assignments without emitting a new fingerprint and therefore establishing an explicit break in lineage. This strict layering—abstract counts first, concrete coordinates later—is what allows the entire location pipeline to be audited in two clean, orthogonal chunks, a property that weighs heavily in our favour during the inevitable ruthless review.
