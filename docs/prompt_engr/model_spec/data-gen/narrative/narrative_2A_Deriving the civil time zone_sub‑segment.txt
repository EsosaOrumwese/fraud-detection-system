The material that follows is not a synopsis; it is a verbatim exposure of every premise, data‑source dependency, numerical convention and protective rail that underpins the **“Deriving the civil time zone”** stage. The language is continuous and discursive so that the implementer can translate it line‑for‑line into code without needing to infer anything. All references to artefacts—whether shapefiles, YAML registries or version strings—are the literal filenames used in the build tree, and every safeguard is identified by the exact condition that triggers it.

---

The stage begins by opening the **tz‑world shapefile** whose basename is `tz_world_2025a.shp`. Its SHA‑256 digest is embedded in the catalogue manifest, under the key `tz_polygon_digest`. The file’s coordinate reference system is EPSG:4326; the build refuses to continue if `fiona.open()` reports anything else. Every polygon record’s attribute table contains the column `TZID`. Immediately after loading, the code populates an **STR‑tree spatial index** with each polygon’s bounding box; the index’s construction is deterministic because the polygons are inserted in lexicographic order of `TZID`. Determinism of the index matters because the order in which bounding‐box enlargement occurs inside the STR‑tree algorithm affects its shape and therefore its traversal path later. An audit routine computes an MD5 digest of the pickled STR‑tree object and stores it in the manifest. A re‑run with the same shapefile and seed produces the identical digest, proving that the spatial index itself is reproducible.

When an outlet coordinate is fed into the engine, the STR‑tree is queried for candidate polygons. The shortlist is traversed in the order returned by the index; for each polygon the engine calls `prepared_polygon.contains(point)` from the *Shapely* library. If exactly one polygon claims the point the `TZID` from that polygon becomes the site’s provisional civil time zone. If zero polygons claim the point, the engine raises `TimeZoneLookupError` with the offending latitude, longitude and the `prior_tag` that generated the coordinate; the CI harness intercepts the exception, marks the artefact as defective and halts the build. If two polygons claim the point—a circumstance that occurs along borders where vertex precision is lower than floating‑point representation—the engine performs a deterministic tie‑break. Let the two polygons be $P_1$ and $P_2$ with areas $A_1$ and $A_2$. The algorithm computes the vector from the point $x$ to the centroid of the smaller polygon, $c = \operatorname{centroid}(\arg\min\{A_1, A_2\})$. It normalises this vector to unit length, multiplies it by a scalar ε where **ε = 0.0001 degrees**, then adds the displacement to $x$ producing $x' = x + ε\frac{c-x}{||c-x||}$. The scalar ε is not an in‑code literal; it is read from `tz_nudge.yml`. Because the smallest strip of land in tz‑world exceeds one kilometre in width, a 0.0001° nudge (\~ 11 m at the equator) cannot push the point into an unrelated third polygon. The updated coordinate $x'$ becomes the query point, and the lookup repeats; by construction it yields a single owner. The nudge vector is stored in the site catalogue columns `nudge_lat` and `nudge_lon` so that a forensic examiner can replicate the calculation with independent software.

Tz‑world polygons, however comprehensive, lag behind political decrees. All deviations are centralised in the file **`tz_overrides.yaml`**. Each override item is a mapping with fields: `scope`, `tzid`, `evidence_url`, `expiry_yyyy_mm_dd`. The scope can be one of three forms: the string `"country:CA"` meaning the entire ISO country, the pattern `"mcc:5411"` meaning every grocery outlet, or the tuple `("merchant_id", "site_id")` targeting an individual site. A `pre‑commit` Git hook verifies that `evidence_url` is a valid URL and that `expiry` is parsable. During the lookup phase the engine checks overrides in precedence order: site‑specific first, then MCC‑wide, then country‑wide. If an override matches, its `TZID` supersedes the polygon result. Nightly CI opens the fresh site catalogue, walks through every row, reapplies the override logic and asserts that at least one override location would change if the shapefile alone were used; a count of zero triggers a rejection because it proves the override is obsolete.

With a final `TZID` attached, the engine consults the **IANA zoneinfo database** version `"tzdata2025a"`. The version string lives in `zoneinfo_version.yml`; changing it changes the manifest hash. Zoneinfo is queried through the standard `zoneinfo.ZoneInfo` API. For each distinct `TZID` present in the catalogue the engine enumerates its `_utc_transition_times` list, intersects it with the simulation horizon and writes a compact two‑column timetable `(transition_epoch, offset_minutes)`. The timetable is stored in memory as two numpy arrays of dtype `int64` and `int16` respectively, then compressed with run‑length encoding before being attached to the `tz_cache` singleton as Python bytes. Run‑length encoding is adopted because zones that abolished DST decades ago consist chiefly of repeated offsets, and encoding each repeated segment as `(count, value)` pairs reduces average size to about 1 kB per zone. A memory gauge after construction writes the exact byte size of the entire cache into the manifest; if a future upgrade to the IANA file inadvertently triples memory, CI surfaces the expansion where a silent leak could otherwise go unnoticed.

When the temporal engine later generates a **local epoch second** $t_{\text{local}}$, the civil‑time module must map it to UTC. The timetable for the site’s zone is bisection‑searched (via `numpy.searchsorted`) to locate the most recent transition epoch not later than $t_{\text{local}}$. The associated offset $o$ in minutes is read; the provisional UTC epoch is $t_{\text{utc}} = t_{\text{local}} - 60o$. If $t_{\text{local}}$ lies strictly between a transition epoch and that epoch plus the forward gap length, it is illegal. The engine replaces it by $t_{\text{legal}} = t_{\text{gap\_end}}$, sets `dst_adjusted=True` and writes `gap_seconds = t_{\text{legal}} - t_{\text{local}}`. That surplus interval is returned to the arrival engine so that the waiting‑time distribution remains statistically faithful. If instead $t_{\text{local}}$ matches two legal instants in the fall‑back fold, the engine determines the correct fold by parity of a hash on `(site_id, t_{\text{local}})`, assigns `fold=0` for the first hour or `fold=1` for the second, and moves on. This parity rule ensures identical behaviour across re‑runs even though CPython’s hash randomisation changes between interpreter invocations.

The module then writes the integer **local‑time offset** $o$ to the transaction buffer. The row now contains `(event_time_utc, local_time_offset, dst_adjusted, fold)`; these four fields are sufficient for any consumer to reconstruct unambiguously the civil time of the transaction. A validation routine outside the critical path performs a Monte‑Carlo check each night: it samples one million rows, reverts them to local time by adding the offset, looks up the `TZID` in tz‑world, checks whether that `TZID` equals the row’s original zone or if the row is covered by an override, and asserts equality. Failure causes the build to abort, guaranteeing that no silent inconsistency can creep in.

The random‑number generator does not influence the civil‑time mapping directly, but the deterministic parity hash for fold assignment uses SHA‑256 with the global seed concatenated to `(site_id, local_epoch)`, ensuring that fold disambiguation remains stable under seed changes but will differ if `site_id` mutates, as required for reproducibility.

Every constant—ε for the border nudge, the IANA version string, the maximum allowed memory for the timetable cache—is stored in a YAML artefact whose digest is printed inside the site catalogue manifest. Every external shapefile, every override line and every computed STR‑tree digest is likewise captured. Therefore, if a reviewer alters *any* of these inputs, downstream consumers will detect the manifest drift and insist on a complete regeneration, maintaining the chain of reproducibility.
