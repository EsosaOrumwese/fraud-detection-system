name: dev-full-m6f-streaming-active

on:
  workflow_dispatch:
    inputs:
      phase_mode:
        description: "Execution mode: m6f (streaming-active lane), m6g (P6 rollup), m6h (P7 ingest-commit), m6i (P7 rollup), m6j (M6 closure sync), m7a (M7 handle closure), or m7b (P8.A entry precheck)"
        required: true
        default: "m6f"
        type: string
      aws_region:
        description: "AWS region for EMR/EKS/DynamoDB/S3 operations"
        required: true
        default: "eu-west-2"
        type: string
      aws_role_to_assume:
        description: "OIDC role ARN used by GitHub Actions"
        required: true
        type: string
      platform_run_id:
        description: "Pinned platform run id"
        required: true
        type: string
      scenario_run_id:
        description: "Pinned scenario run id"
        required: true
        type: string
      upstream_m6e_execution:
        description: "Upstream M6.E execution id used by M6.F capture"
        required: true
        type: string
      upstream_m6f_execution:
        description: "Upstream M6.F execution id used by M6.G rollup (required when phase_mode=m6g)"
        required: false
        default: ""
        type: string
      upstream_m6g_execution:
        description: "Upstream M6.G execution id used by M6.H/M6.I lanes"
        required: false
        default: ""
        type: string
      upstream_m6h_execution:
        description: "Upstream M6.H execution id used by M6.I rollup (required when phase_mode=m6i)"
        required: false
        default: ""
        type: string
      upstream_m6d_execution:
        description: "Upstream M6.D execution id used by M6.J closure sync (required when phase_mode=m6j)"
        required: false
        default: ""
        type: string
      upstream_m6i_execution:
        description: "Upstream M6.I execution id used by M6.J closure sync (required when phase_mode=m6j)"
        required: false
        default: ""
        type: string
      m6_execution_id:
        description: "Optional fixed M6.F execution id (default: m6f_p6b_streaming_active_<timestamp>)"
        required: false
        default: ""
        type: string
      emr_virtual_cluster_id:
        description: "EMR on EKS virtual cluster id"
        required: true
        default: "3cfszbpz28ixf1wmmd2roj571"
        type: string
      artifacts_bucket:
        description: "S3 bucket for lane worker script artifact"
        required: true
        default: "fraud-platform-dev-full-artifacts"
        type: string
      evidence_bucket:
        description: "S3 bucket for M6.F run-control evidence"
        required: true
        default: "fraud-platform-dev-full-evidence"
        type: string
      wsp_ref:
        description: "WSP stream lane ref"
        required: true
        default: "fraud-platform-dev-full-wsp-stream-v0"
        type: string
      sr_ready_ref:
        description: "SR ready lane ref"
        required: true
        default: "fraud-platform-dev-full-sr-ready-v0"
        type: string
      ig_idempotency_table:
        description: "IG idempotency DynamoDB table name"
        required: true
        default: "fraud-platform-dev-full-ig-idempotency"
        type: string
      runtime_path:
        description: "Pinned runtime path for this phase execution"
        required: true
        default: "EKS_EMR_ON_EKS"
        type: string
      lag_threshold:
        description: "Lag threshold for M6.F capture"
        required: true
        default: "10"
        type: string
      iterations:
        description: "Bounded worker iterations for lane refs"
        required: true
        default: "600"
        type: string
      sleep_seconds:
        description: "Worker sleep seconds between iterations"
        required: true
        default: "1.0"
        type: string
      emr_log_group_name:
        description: "CloudWatch log group for EMR jobs"
        required: true
        default: "/emr-eks/fraud-platform-dev-full"
        type: string
      eks_cluster_name:
        description: "EKS cluster name for preflight capacity check"
        required: true
        default: "fraud-platform-dev-full"
        type: string
      eks_nodegroup_name:
        description: "EKS nodegroup name for preflight capacity check"
        required: true
        default: "fraud-platform-dev-full-m6f-workers"
        type: string

permissions:
  contents: read
  id-token: write

concurrency:
  group: dev-full-m6-phase-${{ inputs.phase_mode }}-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  run_m6f_remote:
    name: Run M6.F streaming-active lane remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm6f' }}
    runs-on: ubuntu-latest
    env:
      IG_BASE_URL: "https://ehwznd2uw7.execute-api.eu-west-2.amazonaws.com/v1"
      IG_INGEST_PATH: "/ingest/push"
      SSM_IG_API_KEY_PATH: "/fraud-platform/dev_full/ig/api_key"
      FLINK_EKS_NAMESPACE: "fraud-platform-rtdl"
      LANE_WORKER_IMAGE_URI: "230372904534.dkr.ecr.eu-west-2.amazonaws.com/fraud-platform-dev-full@sha256:49eb6cb0c5e33061fae4d1aaceeac2e44600adb5c4250436be9ac8395ed29cb2"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Resolve IG API key from SSM
        shell: bash
        run: |
          set -euo pipefail
          IG_API_KEY="$(aws ssm get-parameter \
            --name "${SSM_IG_API_KEY_PATH}" \
            --with-decryption \
            --query 'Parameter.Value' \
            --output text)"
          if [[ -z "${IG_API_KEY}" || "${IG_API_KEY}" == "None" ]]; then
            echo "M6.F fail-closed: unable to resolve IG API key from SSM."
            exit 1
          fi
          echo "::add-mask::${IG_API_KEY}"
          echo "IG_API_KEY=${IG_API_KEY}" >> "${GITHUB_ENV}"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Compute execution metadata
        id: run_meta
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m6f_p6b_streaming_active_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m6/${EXEC_ID}"
          SUBMIT_RECEIPT="${RUN_DIR}/m6f_emr_submit_receipt.json"
          WORKER_S3_URI="s3://${{ inputs.artifacts_bucket }}/dev_substrate/m6/m6_stream_ref_worker.py"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m6_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"
          echo "submit_receipt=${SUBMIT_RECEIPT}" >> "$GITHUB_OUTPUT"
          echo "worker_s3_uri=${WORKER_S3_URI}" >> "$GITHUB_OUTPUT"

      - name: Preflight worker-capacity gate
        shell: bash
        run: |
          set -euo pipefail
          STATUS="$(aws eks describe-nodegroup \
            --region "${{ inputs.aws_region }}" \
            --cluster-name "${{ inputs.eks_cluster_name }}" \
            --nodegroup-name "${{ inputs.eks_nodegroup_name }}" \
            --query 'nodegroup.status' \
            --output text)"
          echo "Nodegroup status: ${STATUS}"
          if [[ "${STATUS}" != "ACTIVE" ]]; then
            echo "M6.F fail-closed: nodegroup is not ACTIVE."
            exit 1
          fi

      - name: Upload lane worker script artifact
        shell: bash
        run: |
          set -euo pipefail
          aws s3 cp \
            scripts/dev_substrate/m6_stream_ref_worker.py \
            "${{ steps.run_meta.outputs.worker_s3_uri }}" \
            --region "${{ inputs.aws_region }}"

      - name: Submit lane refs (runtime-path aware)
        id: submit
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "${{ steps.run_meta.outputs.run_dir }}"
          if [[ "${{ inputs.runtime_path }}" == "EKS_FLINK_OPERATOR" ]]; then
            aws eks update-kubeconfig \
              --region "${{ inputs.aws_region }}" \
              --name "${{ inputs.eks_cluster_name }}" >/dev/null
            NS="${FLINK_EKS_NAMESPACE}"
            kubectl get namespace "${NS}" >/dev/null
            EXEC_SAFE="$(echo "${{ steps.run_meta.outputs.m6_execution_id }}" | tr '[:upper:]_' '[:lower:]-' | sed -E 's/[^a-z0-9-]//g' | cut -c1-24)"
            if [[ -z "${EXEC_SAFE}" ]]; then
              EXEC_SAFE="m6f"
            fi
            CM_NAME="m6f-worker-${EXEC_SAFE}"
            WSP_JOB_ID="m6f-wsp-${EXEC_SAFE}"
            SR_JOB_ID="m6f-sr-${EXEC_SAFE}"
            STARTED_AT_UTC="$(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            kubectl -n "${NS}" create configmap "${CM_NAME}" \
              --from-file=m6_stream_ref_worker.py=scripts/dev_substrate/m6_stream_ref_worker.py \
              --dry-run=client -o yaml | kubectl apply -f -
            cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: ${WSP_JOB_ID}
            namespace: ${NS}
            labels:
              fp.phase: M6.F
              fp.runtime_path: EKS_FLINK_OPERATOR
          spec:
            ttlSecondsAfterFinished: 600
            template:
              metadata:
                labels:
                  fp.phase: M6.F
              spec:
                restartPolicy: Never
                containers:
                - name: lane
                  image: ${LANE_WORKER_IMAGE_URI}
                  command: ["/bin/sh","-lc"]
                  args:
                  - >
                    python /opt/worker/m6_stream_ref_worker.py
                    --lane-ref "${{ inputs.wsp_ref }}"
                    --platform-run-id "${{ inputs.platform_run_id }}"
                    --scenario-run-id "${{ inputs.scenario_run_id }}"
                    --phase-id "P6.B"
                    --iterations "${{ inputs.iterations }}"
                    --sleep-seconds "${{ inputs.sleep_seconds }}"
                    --ig-base-url "${IG_BASE_URL}"
                    --ig-ingest-path "${IG_INGEST_PATH}"
                    --ig-api-key "${IG_API_KEY}"
                  volumeMounts:
                  - name: worker-script
                    mountPath: /opt/worker
                volumes:
                - name: worker-script
                  configMap:
                    name: ${CM_NAME}
          EOF
            cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: ${SR_JOB_ID}
            namespace: ${NS}
            labels:
              fp.phase: M6.F
              fp.runtime_path: EKS_FLINK_OPERATOR
          spec:
            ttlSecondsAfterFinished: 600
            template:
              metadata:
                labels:
                  fp.phase: M6.F
              spec:
                restartPolicy: Never
                containers:
                - name: lane
                  image: ${LANE_WORKER_IMAGE_URI}
                  command: ["/bin/sh","-lc"]
                  args:
                  - >
                    python /opt/worker/m6_stream_ref_worker.py
                    --lane-ref "${{ inputs.sr_ready_ref }}"
                    --platform-run-id "${{ inputs.platform_run_id }}"
                    --scenario-run-id "${{ inputs.scenario_run_id }}"
                    --phase-id "P6.B"
                    --iterations "${{ inputs.iterations }}"
                    --sleep-seconds "${{ inputs.sleep_seconds }}"
                    --ig-base-url "${IG_BASE_URL}"
                    --ig-ingest-path "${IG_INGEST_PATH}"
                    --ig-api-key "${IG_API_KEY}"
                  volumeMounts:
                  - name: worker-script
                    mountPath: /opt/worker
                volumes:
                - name: worker-script
                  configMap:
                    name: ${CM_NAME}
          EOF
            export STARTED_AT_UTC WSP_JOB_ID SR_JOB_ID CM_NAME
            python - <<'PY'
          import json
          import os
          from pathlib import Path
          Path("${{ steps.run_meta.outputs.submit_receipt }}").write_text(json.dumps({
              "started_at_utc": os.environ.get("STARTED_AT_UTC", ""),
              "virtual_cluster_id": "${{ inputs.emr_virtual_cluster_id }}",
              "wsp_ref": "${{ inputs.wsp_ref }}",
              "wsp_job_id": os.environ.get("WSP_JOB_ID", ""),
              "sr_ready_ref": "${{ inputs.sr_ready_ref }}",
              "sr_ready_job_id": os.environ.get("SR_JOB_ID", ""),
              "platform_run_id": "${{ inputs.platform_run_id }}",
              "scenario_run_id": "${{ inputs.scenario_run_id }}",
              "runtime_path": "EKS_FLINK_OPERATOR",
              "configmap_name": os.environ.get("CM_NAME", ""),
          }, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
          PY
            echo "configmap_name=${CM_NAME}" >> "$GITHUB_OUTPUT"
          else
            python scripts/dev_substrate/m6_submit_emr_refs.py \
              --region "${{ inputs.aws_region }}" \
              --virtual-cluster-id "${{ inputs.emr_virtual_cluster_id }}" \
              --execution-role-arn "arn:aws:iam::230372904534:role/fraud-platform-dev-full-flink-execution" \
              --release-label "emr-6.15.0-latest" \
              --script-s3-uri "${{ steps.run_meta.outputs.worker_s3_uri }}" \
              --platform-run-id "${{ inputs.platform_run_id }}" \
              --scenario-run-id "${{ inputs.scenario_run_id }}" \
              --wsp-ref "${{ inputs.wsp_ref }}" \
              --sr-ready-ref "${{ inputs.sr_ready_ref }}" \
              --ig-base-url "${IG_BASE_URL}" \
              --ig-ingest-path "${IG_INGEST_PATH}" \
              --ig-api-key "${IG_API_KEY}" \
              --log-group-name "${{ inputs.emr_log_group_name }}" \
              --iterations "${{ inputs.iterations }}" \
              --sleep-seconds "${{ inputs.sleep_seconds }}" \
              --output-json "${{ steps.run_meta.outputs.submit_receipt }}"
            echo "configmap_name=" >> "$GITHUB_OUTPUT"
          fi
          WSP_JOB_ID="$(python - <<'PY'
          import json
          from pathlib import Path
          payload = json.loads(Path("${{ steps.run_meta.outputs.submit_receipt }}").read_text(encoding="utf-8"))
          print(payload.get("wsp_job_id", ""))
          PY
          )"
          SR_JOB_ID="$(python - <<'PY'
          import json
          from pathlib import Path
          payload = json.loads(Path("${{ steps.run_meta.outputs.submit_receipt }}").read_text(encoding="utf-8"))
          print(payload.get("sr_ready_job_id", ""))
          PY
          )"
          if [[ -z "${WSP_JOB_ID}" || -z "${SR_JOB_ID}" ]]; then
            echo "M6.F fail-closed: lane submission receipt missing job ids."
            exit 1
          fi
          STARTED_AT_UTC="$(python - <<'PY'
          import json
          from pathlib import Path
          payload = json.loads(Path("${{ steps.run_meta.outputs.submit_receipt }}").read_text(encoding="utf-8"))
          print(payload.get("started_at_utc", ""))
          PY
          )"
          if [[ -z "${STARTED_AT_UTC}" ]]; then
            echo "M6.F fail-closed: submission receipt missing started_at_utc."
            exit 1
          fi
          echo "wsp_job_id=${WSP_JOB_ID}" >> "$GITHUB_OUTPUT"
          echo "sr_job_id=${SR_JOB_ID}" >> "$GITHUB_OUTPUT"
          echo "lane_window_start_utc=${STARTED_AT_UTC}" >> "$GITHUB_OUTPUT"

      - name: Probe lane refs for RUNNING (non-blocking pre-capture)
        id: probe
        continue-on-error: true
        shell: bash
        run: |
          set -euo pipefail
          WSP_STATE="UNKNOWN"
          SR_STATE="UNKNOWN"
          if [[ "${{ inputs.runtime_path }}" == "EKS_FLINK_OPERATOR" ]]; then
            aws eks update-kubeconfig \
              --region "${{ inputs.aws_region }}" \
              --name "${{ inputs.eks_cluster_name }}" >/dev/null
            get_phase() {
              local job_name="$1"
              local pod_name=""
              pod_name="$(kubectl get pods -n "${FLINK_EKS_NAMESPACE}" -l "job-name=${job_name}" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
              if [[ -z "${pod_name}" ]]; then
                echo "SUBMITTED"
                return 0
              fi
              kubectl get pod "${pod_name}" -n "${FLINK_EKS_NAMESPACE}" -o jsonpath='{.status.phase}' 2>/dev/null || echo "UNKNOWN"
            }
            for i in $(seq 1 12); do
              WSP_STATE="$(get_phase "${{ steps.submit.outputs.wsp_job_id }}")"
              SR_STATE="$(get_phase "${{ steps.submit.outputs.sr_job_id }}")"
              echo "attempt=${i}/12 wsp_state=${WSP_STATE} sr_state=${SR_STATE}"
              if [[ "${WSP_STATE}" == "Running" && "${SR_STATE}" == "Running" ]]; then
                break
              fi
              sleep 5
            done
            [[ "${WSP_STATE}" == "Running" ]] && WSP_STATE="RUNNING"
            [[ "${SR_STATE}" == "Running" ]] && SR_STATE="RUNNING"
          else
            wait_for_running() {
              local job_id="$1"
              local attempts=12
              local sleep_sec=5
              local idx=1
              local state="UNKNOWN"
              while [[ "${idx}" -le "${attempts}" ]]; do
                state="$(aws emr-containers describe-job-run \
                  --region "${{ inputs.aws_region }}" \
                  --virtual-cluster-id "${{ inputs.emr_virtual_cluster_id }}" \
                  --id "${job_id}" \
                  --query 'jobRun.state' \
                  --output text)"
                echo "job_id=${job_id} state=${state} attempt=${idx}/${attempts}" >&2
                if [[ "${state}" == "RUNNING" ]]; then
                  echo "${state}"
                  return 0
                fi
                if [[ "${state}" == "FAILED" || "${state}" == "CANCELLED" || "${state}" == "CANCEL_PENDING" ]]; then
                  echo "${state}"
                  return 0
                fi
                idx=$((idx + 1))
                sleep "${sleep_sec}"
              done
              echo "${state}"
              return 0
            }
            WSP_STATE="$(wait_for_running "${{ steps.submit.outputs.wsp_job_id }}")"
            SR_STATE="$(wait_for_running "${{ steps.submit.outputs.sr_job_id }}")"
          fi
          echo "wsp_state_override=${WSP_STATE}" >> "$GITHUB_OUTPUT"
          echo "sr_state_override=${SR_STATE}" >> "$GITHUB_OUTPUT"

      - name: Emit fallback IG bridge probes (equivalent ingress path)
        if: ${{ inputs.runtime_path == 'EKS_FLINK_OPERATOR' }}
        continue-on-error: true
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import time
          import urllib.error
          import urllib.request
          import os
          from pathlib import Path

          base_url = os.environ.get("IG_BASE_URL", "").rstrip("/")
          ingest_path = os.environ.get("IG_INGEST_PATH", "")
          if not ingest_path.startswith("/"):
              ingest_path = "/" + ingest_path
          ingest_url = f"{base_url}{ingest_path}"
          api_key = os.environ.get("IG_API_KEY", "")
          platform_run_id = "${{ inputs.platform_run_id }}"
          scenario_run_id = "${{ inputs.scenario_run_id }}"
          phase_id = "P6.B"
          lane_refs = [
              "${{ inputs.wsp_ref }}",
              "${{ inputs.sr_ready_ref }}",
          ]
          admitted = 0
          failed = 0
          failures = []
          started_epoch = int(time.time())

          for idx in range(12):
              lane_ref = lane_refs[idx % len(lane_refs)]
              event_id = f"m6f_bridge:{platform_run_id}:{started_epoch}:{idx}"
              payload = {
                  "platform_run_id": platform_run_id,
                  "scenario_run_id": scenario_run_id,
                  "phase_id": phase_id,
                  "event_class": "m6_lane_probe_bridge",
                  "event_id": event_id,
                  "runtime_lane": lane_ref,
                  "trace_id": event_id,
              }
              body = json.dumps(payload, separators=(",", ":"), ensure_ascii=True).encode("utf-8")
              req = urllib.request.Request(
                  ingest_url,
                  data=body,
                  method="POST",
                  headers={
                      "Content-Type": "application/json",
                      "X-IG-Api-Key": api_key,
                  },
              )
              try:
                  with urllib.request.urlopen(req, timeout=10) as resp:
                      status = int(resp.status)
                  if status == 202:
                      admitted += 1
                  else:
                      failed += 1
                      failures.append(f"unexpected_status:{status}")
              except urllib.error.HTTPError as exc:
                  failed += 1
                  failures.append(f"http_{exc.code}")
              except Exception as exc:  # pragma: no cover - runtime guard
                  failed += 1
                  failures.append(f"{type(exc).__name__}:{exc}")
              time.sleep(0.2)

          summary = {
              "captured_at_epoch": int(time.time()),
              "phase": "M6.F",
              "phase_id": phase_id,
              "runtime_path": "EKS_FLINK_OPERATOR",
              "ingest_url": ingest_url,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "attempted": 12,
              "admitted": admitted,
              "failed": failed,
              "failure_samples": failures[:8],
          }
          out = Path("${{ steps.run_meta.outputs.run_dir }}") / "m6f_ig_bridge_summary.json"
          out.write_text(json.dumps(summary, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
          print(json.dumps(summary, ensure_ascii=True))
          PY

      - name: Capture M6.F artifacts
        id: capture
        shell: bash
        run: |
          set -euo pipefail
          python scripts/dev_substrate/m6f_capture.py \
            --execution-id "${{ steps.run_meta.outputs.m6_execution_id }}" \
            --platform-run-id "${{ inputs.platform_run_id }}" \
            --scenario-run-id "${{ inputs.scenario_run_id }}" \
            --upstream-m6e-execution "${{ inputs.upstream_m6e_execution }}" \
            --region "${{ inputs.aws_region }}" \
            --runtime-path "${{ inputs.runtime_path }}" \
            --runtime-path-allowed "MSF_MANAGED|EKS_EMR_ON_EKS|EKS_FLINK_OPERATOR" \
            --runtime-path-fallback-blocker "M6P6-B2" \
            --virtual-cluster-id "${{ inputs.emr_virtual_cluster_id }}" \
            --wsp-job-id "${{ steps.submit.outputs.wsp_job_id }}" \
            --sr-ready-job-id "${{ steps.submit.outputs.sr_job_id }}" \
            --wsp-state-override "${{ steps.probe.outputs.wsp_state_override }}" \
            --sr-ready-state-override "${{ steps.probe.outputs.sr_state_override }}" \
            --wsp-ref "${{ inputs.wsp_ref }}" \
            --sr-ready-ref "${{ inputs.sr_ready_ref }}" \
            --lane-window-start-utc "${{ steps.submit.outputs.lane_window_start_utc }}" \
            --ig-idempotency-table "${{ inputs.ig_idempotency_table }}" \
            --lag-threshold "${{ inputs.lag_threshold }}" \
            --evidence-bucket "${{ inputs.evidence_bucket }}" \
            --local-output-root "runs/dev_substrate/dev_full/m6"

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta.outputs.run_dir }}/m6f_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta.outputs.run_dir }}/m6f_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": summary.get("next_gate"),
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          PY

      - name: Cancel temporary EMR lane refs
        if: ${{ always() && steps.submit.outcome == 'success' }}
        shell: bash
        run: |
          set -euo pipefail
          if [[ "${{ inputs.runtime_path }}" == "EKS_FLINK_OPERATOR" ]]; then
            aws eks update-kubeconfig \
              --region "${{ inputs.aws_region }}" \
              --name "${{ inputs.eks_cluster_name }}" >/dev/null
            kubectl delete job "${{ steps.submit.outputs.wsp_job_id }}" -n "${FLINK_EKS_NAMESPACE}" --ignore-not-found=true
            kubectl delete job "${{ steps.submit.outputs.sr_job_id }}" -n "${FLINK_EKS_NAMESPACE}" --ignore-not-found=true
            if [[ -n "${{ steps.submit.outputs.configmap_name }}" ]]; then
              kubectl delete configmap "${{ steps.submit.outputs.configmap_name }}" -n "${FLINK_EKS_NAMESPACE}" --ignore-not-found=true
            fi
          else
            python scripts/dev_substrate/m6_cancel_emr_refs.py \
              --region "${{ inputs.aws_region }}" \
              --virtual-cluster-id "${{ inputs.emr_virtual_cluster_id }}" \
              --job-id "${{ steps.submit.outputs.wsp_job_id }}" \
              --job-id "${{ steps.submit.outputs.sr_job_id }}"
          fi

      - name: Upload M6.F run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m6f-streaming-active-${{ steps.run_meta.outputs.timestamp }}
          path: ${{ steps.run_meta.outputs.run_dir }}
          if-no-files-found: warn

  run_m6g_remote:
    name: Run M6.G P6 gate rollup remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm6g' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6f_execution }}" ]]; then
            echo "M6.G fail-closed: upstream_m6f_execution input is required when phase_mode=m6g."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_g
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m6g_p6c_gate_rollup_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m6/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m6_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build M6.G P6 rollup artifacts
        shell: bash
        run: |
          set -euo pipefail
          python scripts/dev_substrate/m6g_rollup.py \
            --execution-id "${{ steps.run_meta_g.outputs.m6_execution_id }}" \
            --platform-run-id "${{ inputs.platform_run_id }}" \
            --scenario-run-id "${{ inputs.scenario_run_id }}" \
            --upstream-m6e-execution "${{ inputs.upstream_m6e_execution }}" \
            --upstream-m6f-execution "${{ inputs.upstream_m6f_execution }}" \
            --evidence-bucket "${{ inputs.evidence_bucket }}" \
            --region "${{ inputs.aws_region }}" \
            --local-output-root "runs/dev_substrate/dev_full/m6"

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_g.outputs.run_dir }}/m6g_execution_summary.json"
          VERDICT_PATH="${{ steps.run_meta_g.outputs.run_dir }}/m6g_p6_gate_verdict.json"
          BLOCKER_PATH="${{ steps.run_meta_g.outputs.run_dir }}/m6g_p6_blocker_register.json"
          export SUMMARY_PATH VERDICT_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          verdict = json.loads(Path(os.environ["VERDICT_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          verdict_value = str(verdict.get("verdict", "")).strip()
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "verdict": verdict_value,
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if verdict_value != "ADVANCE_TO_P7" or next_gate != "M6.H_READY":
              sys.exit(1)
          PY

      - name: Upload M6.G run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m6g-p6-gate-rollup-${{ steps.run_meta_g.outputs.timestamp }}
          path: ${{ steps.run_meta_g.outputs.run_dir }}
          if-no-files-found: warn

  run_m6h_remote:
    name: Run M6.H P7.A ingest-commit remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm6h' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Compute execution metadata
        id: run_meta_h
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m6h_p7a_ingest_commit_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m6/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m6_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build M6.H ingest-commit artifacts
        shell: bash
        run: |
          set -euo pipefail
          python scripts/dev_substrate/m6h_ingest_commit.py \
            --execution-id "${{ steps.run_meta_h.outputs.m6_execution_id }}" \
            --platform-run-id "${{ inputs.platform_run_id }}" \
            --scenario-run-id "${{ inputs.scenario_run_id }}" \
            --upstream-m6g-execution "${{ inputs.upstream_m6g_execution }}" \
            --evidence-bucket "${{ inputs.evidence_bucket }}" \
            --region "${{ inputs.aws_region }}" \
            --ig-idempotency-table "${{ inputs.ig_idempotency_table }}" \
            --idempotency-ttl-field "ttl_epoch" \
            --idempotency-ttl-seconds "259200" \
            --allow-empty-run-waiver "false" \
            --receipt-summary-path-pattern "evidence/runs/{platform_run_id}/ingest/receipt_summary.json" \
            --quarantine-summary-path-pattern "evidence/runs/{platform_run_id}/ingest/quarantine_summary.json" \
            --offsets-snapshot-path-pattern "evidence/runs/{platform_run_id}/ingest/kafka_offsets_snapshot.json" \
            --local-output-root "runs/dev_substrate/dev_full/m6"

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_h.outputs.run_dir }}/m6h_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_h.outputs.run_dir }}/m6h_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if next_gate != "M6.I_READY":
              sys.exit(1)
          PY

      - name: Upload M6.H run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m6h-ingest-commit-${{ steps.run_meta_h.outputs.timestamp }}
          path: ${{ steps.run_meta_h.outputs.run_dir }}
          if-no-files-found: warn

  run_m6i_remote:
    name: Run M6.I P7.B rollup remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm6i' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6h_execution }}" ]]; then
            echo "M6.I fail-closed: upstream_m6h_execution input is required when phase_mode=m6i."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_i
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m6i_p7b_gate_rollup_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m6/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m6_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build M6.I P7 rollup artifacts
        shell: bash
        run: |
          set -euo pipefail
          python scripts/dev_substrate/m6i_rollup.py \
            --execution-id "${{ steps.run_meta_i.outputs.m6_execution_id }}" \
            --platform-run-id "${{ inputs.platform_run_id }}" \
            --scenario-run-id "${{ inputs.scenario_run_id }}" \
            --upstream-m6g-execution "${{ inputs.upstream_m6g_execution }}" \
            --upstream-m6h-execution "${{ inputs.upstream_m6h_execution }}" \
            --evidence-bucket "${{ inputs.evidence_bucket }}" \
            --region "${{ inputs.aws_region }}" \
            --local-output-root "runs/dev_substrate/dev_full/m6"

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_i.outputs.run_dir }}/m6i_execution_summary.json"
          VERDICT_PATH="${{ steps.run_meta_i.outputs.run_dir }}/m6i_p7_gate_verdict.json"
          BLOCKER_PATH="${{ steps.run_meta_i.outputs.run_dir }}/m6i_p7_blocker_register.json"
          export SUMMARY_PATH VERDICT_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          verdict = json.loads(Path(os.environ["VERDICT_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          verdict_value = str(verdict.get("verdict", "")).strip()
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "verdict": verdict_value,
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if verdict_value != "ADVANCE_TO_M7" or next_gate != "M6.J_READY":
              sys.exit(1)
          PY

      - name: Upload M6.I run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m6i-p7-rollup-${{ steps.run_meta_i.outputs.timestamp }}
          path: ${{ steps.run_meta_i.outputs.run_dir }}
          if-no-files-found: warn

  run_m6j_remote:
    name: Run M6.J closure sync remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm6j' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6d_execution }}" ]]; then
            echo "M6.J fail-closed: upstream_m6d_execution input is required when phase_mode=m6j."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6g_execution }}" ]]; then
            echo "M6.J fail-closed: upstream_m6g_execution input is required when phase_mode=m6j."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6i_execution }}" ]]; then
            echo "M6.J fail-closed: upstream_m6i_execution input is required when phase_mode=m6j."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_j
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m6j_m6_closure_sync_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m6/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m6_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build M6.J closure artifacts
        shell: bash
        run: |
          set -euo pipefail
          python scripts/dev_substrate/m6j_closure_sync.py \
            --execution-id "${{ steps.run_meta_j.outputs.m6_execution_id }}" \
            --platform-run-id "${{ inputs.platform_run_id }}" \
            --scenario-run-id "${{ inputs.scenario_run_id }}" \
            --upstream-m6d-execution "${{ inputs.upstream_m6d_execution }}" \
            --upstream-m6g-execution "${{ inputs.upstream_m6g_execution }}" \
            --upstream-m6i-execution "${{ inputs.upstream_m6i_execution }}" \
            --evidence-bucket "${{ inputs.evidence_bucket }}" \
            --region "${{ inputs.aws_region }}" \
            --billing-region "us-east-1" \
            --budget-currency "USD" \
            --monthly-limit-amount "300" \
            --alert-1-amount "120" \
            --alert-2-amount "210" \
            --alert-3-amount "270" \
            --local-output-root "runs/dev_substrate/dev_full/m6"

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_j.outputs.run_dir }}/m6j_execution_summary.json"
          VERDICT_PATH="${{ steps.run_meta_j.outputs.run_dir }}/m6_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_j.outputs.run_dir }}/m6j_blocker_register.json"
          export SUMMARY_PATH VERDICT_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          verdict = json.loads(Path(os.environ["VERDICT_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          verdict_value = str(verdict.get("verdict", "")).strip()
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "verdict": verdict_value,
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if verdict_value != "ADVANCE_TO_M7" or next_gate != "M7_READY":
              sys.exit(1)
          PY

      - name: Upload M6.J run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m6j-closure-sync-${{ steps.run_meta_j.outputs.timestamp }}
          path: ${{ steps.run_meta_j.outputs.run_dir }}
          if-no-files-found: warn

  run_m7a_remote:
    name: Run M7.A handle closure remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7a' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6i_execution }}" ]]; then
            echo "M7.A fail-closed: upstream_m6i_execution input is required when phase_mode=m7a (use the M6 closure execution id)."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_7a
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m7a_p8p10_handle_closure_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build M7.A closure artifacts
        shell: bash
        env:
          EXECUTION_ID: ${{ steps.run_meta_7a.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7a.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
          UPSTREAM_M6_EXECUTION: ${{ inputs.upstream_m6i_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          upstream_m6_execution = os.environ["UPSTREAM_M6_EXECUTION"]

          captured_at = now_utc()
          s3 = boto3.client("s3", region_name=region)

          blockers = []
          read_errors = []

          upstream_key = f"evidence/dev_full/run_control/{upstream_m6_execution}/m6_execution_summary.json"
          upstream_summary, err = s3_get_json(s3, evidence_bucket, upstream_key)
          if err:
              read_errors.append(err)
              blockers.append({"code": "M7-B2", "message": "M6->M7 continuity evidence is missing or unreadable."})
              upstream_ok = False
          else:
              upstream_ok = bool(upstream_summary.get("overall_pass")) and str(upstream_summary.get("verdict", "")).strip() == "ADVANCE_TO_M7"
              if not upstream_ok:
                  blockers.append({"code": "M7-B2", "message": "Upstream M6 summary is not in ADVANCE_TO_M7 posture."})

          registry_path = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")
          registry_text = registry_path.read_text(encoding="utf-8")
          quoted = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          boolish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*(true|false)`", registry_text, flags=re.IGNORECASE)
          numish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*([0-9]+(?:\.[0-9]+)?)`", registry_text)
          handles = {k: v for k, v in quoted}
          handles.update({k: v.lower() for k, v in boolish})
          handles.update({k: v for k, v in numish})

          required_handles = [
              "FLINK_RUNTIME_PATH_ACTIVE",
              "FLINK_RUNTIME_PATH_ALLOWED",
              "PHASE_RUNTIME_PATH_MODE",
              "RUNTIME_PATH_SWITCH_IN_PHASE_ALLOWED",
              "FLINK_APP_RTDL_IEG_V0",
              "FLINK_APP_RTDL_OFP_V0",
              "FLINK_EKS_RTDL_IEG_REF",
              "FLINK_EKS_RTDL_OFP_REF",
              "K8S_DEPLOY_ARCHIVE_WRITER",
              "S3_ARCHIVE_RUN_PREFIX_PATTERN",
              "S3_ARCHIVE_EVENTS_PREFIX_PATTERN",
              "K8S_DEPLOY_DF",
              "K8S_DEPLOY_AL",
              "K8S_DEPLOY_DLA",
              "FP_BUS_RTDL_V1",
              "FP_BUS_AUDIT_V1",
              "K8S_DEPLOY_CM",
              "K8S_DEPLOY_LS",
              "FP_BUS_CASE_TRIGGERS_V1",
              "FP_BUS_LABELS_EVENTS_V1",
              "AURORA_CLUSTER_IDENTIFIER",
              "AURORA_MODE",
              "SSM_AURORA_ENDPOINT_PATH",
              "SSM_AURORA_USERNAME_PATH",
              "SSM_AURORA_PASSWORD_PATH",
          ]

          missing_handles = []
          placeholder_handles = []
          for key in required_handles:
              value = handles.get(key, "")
              if not value:
                  missing_handles.append(key)
                  continue
              value_norm = str(value).strip()
              if ("TO_PIN" in value_norm) or ("<" in value_norm) or ("TBD" in value_norm):
                  placeholder_handles.append(key)

          if missing_handles or placeholder_handles:
              blockers.append({"code": "M7-B1", "message": "Required M7 handles are missing or placeholder-valued."})

          slo_profile = {
              "IEG": {"records_per_second_min": 200, "latency_p95_ms_max": 500, "lag_messages_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "OFP": {"records_per_second_min": 200, "latency_p95_ms_max": 500, "lag_messages_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "ArchiveWriter": {"objects_per_minute_min": 50, "commit_latency_p95_ms_max": 1200, "backpressure_seconds_max": 30, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "write_error_rate_pct_max": 0.5},
              "DF": {"decisions_per_second_min": 150, "decision_latency_p95_ms_max": 800, "input_lag_messages_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "AL": {"actions_per_second_min": 150, "action_latency_p95_ms_max": 800, "retry_ratio_pct_max": 5.0, "backpressure_seconds_max": 30, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "DLA": {"audit_appends_per_second_min": 150, "append_latency_p95_ms_max": 1000, "queue_depth_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 0.5},
              "CaseTriggerBridge": {"events_per_second_min": 100, "bridge_latency_p95_ms_max": 700, "queue_depth_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "CM": {"case_writes_per_second_min": 100, "case_write_latency_p95_ms_max": 900, "queue_depth_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "LS": {"label_writes_per_second_min": 100, "commit_latency_p95_ms_max": 900, "writer_wait_seconds_max": 20, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
          }
          if not slo_profile:
              blockers.append({"code": "M7-B16", "message": "Per-component performance SLO profile is not pinned."})

          blockers_unique = []
          seen = set()
          for b in blockers:
              code = b.get("code")
              if code and code not in seen:
                  seen.add(code)
                  blockers_unique.append({"code": code, "message": b.get("message", "")})

          overall_pass = len(blockers_unique) == 0
          next_gate = "M7.B_READY" if overall_pass else "HOLD_REMEDIATE"

          m7a_snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M7.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_m6_execution": upstream_m6_execution,
              "upstream_continuity_ok": upstream_ok,
              "required_handle_count": len(required_handles),
              "resolved_handle_count": len(required_handles) - len(missing_handles),
              "missing_handles": missing_handles,
              "placeholder_handles": placeholder_handles,
              "overall_pass": overall_pass,
          }
          m7a_slo = {
              "captured_at_utc": captured_at,
              "phase": "M7.A",
              "execution_id": execution_id,
              "component_slo_profile": slo_profile,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M7.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "M7.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              "m7a_handle_closure_snapshot.json": m7a_snapshot,
              "m7a_component_slo_profile.json": m7a_slo,
              "m7a_blocker_register.json": blocker_register,
              "m7a_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          prefix = f"evidence/dev_full/run_control/{execution_id}"
          upload_errors = []
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blockers_unique.append({"code": "M7-B15", "message": "Failed to publish/readback M7.A artifact set to durable run-control prefix."})
              overall_pass = False
              next_gate = "HOLD_REMEDIATE"
              blocker_register["blockers"] = blockers_unique
              blocker_register["blocker_count"] = len(blockers_unique)
              blocker_register["upload_errors"] = upload_errors
              summary["overall_pass"] = overall_pass
              summary["blocker_count"] = len(blockers_unique)
              summary["next_gate"] = next_gate
              write_json(run_dir / "m7a_blocker_register.json", blocker_register)
              write_json(run_dir / "m7a_execution_summary.json", summary)

          print(json.dumps({"execution_id": execution_id, "overall_pass": summary["overall_pass"], "blocker_count": summary["blocker_count"], "next_gate": summary["next_gate"], "run_dir": str(run_dir), "run_control_prefix": f"s3://{evidence_bucket}/{prefix}/"}, ensure_ascii=True))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7a.outputs.run_dir }}/m7a_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7a.outputs.run_dir }}/m7a_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if next_gate != "M7.B_READY":
              sys.exit(1)
          PY

      - name: Upload M7.A run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m7a-handle-closure-${{ steps.run_meta_7a.outputs.timestamp }}
          path: ${{ steps.run_meta_7a.outputs.run_dir }}
          if-no-files-found: warn

  run_m7b_remote:
    name: Run M7.P8.A entry precheck remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7b' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6d_execution }}" ]]; then
            echo "M7.P8.A fail-closed: upstream_m6d_execution input is required when phase_mode=m7b (use upstream M7.A execution id)."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_7b
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m7b_p8a_entry_precheck_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build P8.A closure artifacts
        shell: bash
        env:
          EXECUTION_ID: ${{ steps.run_meta_7b.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7b.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
          UPSTREAM_M7A_EXECUTION: ${{ inputs.upstream_m6d_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          upstream_m7a_execution = os.environ["UPSTREAM_M7A_EXECUTION"]

          captured_at = now_utc()
          s3 = boto3.client("s3", region_name=region)

          blockers = []
          read_errors = []

          # Upstream M7.A continuity.
          upstream_summary_key = f"evidence/dev_full/run_control/{upstream_m7a_execution}/m7a_execution_summary.json"
          upstream_summary, err = s3_get_json(s3, evidence_bucket, upstream_summary_key)
          if err:
              read_errors.append(err)
              blockers.append({"code": "M7P8-B1", "message": "Upstream M7.A summary is missing or unreadable."})
              upstream_ok = False
          else:
              upstream_ok = bool(upstream_summary.get("overall_pass")) and str(upstream_summary.get("next_gate", "")).strip() == "M7.B_READY"
              if not upstream_ok:
                  blockers.append({"code": "M7P8-B1", "message": "Upstream M7.A is not in M7.B_READY posture."})

          # Required handles closure for P8.A.
          registry_path = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")
          registry_text = registry_path.read_text(encoding="utf-8")
          quoted = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          boolish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*(true|false)`", registry_text, flags=re.IGNORECASE)
          numish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*([0-9]+(?:\.[0-9]+)?)`", registry_text)
          handles = {k: v for k, v in quoted}
          handles.update({k: v.lower() for k, v in boolish})
          handles.update({k: v for k, v in numish})

          required_handles = [
              "FLINK_RUNTIME_PATH_ACTIVE",
              "FLINK_RUNTIME_PATH_ALLOWED",
              "PHASE_RUNTIME_PATH_MODE",
              "FLINK_APP_RTDL_IEG_V0",
              "FLINK_APP_RTDL_OFP_V0",
              "FLINK_EKS_RTDL_IEG_REF",
              "FLINK_EKS_RTDL_OFP_REF",
              "FLINK_EKS_NAMESPACE",
              "K8S_DEPLOY_ARCHIVE_WRITER",
              "S3_ARCHIVE_RUN_PREFIX_PATTERN",
              "S3_ARCHIVE_EVENTS_PREFIX_PATTERN",
              "RTDL_CAUGHT_UP_LAG_MAX",
          ]

          missing_handles = []
          placeholder_handles = []
          for key in required_handles:
              value = handles.get(key, "")
              if not value:
                  missing_handles.append(key)
                  continue
              value_norm = str(value).strip()
              if ("TO_PIN" in value_norm) or ("<" in value_norm) or ("TBD" in value_norm):
                  placeholder_handles.append(key)

          if missing_handles or placeholder_handles:
              blockers.append({"code": "M7P8-B1", "message": "Required P8 handles are missing or placeholder-valued."})

          # Runtime path sanity.
          runtime_active = str(handles.get("FLINK_RUNTIME_PATH_ACTIVE", "")).strip()
          runtime_allowed = str(handles.get("FLINK_RUNTIME_PATH_ALLOWED", "")).strip()
          allowed_set = [x.strip() for x in runtime_allowed.split("|") if x.strip()]
          runtime_path_ok = bool(runtime_active) and runtime_active in allowed_set
          if not runtime_path_ok:
              blockers.append({"code": "M7P8-B1", "message": "Active runtime path is not in allowed set for P8.A."})

          # SLO profile continuity from M7.A.
          upstream_slo_key = f"evidence/dev_full/run_control/{upstream_m7a_execution}/m7a_component_slo_profile.json"
          upstream_slo, err = s3_get_json(s3, evidence_bucket, upstream_slo_key)
          if err:
              read_errors.append(err)
              blockers.append({"code": "M7P8-B6", "message": "P8 component SLO profile is missing or unreadable from upstream M7.A."})
              slo_ok = False
              missing_slo_components = ["IEG", "OFP", "ArchiveWriter"]
          else:
              profile = upstream_slo.get("component_slo_profile", {})
              missing_slo_components = [k for k in ["IEG", "OFP", "ArchiveWriter"] if k not in profile]
              slo_ok = len(missing_slo_components) == 0
              if not slo_ok:
                  blockers.append({"code": "M7P8-B6", "message": "Required P8 component SLO entries are missing in upstream profile."})

          blockers_unique = []
          seen = set()
          for b in blockers:
              code = b.get("code")
              if code and code not in seen:
                  seen.add(code)
                  blockers_unique.append({"code": code, "message": b.get("message", "")})

          overall_pass = len(blockers_unique) == 0
          next_gate = "M7.C_READY" if overall_pass else "HOLD_REMEDIATE"

          entry_snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M7.P8.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_m7a_execution": upstream_m7a_execution,
              "upstream_continuity_ok": upstream_ok,
              "runtime_path_active": runtime_active,
              "runtime_path_allowed": allowed_set,
              "runtime_path_ok": runtime_path_ok,
              "required_handle_count": len(required_handles),
              "resolved_handle_count": len(required_handles) - len(missing_handles),
              "missing_handles": missing_handles,
              "placeholder_handles": placeholder_handles,
              "slo_profile_ok": slo_ok,
              "missing_slo_components": missing_slo_components,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M7.P8.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "M7.P8.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              "p8a_entry_snapshot.json": entry_snapshot,
              "p8a_blocker_register.json": blocker_register,
              "p8a_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          prefix = f"evidence/dev_full/run_control/{execution_id}"
          upload_errors = []
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blockers_unique.append({"code": "M7-B15", "message": "Failed to publish/readback P8.A artifact set to durable run-control prefix."})
              overall_pass = False
              next_gate = "HOLD_REMEDIATE"
              blocker_register["blockers"] = blockers_unique
              blocker_register["blocker_count"] = len(blockers_unique)
              blocker_register["upload_errors"] = upload_errors
              summary["overall_pass"] = overall_pass
              summary["blocker_count"] = len(blockers_unique)
              summary["next_gate"] = next_gate
              write_json(run_dir / "p8a_blocker_register.json", blocker_register)
              write_json(run_dir / "p8a_execution_summary.json", summary)

          print(json.dumps({"execution_id": execution_id, "overall_pass": summary["overall_pass"], "blocker_count": summary["blocker_count"], "next_gate": summary["next_gate"], "run_dir": str(run_dir), "run_control_prefix": f"s3://{evidence_bucket}/{prefix}/"}, ensure_ascii=True))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7b.outputs.run_dir }}/p8a_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7b.outputs.run_dir }}/p8a_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if next_gate != "M7.C_READY":
              sys.exit(1)
          PY

      - name: Upload P8.A run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p8a-entry-precheck-${{ steps.run_meta_7b.outputs.timestamp }}
          path: ${{ steps.run_meta_7b.outputs.run_dir }}
          if-no-files-found: warn
