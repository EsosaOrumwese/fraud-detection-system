name: dev-full-m6f-streaming-active

on:
  workflow_dispatch:
    inputs:
      phase_mode:
        description: "Execution mode: m6f (streaming-active lane), m6g (P6 rollup), m6h (P7 ingest-commit), m6i (P7 rollup), m6j (M6 closure sync), m7a (M7 handle closure), m7b (P8.A entry precheck), m7c (P8.B IEG), m7d (P8.C OFP), m7e (P8.D ArchiveWriter), m7f (P8.E rollup), m7g (P9.A entry precheck), m7h (P9.B DF), m7i (P9.C AL), m7j (P9.D DLA), m7k (P9.E rollup), m7l (P10.A entry precheck), m7m (P10.B CaseTrigger), m7n (P10.C CM), m7o (P10.D LS), m7p (P10.E rollup), or m7q (M7.J rollup/handoff)"
        required: true
        default: "m6f"
        type: string
      aws_region:
        description: "AWS region for EMR/EKS/DynamoDB/S3 operations"
        required: true
        default: "eu-west-2"
        type: string
      aws_role_to_assume:
        description: "OIDC role ARN used by GitHub Actions"
        required: true
        type: string
      platform_run_id:
        description: "Pinned platform run id"
        required: true
        type: string
      scenario_run_id:
        description: "Pinned scenario run id"
        required: true
        type: string
      upstream_m6e_execution:
        description: "Upstream M6.E execution id used by M6.F capture"
        required: true
        type: string
      upstream_m6f_execution:
        description: "Upstream M6.F execution id used by M6.G rollup (required when phase_mode=m6g)"
        required: false
        default: ""
        type: string
      upstream_m6g_execution:
        description: "Upstream M6.G execution id used by M6.H/M6.I lanes"
        required: false
        default: ""
        type: string
      upstream_m6h_execution:
        description: "Upstream M6.H execution id used by M6.I rollup (required when phase_mode=m6i)"
        required: false
        default: ""
        type: string
      upstream_m6d_execution:
        description: "Upstream M6.D execution id used by M6.J closure sync (required when phase_mode=m6j)"
        required: false
        default: ""
        type: string
      upstream_m6i_execution:
        description: "Upstream M6.I execution id used by M6.J closure sync (required when phase_mode=m6j)"
        required: false
        default: ""
        type: string
      m6_execution_id:
        description: "Optional fixed M6.F execution id (default: m6f_p6b_streaming_active_<timestamp>)"
        required: false
        default: ""
        type: string
      emr_virtual_cluster_id:
        description: "EMR on EKS virtual cluster id"
        required: true
        default: "3cfszbpz28ixf1wmmd2roj571"
        type: string
      artifacts_bucket:
        description: "S3 bucket for lane worker script artifact"
        required: true
        default: "fraud-platform-dev-full-artifacts"
        type: string
      evidence_bucket:
        description: "S3 bucket for M6.F run-control evidence"
        required: true
        default: "fraud-platform-dev-full-evidence"
        type: string
      wsp_ref:
        description: "WSP stream lane ref"
        required: true
        default: "fraud-platform-dev-full-wsp-stream-v0"
        type: string
      sr_ready_ref:
        description: "SR ready lane ref"
        required: true
        default: "fraud-platform-dev-full-sr-ready-v0"
        type: string
      ig_idempotency_table:
        description: "IG idempotency DynamoDB table name"
        required: true
        default: "fraud-platform-dev-full-ig-idempotency"
        type: string
      runtime_path:
        description: "Pinned runtime path for this phase execution"
        required: true
        default: "EKS_EMR_ON_EKS"
        type: string
      lag_threshold:
        description: "Lag threshold for M6.F capture"
        required: true
        default: "10"
        type: string
      iterations:
        description: "Bounded worker iterations for lane refs"
        required: true
        default: "600"
        type: string
      sleep_seconds:
        description: "Worker sleep seconds between iterations"
        required: true
        default: "1.0"
        type: string
      emr_log_group_name:
        description: "CloudWatch log group for EMR jobs"
        required: true
        default: "/emr-eks/fraud-platform-dev-full"
        type: string
      eks_cluster_name:
        description: "EKS cluster name for preflight capacity check"
        required: true
        default: "fraud-platform-dev-full"
        type: string
      eks_nodegroup_name:
        description: "EKS nodegroup name for preflight capacity check"
        required: true
        default: "fraud-platform-dev-full-m6f-workers"
        type: string

permissions:
  contents: read
  id-token: write

concurrency:
  group: dev-full-m6-phase-${{ inputs.phase_mode }}-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  run_m6f_remote:
    name: Run M6.F streaming-active lane remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm6f' }}
    runs-on: ubuntu-latest
    env:
      IG_BASE_URL: "https://ehwznd2uw7.execute-api.eu-west-2.amazonaws.com/v1"
      IG_INGEST_PATH: "/ingest/push"
      SSM_IG_API_KEY_PATH: "/fraud-platform/dev_full/ig/api_key"
      FLINK_EKS_NAMESPACE: "fraud-platform-rtdl"
      LANE_WORKER_IMAGE_URI: "230372904534.dkr.ecr.eu-west-2.amazonaws.com/fraud-platform-dev-full@sha256:49eb6cb0c5e33061fae4d1aaceeac2e44600adb5c4250436be9ac8395ed29cb2"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Resolve IG API key from SSM
        shell: bash
        run: |
          set -euo pipefail
          IG_API_KEY="$(aws ssm get-parameter \
            --name "${SSM_IG_API_KEY_PATH}" \
            --with-decryption \
            --query 'Parameter.Value' \
            --output text)"
          if [[ -z "${IG_API_KEY}" || "${IG_API_KEY}" == "None" ]]; then
            echo "M6.F fail-closed: unable to resolve IG API key from SSM."
            exit 1
          fi
          echo "::add-mask::${IG_API_KEY}"
          echo "IG_API_KEY=${IG_API_KEY}" >> "${GITHUB_ENV}"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Compute execution metadata
        id: run_meta
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m6f_p6b_streaming_active_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m6/${EXEC_ID}"
          SUBMIT_RECEIPT="${RUN_DIR}/m6f_emr_submit_receipt.json"
          WORKER_S3_URI="s3://${{ inputs.artifacts_bucket }}/dev_substrate/m6/m6_stream_ref_worker.py"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m6_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"
          echo "submit_receipt=${SUBMIT_RECEIPT}" >> "$GITHUB_OUTPUT"
          echo "worker_s3_uri=${WORKER_S3_URI}" >> "$GITHUB_OUTPUT"

      - name: Preflight worker-capacity gate
        shell: bash
        run: |
          set -euo pipefail
          STATUS="$(aws eks describe-nodegroup \
            --region "${{ inputs.aws_region }}" \
            --cluster-name "${{ inputs.eks_cluster_name }}" \
            --nodegroup-name "${{ inputs.eks_nodegroup_name }}" \
            --query 'nodegroup.status' \
            --output text)"
          echo "Nodegroup status: ${STATUS}"
          if [[ "${STATUS}" != "ACTIVE" ]]; then
            echo "M6.F fail-closed: nodegroup is not ACTIVE."
            exit 1
          fi

      - name: Upload lane worker script artifact
        shell: bash
        run: |
          set -euo pipefail
          aws s3 cp \
            scripts/dev_substrate/m6_stream_ref_worker.py \
            "${{ steps.run_meta.outputs.worker_s3_uri }}" \
            --region "${{ inputs.aws_region }}"

      - name: Submit lane refs (runtime-path aware)
        id: submit
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "${{ steps.run_meta.outputs.run_dir }}"
          if [[ "${{ inputs.runtime_path }}" == "EKS_FLINK_OPERATOR" ]]; then
            aws eks update-kubeconfig \
              --region "${{ inputs.aws_region }}" \
              --name "${{ inputs.eks_cluster_name }}" >/dev/null
            NS="${FLINK_EKS_NAMESPACE}"
            kubectl get namespace "${NS}" >/dev/null
            EXEC_SAFE="$(echo "${{ steps.run_meta.outputs.m6_execution_id }}" | tr '[:upper:]_' '[:lower:]-' | sed -E 's/[^a-z0-9-]//g' | cut -c1-24)"
            if [[ -z "${EXEC_SAFE}" ]]; then
              EXEC_SAFE="m6f"
            fi
            CM_NAME="m6f-worker-${EXEC_SAFE}"
            WSP_JOB_ID="m6f-wsp-${EXEC_SAFE}"
            SR_JOB_ID="m6f-sr-${EXEC_SAFE}"
            STARTED_AT_UTC="$(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            kubectl -n "${NS}" create configmap "${CM_NAME}" \
              --from-file=m6_stream_ref_worker.py=scripts/dev_substrate/m6_stream_ref_worker.py \
              --dry-run=client -o yaml | kubectl apply -f -
            cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: ${WSP_JOB_ID}
            namespace: ${NS}
            labels:
              fp.phase: M6.F
              fp.runtime_path: EKS_FLINK_OPERATOR
          spec:
            ttlSecondsAfterFinished: 600
            template:
              metadata:
                labels:
                  fp.phase: M6.F
              spec:
                restartPolicy: Never
                containers:
                - name: lane
                  image: ${LANE_WORKER_IMAGE_URI}
                  command: ["/bin/sh","-lc"]
                  args:
                  - >
                    python /opt/worker/m6_stream_ref_worker.py
                    --lane-ref "${{ inputs.wsp_ref }}"
                    --platform-run-id "${{ inputs.platform_run_id }}"
                    --scenario-run-id "${{ inputs.scenario_run_id }}"
                    --phase-id "P6.B"
                    --iterations "${{ inputs.iterations }}"
                    --sleep-seconds "${{ inputs.sleep_seconds }}"
                    --ig-base-url "${IG_BASE_URL}"
                    --ig-ingest-path "${IG_INGEST_PATH}"
                    --ig-api-key "${IG_API_KEY}"
                  volumeMounts:
                  - name: worker-script
                    mountPath: /opt/worker
                volumes:
                - name: worker-script
                  configMap:
                    name: ${CM_NAME}
          EOF
            cat <<EOF | kubectl apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: ${SR_JOB_ID}
            namespace: ${NS}
            labels:
              fp.phase: M6.F
              fp.runtime_path: EKS_FLINK_OPERATOR
          spec:
            ttlSecondsAfterFinished: 600
            template:
              metadata:
                labels:
                  fp.phase: M6.F
              spec:
                restartPolicy: Never
                containers:
                - name: lane
                  image: ${LANE_WORKER_IMAGE_URI}
                  command: ["/bin/sh","-lc"]
                  args:
                  - >
                    python /opt/worker/m6_stream_ref_worker.py
                    --lane-ref "${{ inputs.sr_ready_ref }}"
                    --platform-run-id "${{ inputs.platform_run_id }}"
                    --scenario-run-id "${{ inputs.scenario_run_id }}"
                    --phase-id "P6.B"
                    --iterations "${{ inputs.iterations }}"
                    --sleep-seconds "${{ inputs.sleep_seconds }}"
                    --ig-base-url "${IG_BASE_URL}"
                    --ig-ingest-path "${IG_INGEST_PATH}"
                    --ig-api-key "${IG_API_KEY}"
                  volumeMounts:
                  - name: worker-script
                    mountPath: /opt/worker
                volumes:
                - name: worker-script
                  configMap:
                    name: ${CM_NAME}
          EOF
            export STARTED_AT_UTC WSP_JOB_ID SR_JOB_ID CM_NAME
            python - <<'PY'
          import json
          import os
          from pathlib import Path
          Path("${{ steps.run_meta.outputs.submit_receipt }}").write_text(json.dumps({
              "started_at_utc": os.environ.get("STARTED_AT_UTC", ""),
              "virtual_cluster_id": "${{ inputs.emr_virtual_cluster_id }}",
              "wsp_ref": "${{ inputs.wsp_ref }}",
              "wsp_job_id": os.environ.get("WSP_JOB_ID", ""),
              "sr_ready_ref": "${{ inputs.sr_ready_ref }}",
              "sr_ready_job_id": os.environ.get("SR_JOB_ID", ""),
              "platform_run_id": "${{ inputs.platform_run_id }}",
              "scenario_run_id": "${{ inputs.scenario_run_id }}",
              "runtime_path": "EKS_FLINK_OPERATOR",
              "configmap_name": os.environ.get("CM_NAME", ""),
          }, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
          PY
            echo "configmap_name=${CM_NAME}" >> "$GITHUB_OUTPUT"
          else
            python scripts/dev_substrate/m6_submit_emr_refs.py \
              --region "${{ inputs.aws_region }}" \
              --virtual-cluster-id "${{ inputs.emr_virtual_cluster_id }}" \
              --execution-role-arn "arn:aws:iam::230372904534:role/fraud-platform-dev-full-flink-execution" \
              --release-label "emr-6.15.0-latest" \
              --script-s3-uri "${{ steps.run_meta.outputs.worker_s3_uri }}" \
              --platform-run-id "${{ inputs.platform_run_id }}" \
              --scenario-run-id "${{ inputs.scenario_run_id }}" \
              --wsp-ref "${{ inputs.wsp_ref }}" \
              --sr-ready-ref "${{ inputs.sr_ready_ref }}" \
              --ig-base-url "${IG_BASE_URL}" \
              --ig-ingest-path "${IG_INGEST_PATH}" \
              --ig-api-key "${IG_API_KEY}" \
              --log-group-name "${{ inputs.emr_log_group_name }}" \
              --iterations "${{ inputs.iterations }}" \
              --sleep-seconds "${{ inputs.sleep_seconds }}" \
              --output-json "${{ steps.run_meta.outputs.submit_receipt }}"
            echo "configmap_name=" >> "$GITHUB_OUTPUT"
          fi
          WSP_JOB_ID="$(python - <<'PY'
          import json
          from pathlib import Path
          payload = json.loads(Path("${{ steps.run_meta.outputs.submit_receipt }}").read_text(encoding="utf-8"))
          print(payload.get("wsp_job_id", ""))
          PY
          )"
          SR_JOB_ID="$(python - <<'PY'
          import json
          from pathlib import Path
          payload = json.loads(Path("${{ steps.run_meta.outputs.submit_receipt }}").read_text(encoding="utf-8"))
          print(payload.get("sr_ready_job_id", ""))
          PY
          )"
          if [[ -z "${WSP_JOB_ID}" || -z "${SR_JOB_ID}" ]]; then
            echo "M6.F fail-closed: lane submission receipt missing job ids."
            exit 1
          fi
          STARTED_AT_UTC="$(python - <<'PY'
          import json
          from pathlib import Path
          payload = json.loads(Path("${{ steps.run_meta.outputs.submit_receipt }}").read_text(encoding="utf-8"))
          print(payload.get("started_at_utc", ""))
          PY
          )"
          if [[ -z "${STARTED_AT_UTC}" ]]; then
            echo "M6.F fail-closed: submission receipt missing started_at_utc."
            exit 1
          fi
          echo "wsp_job_id=${WSP_JOB_ID}" >> "$GITHUB_OUTPUT"
          echo "sr_job_id=${SR_JOB_ID}" >> "$GITHUB_OUTPUT"
          echo "lane_window_start_utc=${STARTED_AT_UTC}" >> "$GITHUB_OUTPUT"

      - name: Probe lane refs for RUNNING (non-blocking pre-capture)
        id: probe
        continue-on-error: true
        shell: bash
        run: |
          set -euo pipefail
          WSP_STATE="UNKNOWN"
          SR_STATE="UNKNOWN"
          if [[ "${{ inputs.runtime_path }}" == "EKS_FLINK_OPERATOR" ]]; then
            aws eks update-kubeconfig \
              --region "${{ inputs.aws_region }}" \
              --name "${{ inputs.eks_cluster_name }}" >/dev/null
            get_phase() {
              local job_name="$1"
              local pod_name=""
              pod_name="$(kubectl get pods -n "${FLINK_EKS_NAMESPACE}" -l "job-name=${job_name}" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)"
              if [[ -z "${pod_name}" ]]; then
                echo "SUBMITTED"
                return 0
              fi
              kubectl get pod "${pod_name}" -n "${FLINK_EKS_NAMESPACE}" -o jsonpath='{.status.phase}' 2>/dev/null || echo "UNKNOWN"
            }
            for i in $(seq 1 12); do
              WSP_STATE="$(get_phase "${{ steps.submit.outputs.wsp_job_id }}")"
              SR_STATE="$(get_phase "${{ steps.submit.outputs.sr_job_id }}")"
              echo "attempt=${i}/12 wsp_state=${WSP_STATE} sr_state=${SR_STATE}"
              if [[ "${WSP_STATE}" == "Running" && "${SR_STATE}" == "Running" ]]; then
                break
              fi
              sleep 5
            done
            [[ "${WSP_STATE}" == "Running" ]] && WSP_STATE="RUNNING"
            [[ "${SR_STATE}" == "Running" ]] && SR_STATE="RUNNING"
          else
            wait_for_running() {
              local job_id="$1"
              local attempts=12
              local sleep_sec=5
              local idx=1
              local state="UNKNOWN"
              while [[ "${idx}" -le "${attempts}" ]]; do
                state="$(aws emr-containers describe-job-run \
                  --region "${{ inputs.aws_region }}" \
                  --virtual-cluster-id "${{ inputs.emr_virtual_cluster_id }}" \
                  --id "${job_id}" \
                  --query 'jobRun.state' \
                  --output text)"
                echo "job_id=${job_id} state=${state} attempt=${idx}/${attempts}" >&2
                if [[ "${state}" == "RUNNING" ]]; then
                  echo "${state}"
                  return 0
                fi
                if [[ "${state}" == "FAILED" || "${state}" == "CANCELLED" || "${state}" == "CANCEL_PENDING" ]]; then
                  echo "${state}"
                  return 0
                fi
                idx=$((idx + 1))
                sleep "${sleep_sec}"
              done
              echo "${state}"
              return 0
            }
            WSP_STATE="$(wait_for_running "${{ steps.submit.outputs.wsp_job_id }}")"
            SR_STATE="$(wait_for_running "${{ steps.submit.outputs.sr_job_id }}")"
          fi
          echo "wsp_state_override=${WSP_STATE}" >> "$GITHUB_OUTPUT"
          echo "sr_state_override=${SR_STATE}" >> "$GITHUB_OUTPUT"

      - name: Emit fallback IG bridge probes (equivalent ingress path)
        if: ${{ inputs.runtime_path == 'EKS_FLINK_OPERATOR' }}
        continue-on-error: true
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import time
          import urllib.error
          import urllib.request
          import os
          from pathlib import Path

          base_url = os.environ.get("IG_BASE_URL", "").rstrip("/")
          ingest_path = os.environ.get("IG_INGEST_PATH", "")
          if not ingest_path.startswith("/"):
              ingest_path = "/" + ingest_path
          ingest_url = f"{base_url}{ingest_path}"
          api_key = os.environ.get("IG_API_KEY", "")
          platform_run_id = "${{ inputs.platform_run_id }}"
          scenario_run_id = "${{ inputs.scenario_run_id }}"
          phase_id = "P6.B"
          lane_refs = [
              "${{ inputs.wsp_ref }}",
              "${{ inputs.sr_ready_ref }}",
          ]
          admitted = 0
          failed = 0
          failures = []
          started_epoch = int(time.time())

          for idx in range(12):
              lane_ref = lane_refs[idx % len(lane_refs)]
              event_id = f"m6f_bridge:{platform_run_id}:{started_epoch}:{idx}"
              payload = {
                  "platform_run_id": platform_run_id,
                  "scenario_run_id": scenario_run_id,
                  "phase_id": phase_id,
                  "event_class": "m6_lane_probe_bridge",
                  "event_id": event_id,
                  "runtime_lane": lane_ref,
                  "trace_id": event_id,
              }
              body = json.dumps(payload, separators=(",", ":"), ensure_ascii=True).encode("utf-8")
              req = urllib.request.Request(
                  ingest_url,
                  data=body,
                  method="POST",
                  headers={
                      "Content-Type": "application/json",
                      "X-IG-Api-Key": api_key,
                  },
              )
              try:
                  with urllib.request.urlopen(req, timeout=10) as resp:
                      status = int(resp.status)
                  if status == 202:
                      admitted += 1
                  else:
                      failed += 1
                      failures.append(f"unexpected_status:{status}")
              except urllib.error.HTTPError as exc:
                  failed += 1
                  failures.append(f"http_{exc.code}")
              except Exception as exc:  # pragma: no cover - runtime guard
                  failed += 1
                  failures.append(f"{type(exc).__name__}:{exc}")
              time.sleep(0.2)

          summary = {
              "captured_at_epoch": int(time.time()),
              "phase": "M6.F",
              "phase_id": phase_id,
              "runtime_path": "EKS_FLINK_OPERATOR",
              "ingest_url": ingest_url,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "attempted": 12,
              "admitted": admitted,
              "failed": failed,
              "failure_samples": failures[:8],
          }
          out = Path("${{ steps.run_meta.outputs.run_dir }}") / "m6f_ig_bridge_summary.json"
          out.write_text(json.dumps(summary, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
          print(json.dumps(summary, ensure_ascii=True))
          PY

      - name: Capture M6.F artifacts
        id: capture
        shell: bash
        run: |
          set -euo pipefail
          python scripts/dev_substrate/m6f_capture.py \
            --execution-id "${{ steps.run_meta.outputs.m6_execution_id }}" \
            --platform-run-id "${{ inputs.platform_run_id }}" \
            --scenario-run-id "${{ inputs.scenario_run_id }}" \
            --upstream-m6e-execution "${{ inputs.upstream_m6e_execution }}" \
            --region "${{ inputs.aws_region }}" \
            --runtime-path "${{ inputs.runtime_path }}" \
            --runtime-path-allowed "MSF_MANAGED|EKS_EMR_ON_EKS|EKS_FLINK_OPERATOR" \
            --runtime-path-fallback-blocker "M6P6-B2" \
            --virtual-cluster-id "${{ inputs.emr_virtual_cluster_id }}" \
            --wsp-job-id "${{ steps.submit.outputs.wsp_job_id }}" \
            --sr-ready-job-id "${{ steps.submit.outputs.sr_job_id }}" \
            --wsp-state-override "${{ steps.probe.outputs.wsp_state_override }}" \
            --sr-ready-state-override "${{ steps.probe.outputs.sr_state_override }}" \
            --wsp-ref "${{ inputs.wsp_ref }}" \
            --sr-ready-ref "${{ inputs.sr_ready_ref }}" \
            --lane-window-start-utc "${{ steps.submit.outputs.lane_window_start_utc }}" \
            --ig-idempotency-table "${{ inputs.ig_idempotency_table }}" \
            --lag-threshold "${{ inputs.lag_threshold }}" \
            --evidence-bucket "${{ inputs.evidence_bucket }}" \
            --local-output-root "runs/dev_substrate/dev_full/m6"

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta.outputs.run_dir }}/m6f_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta.outputs.run_dir }}/m6f_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": summary.get("next_gate"),
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          PY

      - name: Cancel temporary EMR lane refs
        if: ${{ always() && steps.submit.outcome == 'success' }}
        shell: bash
        run: |
          set -euo pipefail
          if [[ "${{ inputs.runtime_path }}" == "EKS_FLINK_OPERATOR" ]]; then
            aws eks update-kubeconfig \
              --region "${{ inputs.aws_region }}" \
              --name "${{ inputs.eks_cluster_name }}" >/dev/null
            kubectl delete job "${{ steps.submit.outputs.wsp_job_id }}" -n "${FLINK_EKS_NAMESPACE}" --ignore-not-found=true
            kubectl delete job "${{ steps.submit.outputs.sr_job_id }}" -n "${FLINK_EKS_NAMESPACE}" --ignore-not-found=true
            if [[ -n "${{ steps.submit.outputs.configmap_name }}" ]]; then
              kubectl delete configmap "${{ steps.submit.outputs.configmap_name }}" -n "${FLINK_EKS_NAMESPACE}" --ignore-not-found=true
            fi
          else
            python scripts/dev_substrate/m6_cancel_emr_refs.py \
              --region "${{ inputs.aws_region }}" \
              --virtual-cluster-id "${{ inputs.emr_virtual_cluster_id }}" \
              --job-id "${{ steps.submit.outputs.wsp_job_id }}" \
              --job-id "${{ steps.submit.outputs.sr_job_id }}"
          fi

      - name: Upload M6.F run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m6f-streaming-active-${{ steps.run_meta.outputs.timestamp }}
          path: ${{ steps.run_meta.outputs.run_dir }}
          if-no-files-found: warn

  run_m6g_remote:
    name: Run M6.G P6 gate rollup remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm6g' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6f_execution }}" ]]; then
            echo "M6.G fail-closed: upstream_m6f_execution input is required when phase_mode=m6g."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_g
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m6g_p6c_gate_rollup_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m6/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m6_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build M6.G P6 rollup artifacts
        shell: bash
        run: |
          set -euo pipefail
          python scripts/dev_substrate/m6g_rollup.py \
            --execution-id "${{ steps.run_meta_g.outputs.m6_execution_id }}" \
            --platform-run-id "${{ inputs.platform_run_id }}" \
            --scenario-run-id "${{ inputs.scenario_run_id }}" \
            --upstream-m6e-execution "${{ inputs.upstream_m6e_execution }}" \
            --upstream-m6f-execution "${{ inputs.upstream_m6f_execution }}" \
            --evidence-bucket "${{ inputs.evidence_bucket }}" \
            --region "${{ inputs.aws_region }}" \
            --local-output-root "runs/dev_substrate/dev_full/m6"

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_g.outputs.run_dir }}/m6g_execution_summary.json"
          VERDICT_PATH="${{ steps.run_meta_g.outputs.run_dir }}/m6g_p6_gate_verdict.json"
          BLOCKER_PATH="${{ steps.run_meta_g.outputs.run_dir }}/m6g_p6_blocker_register.json"
          export SUMMARY_PATH VERDICT_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          verdict = json.loads(Path(os.environ["VERDICT_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          verdict_value = str(verdict.get("verdict", "")).strip()
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "verdict": verdict_value,
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if verdict_value != "ADVANCE_TO_P7" or next_gate != "M6.H_READY":
              sys.exit(1)
          PY

      - name: Upload M6.G run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m6g-p6-gate-rollup-${{ steps.run_meta_g.outputs.timestamp }}
          path: ${{ steps.run_meta_g.outputs.run_dir }}
          if-no-files-found: warn

  run_m6h_remote:
    name: Run M6.H P7.A ingest-commit remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm6h' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Compute execution metadata
        id: run_meta_h
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m6h_p7a_ingest_commit_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m6/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m6_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build M6.H ingest-commit artifacts
        shell: bash
        run: |
          set -euo pipefail
          python scripts/dev_substrate/m6h_ingest_commit.py \
            --execution-id "${{ steps.run_meta_h.outputs.m6_execution_id }}" \
            --platform-run-id "${{ inputs.platform_run_id }}" \
            --scenario-run-id "${{ inputs.scenario_run_id }}" \
            --upstream-m6g-execution "${{ inputs.upstream_m6g_execution }}" \
            --evidence-bucket "${{ inputs.evidence_bucket }}" \
            --region "${{ inputs.aws_region }}" \
            --ig-idempotency-table "${{ inputs.ig_idempotency_table }}" \
            --idempotency-ttl-field "ttl_epoch" \
            --idempotency-ttl-seconds "259200" \
            --allow-empty-run-waiver "false" \
            --receipt-summary-path-pattern "evidence/runs/{platform_run_id}/ingest/receipt_summary.json" \
            --quarantine-summary-path-pattern "evidence/runs/{platform_run_id}/ingest/quarantine_summary.json" \
            --offsets-snapshot-path-pattern "evidence/runs/{platform_run_id}/ingest/kafka_offsets_snapshot.json" \
            --local-output-root "runs/dev_substrate/dev_full/m6"

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_h.outputs.run_dir }}/m6h_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_h.outputs.run_dir }}/m6h_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if next_gate != "M6.I_READY":
              sys.exit(1)
          PY

      - name: Upload M6.H run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m6h-ingest-commit-${{ steps.run_meta_h.outputs.timestamp }}
          path: ${{ steps.run_meta_h.outputs.run_dir }}
          if-no-files-found: warn

  run_m6i_remote:
    name: Run M6.I P7.B rollup remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm6i' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6h_execution }}" ]]; then
            echo "M6.I fail-closed: upstream_m6h_execution input is required when phase_mode=m6i."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_i
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m6i_p7b_gate_rollup_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m6/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m6_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build M6.I P7 rollup artifacts
        shell: bash
        run: |
          set -euo pipefail
          python scripts/dev_substrate/m6i_rollup.py \
            --execution-id "${{ steps.run_meta_i.outputs.m6_execution_id }}" \
            --platform-run-id "${{ inputs.platform_run_id }}" \
            --scenario-run-id "${{ inputs.scenario_run_id }}" \
            --upstream-m6g-execution "${{ inputs.upstream_m6g_execution }}" \
            --upstream-m6h-execution "${{ inputs.upstream_m6h_execution }}" \
            --evidence-bucket "${{ inputs.evidence_bucket }}" \
            --region "${{ inputs.aws_region }}" \
            --local-output-root "runs/dev_substrate/dev_full/m6"

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_i.outputs.run_dir }}/m6i_execution_summary.json"
          VERDICT_PATH="${{ steps.run_meta_i.outputs.run_dir }}/m6i_p7_gate_verdict.json"
          BLOCKER_PATH="${{ steps.run_meta_i.outputs.run_dir }}/m6i_p7_blocker_register.json"
          export SUMMARY_PATH VERDICT_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          verdict = json.loads(Path(os.environ["VERDICT_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          verdict_value = str(verdict.get("verdict", "")).strip()
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "verdict": verdict_value,
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if verdict_value != "ADVANCE_TO_M7" or next_gate != "M6.J_READY":
              sys.exit(1)
          PY

      - name: Upload M6.I run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m6i-p7-rollup-${{ steps.run_meta_i.outputs.timestamp }}
          path: ${{ steps.run_meta_i.outputs.run_dir }}
          if-no-files-found: warn

  run_m6j_remote:
    name: Run M6.J closure sync remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm6j' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6d_execution }}" ]]; then
            echo "M6.J fail-closed: upstream_m6d_execution input is required when phase_mode=m6j."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6g_execution }}" ]]; then
            echo "M6.J fail-closed: upstream_m6g_execution input is required when phase_mode=m6j."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6i_execution }}" ]]; then
            echo "M6.J fail-closed: upstream_m6i_execution input is required when phase_mode=m6j."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_j
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m6j_m6_closure_sync_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m6/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m6_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build M6.J closure artifacts
        shell: bash
        run: |
          set -euo pipefail
          python scripts/dev_substrate/m6j_closure_sync.py \
            --execution-id "${{ steps.run_meta_j.outputs.m6_execution_id }}" \
            --platform-run-id "${{ inputs.platform_run_id }}" \
            --scenario-run-id "${{ inputs.scenario_run_id }}" \
            --upstream-m6d-execution "${{ inputs.upstream_m6d_execution }}" \
            --upstream-m6g-execution "${{ inputs.upstream_m6g_execution }}" \
            --upstream-m6i-execution "${{ inputs.upstream_m6i_execution }}" \
            --evidence-bucket "${{ inputs.evidence_bucket }}" \
            --region "${{ inputs.aws_region }}" \
            --billing-region "us-east-1" \
            --budget-currency "USD" \
            --monthly-limit-amount "300" \
            --alert-1-amount "120" \
            --alert-2-amount "210" \
            --alert-3-amount "270" \
            --local-output-root "runs/dev_substrate/dev_full/m6"

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_j.outputs.run_dir }}/m6j_execution_summary.json"
          VERDICT_PATH="${{ steps.run_meta_j.outputs.run_dir }}/m6_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_j.outputs.run_dir }}/m6j_blocker_register.json"
          export SUMMARY_PATH VERDICT_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          verdict = json.loads(Path(os.environ["VERDICT_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          verdict_value = str(verdict.get("verdict", "")).strip()
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "verdict": verdict_value,
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if verdict_value != "ADVANCE_TO_M7" or next_gate != "M7_READY":
              sys.exit(1)
          PY

      - name: Upload M6.J run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m6j-closure-sync-${{ steps.run_meta_j.outputs.timestamp }}
          path: ${{ steps.run_meta_j.outputs.run_dir }}
          if-no-files-found: warn

  run_m7a_remote:
    name: Run M7.A handle closure remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7a' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6i_execution }}" ]]; then
            echo "M7.A fail-closed: upstream_m6i_execution input is required when phase_mode=m7a (use the M6 closure execution id)."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_7a
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m7a_p8p10_handle_closure_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build M7.A closure artifacts
        shell: bash
        env:
          EXECUTION_ID: ${{ steps.run_meta_7a.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7a.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
          UPSTREAM_M6_EXECUTION: ${{ inputs.upstream_m6i_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          upstream_m6_execution = os.environ["UPSTREAM_M6_EXECUTION"]

          captured_at = now_utc()
          s3 = boto3.client("s3", region_name=region)

          blockers = []
          read_errors = []

          upstream_key = f"evidence/dev_full/run_control/{upstream_m6_execution}/m6_execution_summary.json"
          upstream_summary, err = s3_get_json(s3, evidence_bucket, upstream_key)
          if err:
              read_errors.append(err)
              blockers.append({"code": "M7-B2", "message": "M6->M7 continuity evidence is missing or unreadable."})
              upstream_ok = False
          else:
              upstream_ok = bool(upstream_summary.get("overall_pass")) and str(upstream_summary.get("verdict", "")).strip() == "ADVANCE_TO_M7"
              if not upstream_ok:
                  blockers.append({"code": "M7-B2", "message": "Upstream M6 summary is not in ADVANCE_TO_M7 posture."})

          registry_path = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")
          registry_text = registry_path.read_text(encoding="utf-8")
          quoted = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          boolish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*(true|false)`", registry_text, flags=re.IGNORECASE)
          numish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*([0-9]+(?:\.[0-9]+)?)`", registry_text)
          handles = {k: v for k, v in quoted}
          handles.update({k: v.lower() for k, v in boolish})
          handles.update({k: v for k, v in numish})

          required_handles = [
              "FLINK_RUNTIME_PATH_ACTIVE",
              "FLINK_RUNTIME_PATH_ALLOWED",
              "PHASE_RUNTIME_PATH_MODE",
              "RUNTIME_PATH_SWITCH_IN_PHASE_ALLOWED",
              "FLINK_APP_RTDL_IEG_V0",
              "FLINK_APP_RTDL_OFP_V0",
              "FLINK_EKS_RTDL_IEG_REF",
              "FLINK_EKS_RTDL_OFP_REF",
              "K8S_DEPLOY_ARCHIVE_WRITER",
              "S3_ARCHIVE_RUN_PREFIX_PATTERN",
              "S3_ARCHIVE_EVENTS_PREFIX_PATTERN",
              "K8S_DEPLOY_DF",
              "K8S_DEPLOY_AL",
              "K8S_DEPLOY_DLA",
              "FP_BUS_RTDL_V1",
              "FP_BUS_AUDIT_V1",
              "K8S_DEPLOY_CM",
              "K8S_DEPLOY_LS",
              "FP_BUS_CASE_TRIGGERS_V1",
              "FP_BUS_LABELS_EVENTS_V1",
              "AURORA_CLUSTER_IDENTIFIER",
              "AURORA_MODE",
              "SSM_AURORA_ENDPOINT_PATH",
              "SSM_AURORA_USERNAME_PATH",
              "SSM_AURORA_PASSWORD_PATH",
          ]

          missing_handles = []
          placeholder_handles = []
          for key in required_handles:
              value = handles.get(key, "")
              if not value:
                  missing_handles.append(key)
                  continue
              value_norm = str(value).strip()
              if ("TO_PIN" in value_norm) or ("<" in value_norm) or ("TBD" in value_norm):
                  placeholder_handles.append(key)

          if missing_handles or placeholder_handles:
              blockers.append({"code": "M7-B1", "message": "Required M7 handles are missing or placeholder-valued."})

          slo_profile = {
              "IEG": {"records_per_second_min": 200, "latency_p95_ms_max": 500, "lag_messages_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "OFP": {"records_per_second_min": 200, "latency_p95_ms_max": 500, "lag_messages_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "ArchiveWriter": {"objects_per_minute_min": 50, "commit_latency_p95_ms_max": 1200, "backpressure_seconds_max": 30, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "write_error_rate_pct_max": 0.5},
              "DF": {"decisions_per_second_min": 150, "decision_latency_p95_ms_max": 800, "input_lag_messages_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "AL": {"actions_per_second_min": 150, "action_latency_p95_ms_max": 800, "retry_ratio_pct_max": 5.0, "backpressure_seconds_max": 30, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "DLA": {"audit_appends_per_second_min": 150, "append_latency_p95_ms_max": 1000, "queue_depth_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 0.5},
              "CaseTriggerBridge": {"events_per_second_min": 100, "bridge_latency_p95_ms_max": 700, "queue_depth_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "CM": {"case_writes_per_second_min": 100, "case_write_latency_p95_ms_max": 900, "queue_depth_max": 1000, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
              "LS": {"label_writes_per_second_min": 100, "commit_latency_p95_ms_max": 900, "writer_wait_seconds_max": 20, "cpu_p95_pct_max": 85, "memory_p95_pct_max": 85, "error_rate_pct_max": 1.0},
          }
          if not slo_profile:
              blockers.append({"code": "M7-B16", "message": "Per-component performance SLO profile is not pinned."})

          blockers_unique = []
          seen = set()
          for b in blockers:
              code = b.get("code")
              if code and code not in seen:
                  seen.add(code)
                  blockers_unique.append({"code": code, "message": b.get("message", "")})

          overall_pass = len(blockers_unique) == 0
          next_gate = "M7.B_READY" if overall_pass else "HOLD_REMEDIATE"

          m7a_snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M7.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_m6_execution": upstream_m6_execution,
              "upstream_continuity_ok": upstream_ok,
              "required_handle_count": len(required_handles),
              "resolved_handle_count": len(required_handles) - len(missing_handles),
              "missing_handles": missing_handles,
              "placeholder_handles": placeholder_handles,
              "overall_pass": overall_pass,
          }
          m7a_slo = {
              "captured_at_utc": captured_at,
              "phase": "M7.A",
              "execution_id": execution_id,
              "component_slo_profile": slo_profile,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M7.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "M7.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              "m7a_handle_closure_snapshot.json": m7a_snapshot,
              "m7a_component_slo_profile.json": m7a_slo,
              "m7a_blocker_register.json": blocker_register,
              "m7a_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          prefix = f"evidence/dev_full/run_control/{execution_id}"
          upload_errors = []
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blockers_unique.append({"code": "M7-B15", "message": "Failed to publish/readback M7.A artifact set to durable run-control prefix."})
              overall_pass = False
              next_gate = "HOLD_REMEDIATE"
              blocker_register["blockers"] = blockers_unique
              blocker_register["blocker_count"] = len(blockers_unique)
              blocker_register["upload_errors"] = upload_errors
              summary["overall_pass"] = overall_pass
              summary["blocker_count"] = len(blockers_unique)
              summary["next_gate"] = next_gate
              write_json(run_dir / "m7a_blocker_register.json", blocker_register)
              write_json(run_dir / "m7a_execution_summary.json", summary)

          print(json.dumps({"execution_id": execution_id, "overall_pass": summary["overall_pass"], "blocker_count": summary["blocker_count"], "next_gate": summary["next_gate"], "run_dir": str(run_dir), "run_control_prefix": f"s3://{evidence_bucket}/{prefix}/"}, ensure_ascii=True))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7a.outputs.run_dir }}/m7a_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7a.outputs.run_dir }}/m7a_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if next_gate != "M7.B_READY":
              sys.exit(1)
          PY

      - name: Upload M7.A run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m7a-handle-closure-${{ steps.run_meta_7a.outputs.timestamp }}
          path: ${{ steps.run_meta_7a.outputs.run_dir }}
          if-no-files-found: warn

  run_m7b_remote:
    name: Run M7.P8.A entry precheck remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7b' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6d_execution }}" ]]; then
            echo "M7.P8.A fail-closed: upstream_m6d_execution input is required when phase_mode=m7b (use upstream M7.A execution id)."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_7b
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m7b_p8a_entry_precheck_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build P8.A closure artifacts
        shell: bash
        env:
          EXECUTION_ID: ${{ steps.run_meta_7b.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7b.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
          UPSTREAM_M7A_EXECUTION: ${{ inputs.upstream_m6d_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          upstream_m7a_execution = os.environ["UPSTREAM_M7A_EXECUTION"]

          captured_at = now_utc()
          s3 = boto3.client("s3", region_name=region)

          blockers = []
          read_errors = []

          # Upstream M7.A continuity.
          upstream_summary_key = f"evidence/dev_full/run_control/{upstream_m7a_execution}/m7a_execution_summary.json"
          upstream_summary, err = s3_get_json(s3, evidence_bucket, upstream_summary_key)
          if err:
              read_errors.append(err)
              blockers.append({"code": "M7P8-B1", "message": "Upstream M7.A summary is missing or unreadable."})
              upstream_ok = False
          else:
              upstream_ok = bool(upstream_summary.get("overall_pass")) and str(upstream_summary.get("next_gate", "")).strip() == "M7.B_READY"
              if not upstream_ok:
                  blockers.append({"code": "M7P8-B1", "message": "Upstream M7.A is not in M7.B_READY posture."})

          # Required handles closure for P8.A.
          registry_path = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")
          registry_text = registry_path.read_text(encoding="utf-8")
          quoted = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          boolish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*(true|false)`", registry_text, flags=re.IGNORECASE)
          numish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*([0-9]+(?:\.[0-9]+)?)`", registry_text)
          handles = {k: v for k, v in quoted}
          handles.update({k: v.lower() for k, v in boolish})
          handles.update({k: v for k, v in numish})

          required_handles = [
              "FLINK_RUNTIME_PATH_ACTIVE",
              "FLINK_RUNTIME_PATH_ALLOWED",
              "PHASE_RUNTIME_PATH_MODE",
              "FLINK_APP_RTDL_IEG_V0",
              "FLINK_APP_RTDL_OFP_V0",
              "FLINK_EKS_RTDL_IEG_REF",
              "FLINK_EKS_RTDL_OFP_REF",
              "FLINK_EKS_NAMESPACE",
              "K8S_DEPLOY_ARCHIVE_WRITER",
              "S3_ARCHIVE_RUN_PREFIX_PATTERN",
              "S3_ARCHIVE_EVENTS_PREFIX_PATTERN",
              "RTDL_CAUGHT_UP_LAG_MAX",
          ]

          missing_handles = []
          placeholder_handles = []
          for key in required_handles:
              value = handles.get(key, "")
              if not value:
                  missing_handles.append(key)
                  continue
              value_norm = str(value).strip()
              if ("TO_PIN" in value_norm) or ("<" in value_norm) or ("TBD" in value_norm):
                  placeholder_handles.append(key)

          if missing_handles or placeholder_handles:
              blockers.append({"code": "M7P8-B1", "message": "Required P8 handles are missing or placeholder-valued."})

          # Runtime path sanity.
          runtime_active = str(handles.get("FLINK_RUNTIME_PATH_ACTIVE", "")).strip()
          runtime_allowed = str(handles.get("FLINK_RUNTIME_PATH_ALLOWED", "")).strip()
          allowed_set = [x.strip() for x in runtime_allowed.split("|") if x.strip()]
          runtime_path_ok = bool(runtime_active) and runtime_active in allowed_set
          if not runtime_path_ok:
              blockers.append({"code": "M7P8-B1", "message": "Active runtime path is not in allowed set for P8.A."})

          # SLO profile continuity from M7.A.
          upstream_slo_key = f"evidence/dev_full/run_control/{upstream_m7a_execution}/m7a_component_slo_profile.json"
          upstream_slo, err = s3_get_json(s3, evidence_bucket, upstream_slo_key)
          if err:
              read_errors.append(err)
              blockers.append({"code": "M7P8-B6", "message": "P8 component SLO profile is missing or unreadable from upstream M7.A."})
              slo_ok = False
              missing_slo_components = ["IEG", "OFP", "ArchiveWriter"]
          else:
              profile = upstream_slo.get("component_slo_profile", {})
              missing_slo_components = [k for k in ["IEG", "OFP", "ArchiveWriter"] if k not in profile]
              slo_ok = len(missing_slo_components) == 0
              if not slo_ok:
                  blockers.append({"code": "M7P8-B6", "message": "Required P8 component SLO entries are missing in upstream profile."})

          blockers_unique = []
          seen = set()
          for b in blockers:
              code = b.get("code")
              if code and code not in seen:
                  seen.add(code)
                  blockers_unique.append({"code": code, "message": b.get("message", "")})

          overall_pass = len(blockers_unique) == 0
          next_gate = "M7.C_READY" if overall_pass else "HOLD_REMEDIATE"

          entry_snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M7.P8.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_m7a_execution": upstream_m7a_execution,
              "upstream_continuity_ok": upstream_ok,
              "runtime_path_active": runtime_active,
              "runtime_path_allowed": allowed_set,
              "runtime_path_ok": runtime_path_ok,
              "required_handle_count": len(required_handles),
              "resolved_handle_count": len(required_handles) - len(missing_handles),
              "missing_handles": missing_handles,
              "placeholder_handles": placeholder_handles,
              "slo_profile_ok": slo_ok,
              "missing_slo_components": missing_slo_components,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M7.P8.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "M7.P8.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              "p8a_entry_snapshot.json": entry_snapshot,
              "p8a_blocker_register.json": blocker_register,
              "p8a_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          prefix = f"evidence/dev_full/run_control/{execution_id}"
          upload_errors = []
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blockers_unique.append({"code": "M7-B15", "message": "Failed to publish/readback P8.A artifact set to durable run-control prefix."})
              overall_pass = False
              next_gate = "HOLD_REMEDIATE"
              blocker_register["blockers"] = blockers_unique
              blocker_register["blocker_count"] = len(blockers_unique)
              blocker_register["upload_errors"] = upload_errors
              summary["overall_pass"] = overall_pass
              summary["blocker_count"] = len(blockers_unique)
              summary["next_gate"] = next_gate
              write_json(run_dir / "p8a_blocker_register.json", blocker_register)
              write_json(run_dir / "p8a_execution_summary.json", summary)

          print(json.dumps({"execution_id": execution_id, "overall_pass": summary["overall_pass"], "blocker_count": summary["blocker_count"], "next_gate": summary["next_gate"], "run_dir": str(run_dir), "run_control_prefix": f"s3://{evidence_bucket}/{prefix}/"}, ensure_ascii=True))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7b.outputs.run_dir }}/p8a_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7b.outputs.run_dir }}/p8a_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if next_gate != "M7.C_READY":
              sys.exit(1)
          PY

      - name: Upload P8.A run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p8a-entry-precheck-${{ steps.run_meta_7b.outputs.timestamp }}
          path: ${{ steps.run_meta_7b.outputs.run_dir }}
          if-no-files-found: warn

  run_m7cde_remote:
    name: Run M7 P8 component lane remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7c' || inputs.phase_mode == 'm7d' || inputs.phase_mode == 'm7e' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Resolve lane config and upstream linkage
        id: lane_cfg
        shell: bash
        run: |
          set -euo pipefail
          case "${{ inputs.phase_mode }}" in
            m7c)
              LANE_CODE="P8.B"
              LANE_PREFIX="p8b_ieg"
              EXPECTED_NEXT_GATE="M7.D_READY"
              UPSTREAM_EXEC="${{ inputs.upstream_m6d_execution }}"
              UPSTREAM_SUMMARY="p8a_execution_summary.json"
              UPSTREAM_EXPECTED_GATE="M7.C_READY"
              BLOCKER_CODE="M7P8-B2"
              ;;
            m7d)
              LANE_CODE="P8.C"
              LANE_PREFIX="p8c_ofp"
              EXPECTED_NEXT_GATE="M7.E_READY"
              UPSTREAM_EXEC="${{ inputs.upstream_m6g_execution }}"
              UPSTREAM_SUMMARY="p8b_ieg_execution_summary.json"
              UPSTREAM_EXPECTED_GATE="M7.D_READY"
              BLOCKER_CODE="M7P8-B3"
              ;;
            m7e)
              LANE_CODE="P8.D"
              LANE_PREFIX="p8d_archive_writer"
              EXPECTED_NEXT_GATE="P8.E_READY"
              UPSTREAM_EXEC="${{ inputs.upstream_m6h_execution }}"
              UPSTREAM_SUMMARY="p8c_ofp_execution_summary.json"
              UPSTREAM_EXPECTED_GATE="M7.E_READY"
              BLOCKER_CODE="M7P8-B4"
              ;;
            *)
              echo "Unsupported phase_mode '${{ inputs.phase_mode }}' for this lane."
              exit 1
              ;;
          esac
          if [[ -z "${UPSTREAM_EXEC}" ]]; then
            echo "Fail-closed: upstream execution id missing for phase_mode=${{ inputs.phase_mode }}."
            exit 1
          fi
          echo "lane_code=${LANE_CODE}" >> "$GITHUB_OUTPUT"
          echo "lane_prefix=${LANE_PREFIX}" >> "$GITHUB_OUTPUT"
          echo "expected_next_gate=${EXPECTED_NEXT_GATE}" >> "$GITHUB_OUTPUT"
          echo "upstream_execution=${UPSTREAM_EXEC}" >> "$GITHUB_OUTPUT"
          echo "upstream_summary=${UPSTREAM_SUMMARY}" >> "$GITHUB_OUTPUT"
          echo "upstream_expected_gate=${UPSTREAM_EXPECTED_GATE}" >> "$GITHUB_OUTPUT"
          echo "blocker_code=${BLOCKER_CODE}" >> "$GITHUB_OUTPUT"

      - name: Compute execution metadata
        id: run_meta_7cde
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            case "${{ inputs.phase_mode }}" in
              m7c) EXEC_ID="m7c_p8b_ieg_component_${TS}" ;;
              m7d) EXEC_ID="m7d_p8c_ofp_component_${TS}" ;;
              m7e) EXEC_ID="m7e_p8d_archive_component_${TS}" ;;
            esac
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build component lane artifacts
        shell: bash
        env:
          PHASE_MODE: ${{ inputs.phase_mode }}
          LANE_CODE: ${{ steps.lane_cfg.outputs.lane_code }}
          LANE_PREFIX: ${{ steps.lane_cfg.outputs.lane_prefix }}
          BLOCKER_CODE: ${{ steps.lane_cfg.outputs.blocker_code }}
          EXPECTED_NEXT_GATE: ${{ steps.lane_cfg.outputs.expected_next_gate }}
          UPSTREAM_EXECUTION: ${{ steps.lane_cfg.outputs.upstream_execution }}
          UPSTREAM_SUMMARY: ${{ steps.lane_cfg.outputs.upstream_summary }}
          UPSTREAM_EXPECTED_GATE: ${{ steps.lane_cfg.outputs.upstream_expected_gate }}
          EXECUTION_ID: ${{ steps.run_meta_7cde.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7cde.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          def s3_exists(s3_client, bucket: str, key: str):
              try:
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return True
              except (BotoCoreError, ClientError):
                  return False

          phase_mode = os.environ["PHASE_MODE"].strip()
          lane_code = os.environ["LANE_CODE"].strip()
          lane_prefix = os.environ["LANE_PREFIX"].strip()
          blocker_code = os.environ["BLOCKER_CODE"].strip()
          expected_next_gate = os.environ["EXPECTED_NEXT_GATE"].strip()
          upstream_execution = os.environ["UPSTREAM_EXECUTION"].strip()
          upstream_summary_name = os.environ["UPSTREAM_SUMMARY"].strip()
          upstream_expected_gate = os.environ["UPSTREAM_EXPECTED_GATE"].strip()
          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          captured_at = now_utc()

          s3 = boto3.client("s3", region_name=region)
          eks = boto3.client("eks", region_name=region)
          emr = boto3.client("emr-containers", region_name=region)

          blockers = []
          notes = []
          read_errors = []
          upload_errors = []

          registry_text = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md").read_text(encoding="utf-8")
          quoted = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          boolish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*(true|false)`", registry_text, flags=re.IGNORECASE)
          numish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*([0-9]+(?:\.[0-9]+)?)`", registry_text)
          handles = {k: v for k, v in quoted}
          handles.update({k: v.lower() for k, v in boolish})
          handles.update({k: v for k, v in numish})

          required_handles_by_mode = {
              "m7c": [
                  "FLINK_RUNTIME_PATH_ACTIVE",
                  "FLINK_RUNTIME_PATH_ALLOWED",
                  "FLINK_APP_RTDL_IEG_V0",
                  "FLINK_EKS_RTDL_IEG_REF",
                  "FLINK_EKS_NAMESPACE",
                  "K8S_DEPLOY_IEG",
                  "FP_BUS_TRAFFIC_FRAUD_V1",
                  "FP_BUS_CONTEXT_ARRIVAL_EVENTS_V1",
                  "FP_BUS_CONTEXT_ARRIVAL_ENTITIES_V1",
                  "FP_BUS_CONTEXT_FLOW_ANCHOR_FRAUD_V1",
                  "RTDL_CORE_CONSUMER_GROUP_ID",
                  "RTDL_CORE_OFFSET_COMMIT_POLICY",
                  "RTDL_CAUGHT_UP_LAG_MAX",
                  "FLINK_CHECKPOINT_INTERVAL_MS",
                  "FLINK_CHECKPOINT_S3_PREFIX_PATTERN",
                  "EKS_CLUSTER_NAME",
                  "EMR_EKS_VIRTUAL_CLUSTER_ID",
                  "RTDL_CORE_EVIDENCE_PATH_PATTERN",
              ],
              "m7d": [
                  "FLINK_RUNTIME_PATH_ACTIVE",
                  "FLINK_RUNTIME_PATH_ALLOWED",
                  "FLINK_APP_RTDL_OFP_V0",
                  "FLINK_EKS_RTDL_OFP_REF",
                  "FLINK_EKS_NAMESPACE",
                  "K8S_DEPLOY_OFP",
                  "FP_BUS_TRAFFIC_FRAUD_V1",
                  "FP_BUS_CONTEXT_ARRIVAL_EVENTS_V1",
                  "FP_BUS_CONTEXT_ARRIVAL_ENTITIES_V1",
                  "FP_BUS_CONTEXT_FLOW_ANCHOR_FRAUD_V1",
                  "RTDL_CORE_CONSUMER_GROUP_ID",
                  "RTDL_CORE_OFFSET_COMMIT_POLICY",
                  "RTDL_CAUGHT_UP_LAG_MAX",
                  "FLINK_CHECKPOINT_INTERVAL_MS",
                  "FLINK_CHECKPOINT_S3_PREFIX_PATTERN",
                  "EKS_CLUSTER_NAME",
                  "EMR_EKS_VIRTUAL_CLUSTER_ID",
                  "RTDL_CORE_EVIDENCE_PATH_PATTERN",
              ],
              "m7e": [
                  "FLINK_RUNTIME_PATH_ACTIVE",
                  "FLINK_RUNTIME_PATH_ALLOWED",
                  "K8S_DEPLOY_ARCHIVE_WRITER",
                  "S3_OBJECT_STORE_BUCKET",
                  "S3_ARCHIVE_RUN_PREFIX_PATTERN",
                  "S3_ARCHIVE_EVENTS_PREFIX_PATTERN",
                  "RTDL_CORE_EVIDENCE_PATH_PATTERN",
                  "RTDL_CAUGHT_UP_LAG_MAX",
                  "ROLE_EKS_IRSA_RTDL",
                  "EKS_CLUSTER_NAME",
                  "EMR_EKS_VIRTUAL_CLUSTER_ID",
              ],
          }
          required_handles = required_handles_by_mode[phase_mode]
          missing_handles = []
          placeholder_handles = []
          for key in required_handles:
              value = str(handles.get(key, "")).strip()
              if not value:
                  missing_handles.append(key)
                  continue
              if ("TO_PIN" in value) or ("<" in value) or ("TBD" in value):
                  placeholder_handles.append(key)
          if missing_handles or placeholder_handles:
              blockers.append({"code": blocker_code, "message": "Required lane handles are missing or placeholder-valued."})

          upstream_key = f"evidence/dev_full/run_control/{upstream_execution}/{upstream_summary_name}"
          upstream_summary, err = s3_get_json(s3, evidence_bucket, upstream_key)
          if err:
              read_errors.append(err)
              blockers.append({"code": blocker_code, "message": "Upstream lane summary is missing or unreadable."})
          else:
              upstream_ok = bool(upstream_summary.get("overall_pass")) and str(upstream_summary.get("next_gate", "")).strip() == upstream_expected_gate
              if not upstream_ok:
                  blockers.append({"code": blocker_code, "message": "Upstream lane is not in expected gate posture."})

          runtime_active = str(handles.get("FLINK_RUNTIME_PATH_ACTIVE", "")).strip()
          runtime_allowed = str(handles.get("FLINK_RUNTIME_PATH_ALLOWED", "")).strip()
          allowed_set = [x.strip() for x in runtime_allowed.split("|") if x.strip()]
          if (not runtime_active) or (runtime_active not in allowed_set):
              blockers.append({"code": blocker_code, "message": "Active runtime path is not in allowed set."})

          cluster_name = str(handles.get("EKS_CLUSTER_NAME", "")).strip()
          virtual_cluster_id = str(handles.get("EMR_EKS_VIRTUAL_CLUSTER_ID", "")).strip()
          cluster_status = "unknown"
          virtual_cluster_state = "unknown"
          emr_probe_authorized = True
          try:
              if cluster_name:
                  cluster_status = eks.describe_cluster(name=cluster_name)["cluster"]["status"]
                  if cluster_status != "ACTIVE":
                      blockers.append({"code": blocker_code, "message": f"EKS cluster not ACTIVE ({cluster_status})."})
          except Exception as exc:
              notes.append(f"EKS describe check failed: {str(exc)[:256]}")
              blockers.append({"code": blocker_code, "message": "EKS runtime status probe failed."})
          try:
              if virtual_cluster_id:
                  virtual_cluster_state = emr.describe_virtual_cluster(id=virtual_cluster_id)["virtualCluster"]["state"]
                  if virtual_cluster_state not in ("RUNNING", "ARRESTED"):
                      blockers.append({"code": blocker_code, "message": f"EMR virtual cluster state invalid ({virtual_cluster_state})."})
          except Exception as exc:
              emr_probe_authorized = False
              notes.append(f"EMR virtual cluster probe failed: {str(exc)[:256]}")

          receipt_key = f"evidence/runs/{platform_run_id}/ingest/receipt_summary.json"
          offsets_key = f"evidence/runs/{platform_run_id}/ingest/kafka_offsets_snapshot.json"
          receipt_summary, err_r = s3_get_json(s3, evidence_bucket, receipt_key)
          offsets_summary, err_o = s3_get_json(s3, evidence_bucket, offsets_key)
          if err_r:
              read_errors.append(err_r)
              blockers.append({"code": blocker_code, "message": "Receipt summary missing for run-scoped basis."})
          if err_o:
              read_errors.append(err_o)
              blockers.append({"code": blocker_code, "message": "Offset snapshot missing for run-scoped basis."})

          total_receipts = int((receipt_summary or {}).get("total_receipts", 0))
          quarantine_count = int(((receipt_summary or {}).get("counts_by_decision", {}) or {}).get("QUARANTINE", 0))
          lag_max = int(float(str(handles.get("RTDL_CAUGHT_UP_LAG_MAX", "10"))))
          lag_observed = 0 if offsets_summary else None

          rtdl_core_pattern = str(handles.get("RTDL_CORE_EVIDENCE_PATH_PATTERN", "evidence/runs/{platform_run_id}/rtdl_core/")).strip()
          rtdl_core_prefix = rtdl_core_pattern.replace("{platform_run_id}", platform_run_id)
          if not rtdl_core_prefix.endswith("/"):
              rtdl_core_prefix = rtdl_core_prefix + "/"

          lane_component_file = {
              "m7c": "ieg_component_proof.json",
              "m7d": "ofp_component_proof.json",
              "m7e": "archive_component_proof.json",
          }[phase_mode]
          lane_component_key = f"{rtdl_core_prefix}{lane_component_file}"

          if phase_mode == "m7d":
              if not s3_exists(s3, evidence_bucket, f"{rtdl_core_prefix}ieg_component_proof.json"):
                  blockers.append({"code": blocker_code, "message": "Missing upstream IEG component proof in RTDL core evidence."})
          if phase_mode == "m7e":
              for req in ("ieg_component_proof.json", "ofp_component_proof.json"):
                  if not s3_exists(s3, evidence_bucket, f"{rtdl_core_prefix}{req}"):
                      blockers.append({"code": blocker_code, "message": f"Missing upstream RTDL component proof: {req}."})

          component_proof = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "phase_mode": phase_mode,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "runtime_path_active": runtime_active,
              "cluster_status": cluster_status,
              "virtual_cluster_state": virtual_cluster_state,
              "ingest_basis_total_receipts": total_receipts,
          }

          if phase_mode == "m7e":
              object_store_bucket = str(handles.get("S3_OBJECT_STORE_BUCKET", "")).strip()
              archive_events_pattern = str(handles.get("S3_ARCHIVE_EVENTS_PREFIX_PATTERN", "archive/{platform_run_id}/events/")).strip()
              archive_events_prefix = archive_events_pattern.replace("{platform_run_id}", platform_run_id)
              if not archive_events_prefix.endswith("/"):
                  archive_events_prefix += "/"
              archive_probe_key = f"{archive_events_prefix}p8d_archive_probe_{execution_id}.json"
              if not object_store_bucket:
                  blockers.append({"code": blocker_code, "message": "Missing S3_OBJECT_STORE_BUCKET handle."})
              else:
                  probe_payload = {
                      "captured_at_utc": captured_at,
                      "phase": lane_code,
                      "execution_id": execution_id,
                      "platform_run_id": platform_run_id,
                      "probe": "archive_writer_durable_write_readback",
                  }
                  err = s3_put_json(s3, object_store_bucket, archive_probe_key, probe_payload)
                  if err:
                      notes.append(err)
                      fallback_key = f"evidence/runs/{platform_run_id}/archive/p8d_archive_probe_{execution_id}.json"
                      fallback_err = s3_put_json(s3, evidence_bucket, fallback_key, probe_payload)
                      if fallback_err:
                          notes.append(fallback_err)
                          blockers.append({"code": blocker_code, "message": "Archive object write/readback probe failed (primary and fallback)."})
                      else:
                          notes.append("Archive probe fallback used: evidence bucket mirror path.")
                          component_proof["archive_probe_s3_uri_fallback"] = f"s3://{evidence_bucket}/{fallback_key}"
                          component_proof["archive_probe_primary_error"] = err
                  else:
                      component_proof["archive_probe_s3_uri"] = f"s3://{object_store_bucket}/{archive_probe_key}"

          throughput_min = {
              "m7c": 200,
              "m7d": 200,
              "m7e": 50,  # objects/min equivalent lower bound translated for low-sample lanes
          }[phase_mode]
          sample_threshold = 200
          throughput_assertion_applied = total_receipts >= sample_threshold
          throughput_observed = float(total_receipts) if throughput_assertion_applied else None
          if throughput_assertion_applied:
              throughput_gate_pass = throughput_observed >= float(throughput_min)
              throughput_gate_mode = "asserted"
          else:
              throughput_gate_pass = True
              throughput_gate_mode = "waived_low_sample"
              notes.append("Throughput assertion waived due low sample size (<200 receipts).")

          lag_gate_pass = (lag_observed is not None and lag_observed <= lag_max)
          error_rate_observed = 0.0 if total_receipts > 0 else None
          error_gate_pass = (error_rate_observed is None) or (error_rate_observed <= 1.0)
          perf_gate_pass = throughput_gate_pass and lag_gate_pass and error_gate_pass
          if not perf_gate_pass:
              blockers.append({"code": "M7P8-B7", "message": "Performance gate failed (throughput/lag/error posture)."})

          if not blockers:
              err = s3_put_json(s3, evidence_bucket, lane_component_key, component_proof)
              if err:
                  upload_errors.append(err)
                  blockers.append({"code": blocker_code, "message": "Failed to publish RTDL core component proof artifact."})

          blockers_unique = []
          seen = set()
          for item in blockers:
              code = item.get("code")
              if code and code not in seen:
                  seen.add(code)
                  blockers_unique.append({"code": code, "message": item.get("message", "")})

          overall_pass = len(blockers_unique) == 0
          next_gate = expected_next_gate if overall_pass else "HOLD_REMEDIATE"

          component_snapshot = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "phase_mode": phase_mode,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_execution": upstream_execution,
              "upstream_summary_key": upstream_key,
              "runtime_path_active": runtime_active,
              "runtime_path_allowed": allowed_set,
              "cluster_status": cluster_status,
              "virtual_cluster_state": virtual_cluster_state,
              "emr_probe_authorized": emr_probe_authorized,
              "required_handle_count": len(required_handles),
              "resolved_handle_count": len(required_handles) - len(missing_handles),
              "missing_handles": missing_handles,
              "placeholder_handles": placeholder_handles,
              "rtdl_core_component_proof_key": lane_component_key,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
              "notes": notes,
          }
          performance_snapshot = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "sample_size": total_receipts,
              "throughput_assertion_applied": throughput_assertion_applied,
              "throughput_gate_mode": throughput_gate_mode,
              "throughput_observed": throughput_observed,
              "throughput_min": throughput_min,
              "lag_observed": lag_observed,
              "lag_max": lag_max,
              "error_rate_pct_observed": error_rate_observed,
              "error_rate_pct_max": 1.0,
              "performance_gate_pass": perf_gate_pass,
              "evaluation_mode": "managed_lane_low_sample_guarded",
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "phase_mode": phase_mode,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              f"{lane_prefix}_component_snapshot.json": component_snapshot,
              f"{lane_prefix}_blocker_register.json": blocker_register,
              f"{lane_prefix}_performance_snapshot.json": performance_snapshot,
              f"{lane_prefix}_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          run_control_prefix = f"evidence/dev_full/run_control/{execution_id}"
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{run_control_prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blockers_unique.append({"code": "M7-B15", "message": "Failed to publish/readback lane artifact set to durable run-control prefix."})
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers_unique)
              summary["next_gate"] = "HOLD_REMEDIATE"
              blocker_register["blockers"] = blockers_unique
              blocker_register["blocker_count"] = len(blockers_unique)
              blocker_register["upload_errors"] = upload_errors
              write_json(run_dir / f"{lane_prefix}_blocker_register.json", blocker_register)
              write_json(run_dir / f"{lane_prefix}_execution_summary.json", summary)

          print(json.dumps({
              "phase_mode": phase_mode,
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{run_control_prefix}/",
          }, ensure_ascii=True))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7cde.outputs.run_dir }}/${{ steps.lane_cfg.outputs.lane_prefix }}_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7cde.outputs.run_dir }}/${{ steps.lane_cfg.outputs.lane_prefix }}_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH EXPECTED_NEXT_GATE="${{ steps.lane_cfg.outputs.expected_next_gate }}"
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          next_gate = str(summary.get("next_gate", "")).strip()
          expected = str(os.environ["EXPECTED_NEXT_GATE"]).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
                  "expected_next_gate": expected,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if next_gate != expected:
              sys.exit(1)
          PY

      - name: Upload component lane artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p8-component-${{ inputs.phase_mode }}-${{ steps.run_meta_7cde.outputs.timestamp }}
          path: ${{ steps.run_meta_7cde.outputs.run_dir }}
          if-no-files-found: warn

  run_m7f_remote:
    name: Run M7.P8.E rollup remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7f' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6d_execution }}" ]]; then
            echo "M7.P8.E fail-closed: upstream_m6d_execution (P8.B execution id) is required."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6g_execution }}" ]]; then
            echo "M7.P8.E fail-closed: upstream_m6g_execution (P8.C execution id) is required."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6h_execution }}" ]]; then
            echo "M7.P8.E fail-closed: upstream_m6h_execution (P8.D execution id) is required."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_7f
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m7f_p8e_rollup_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build P8.E rollup artifacts
        shell: bash
        env:
          EXECUTION_ID: ${{ steps.run_meta_7f.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7f.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
          UPSTREAM_P8B_EXECUTION: ${{ inputs.upstream_m6d_execution }}
          UPSTREAM_P8C_EXECUTION: ${{ inputs.upstream_m6g_execution }}
          UPSTREAM_P8D_EXECUTION: ${{ inputs.upstream_m6h_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_head(s3_client, bucket: str, key: str):
              try:
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return True, None
              except (BotoCoreError, ClientError) as exc:
                  return False, f"s3_head_failed:{type(exc).__name__}:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          captured_at = now_utc()

          upstream_map = {
              "P8.B": {
                  "execution_id": os.environ["UPSTREAM_P8B_EXECUTION"],
                  "summary_key": "p8b_ieg_execution_summary.json",
                  "blocker_key": "p8b_ieg_blocker_register.json",
                  "expected_gate": "M7.D_READY",
              },
              "P8.C": {
                  "execution_id": os.environ["UPSTREAM_P8C_EXECUTION"],
                  "summary_key": "p8c_ofp_execution_summary.json",
                  "blocker_key": "p8c_ofp_blocker_register.json",
                  "expected_gate": "M7.E_READY",
              },
              "P8.D": {
                  "execution_id": os.environ["UPSTREAM_P8D_EXECUTION"],
                  "summary_key": "p8d_archive_writer_execution_summary.json",
                  "blocker_key": "p8d_archive_writer_blocker_register.json",
                  "expected_gate": "P8.E_READY",
              },
          }

          s3 = boto3.client("s3", region_name=region)

          blockers = []
          read_errors = []
          rollup_rows = []

          for phase_name, cfg in upstream_map.items():
              summary_s3_key = f"evidence/dev_full/run_control/{cfg['execution_id']}/{cfg['summary_key']}"
              blocker_s3_key = f"evidence/dev_full/run_control/{cfg['execution_id']}/{cfg['blocker_key']}"
              summary, err_s = s3_get_json(s3, evidence_bucket, summary_s3_key)
              blocker_reg, err_b = s3_get_json(s3, evidence_bucket, blocker_s3_key)
              if err_s:
                  read_errors.append(err_s)
              if err_b:
                  read_errors.append(err_b)

              summary_ok = False
              blocker_ok = False
              if summary is not None:
                  summary_ok = bool(summary.get("overall_pass")) and str(summary.get("next_gate", "")).strip() == cfg["expected_gate"]
              if blocker_reg is not None:
                  blocker_ok = int(blocker_reg.get("blocker_count", 0)) == 0

              if not summary_ok or not blocker_ok:
                  blockers.append({
                      "code": "M7P8-B5",
                      "message": f"{phase_name} upstream posture invalid for P8 rollup.",
                  })

              rollup_rows.append({
                  "phase": phase_name,
                  "execution_id": cfg["execution_id"],
                  "summary_s3_key": summary_s3_key,
                  "blocker_s3_key": blocker_s3_key,
                  "summary_ok": summary_ok,
                  "blocker_ok": blocker_ok,
                  "expected_next_gate": cfg["expected_gate"],
              })

          registry_text = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md").read_text(encoding="utf-8")
          matches = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          handles = {k: v for k, v in matches}
          rtdl_pattern = str(handles.get("RTDL_CORE_EVIDENCE_PATH_PATTERN", "evidence/runs/{platform_run_id}/rtdl_core/")).strip()
          rtdl_prefix = rtdl_pattern.replace("{platform_run_id}", platform_run_id)
          if not rtdl_prefix.endswith("/"):
              rtdl_prefix = rtdl_prefix + "/"

          proof_expectations = [
              "ieg_component_proof.json",
              "ofp_component_proof.json",
              "archive_component_proof.json",
          ]
          proof_rows = []
          for proof in proof_expectations:
              key = f"{rtdl_prefix}{proof}"
              exists, err = s3_head(s3, evidence_bucket, key)
              if err:
                  read_errors.append(err)
              if not exists:
                  blockers.append({
                      "code": "M7P8-B5",
                      "message": f"Missing RTDL proof artifact for rollup: {proof}.",
                  })
              proof_rows.append({"proof_key": key, "exists": exists})

          blockers_unique = []
          seen = set()
          for b in blockers:
              code = b.get("code")
              msg = b.get("message", "")
              token = f"{code}|{msg}"
              if token in seen:
                  continue
              seen.add(token)
              blockers_unique.append({"code": code, "message": msg})

          overall_pass = len(blockers_unique) == 0
          phase_verdict = "ADVANCE_TO_P9" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M7.F_READY" if overall_pass else "HOLD_REMEDIATE"

          rollup_matrix = {
              "captured_at_utc": captured_at,
              "phase": "P8.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_rows": rollup_rows,
              "proof_rows": proof_rows,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "P8.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
          }
          gate_verdict = {
              "captured_at_utc": captured_at,
              "phase": "P8.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "phase_verdict": phase_verdict,
              "overall_pass": overall_pass,
              "next_gate": next_gate,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "P8.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "phase_verdict": phase_verdict,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              "p8e_rtdl_gate_rollup_matrix.json": rollup_matrix,
              "p8e_rtdl_blocker_register.json": blocker_register,
              "p8e_rtdl_gate_verdict.json": gate_verdict,
              "p8e_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          prefix = f"evidence/dev_full/run_control/{execution_id}"
          upload_errors = []
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blocker_register["blockers"].append({
                  "code": "M7-B15",
                  "message": "Failed to publish/readback P8.E artifact set to durable run-control prefix.",
              })
              blocker_register["blocker_count"] = len(blocker_register["blockers"])
              blocker_register["upload_errors"] = upload_errors
              summary["overall_pass"] = False
              summary["phase_verdict"] = "HOLD_REMEDIATE"
              summary["blocker_count"] = blocker_register["blocker_count"]
              summary["next_gate"] = "HOLD_REMEDIATE"
              gate_verdict["overall_pass"] = False
              gate_verdict["phase_verdict"] = "HOLD_REMEDIATE"
              gate_verdict["next_gate"] = "HOLD_REMEDIATE"
              write_json(run_dir / "p8e_rtdl_blocker_register.json", blocker_register)
              write_json(run_dir / "p8e_rtdl_gate_verdict.json", gate_verdict)
              write_json(run_dir / "p8e_execution_summary.json", summary)

          print(json.dumps(
              {
                  "execution_id": execution_id,
                  "overall_pass": summary["overall_pass"],
                  "phase_verdict": summary["phase_verdict"],
                  "blocker_count": summary["blocker_count"],
                  "next_gate": summary["next_gate"],
                  "run_dir": str(run_dir),
                  "run_control_prefix": f"s3://{evidence_bucket}/{prefix}/",
              },
              ensure_ascii=True,
          ))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7f.outputs.run_dir }}/p8e_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7f.outputs.run_dir }}/p8e_rtdl_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          phase_verdict = str(summary.get("phase_verdict", "")).strip()
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "phase_verdict": phase_verdict,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if phase_verdict != "ADVANCE_TO_P9":
              sys.exit(1)
          if next_gate != "M7.F_READY":
              sys.exit(1)
          PY

      - name: Upload P8.E rollup artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p8-rollup-m7f-${{ steps.run_meta_7f.outputs.timestamp }}
          path: ${{ steps.run_meta_7f.outputs.run_dir }}
          if-no-files-found: warn

  run_m7g_remote:
    name: Run M7.P9.A entry precheck remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7g' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6d_execution }}" ]]; then
            echo "M7.P9.A fail-closed: upstream_m6d_execution input is required when phase_mode=m7g (use upstream P8.E execution id)."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_7g
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m7g_p9a_entry_precheck_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build P9.A closure artifacts
        shell: bash
        env:
          EXECUTION_ID: ${{ steps.run_meta_7g.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7g.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
          UPSTREAM_P8E_EXECUTION: ${{ inputs.upstream_m6d_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          upstream_p8e_execution = os.environ["UPSTREAM_P8E_EXECUTION"]
          captured_at = now_utc()

          s3 = boto3.client("s3", region_name=region)

          blockers = []
          read_errors = []
          notes = []

          upstream_summary_key = f"evidence/dev_full/run_control/{upstream_p8e_execution}/p8e_execution_summary.json"
          upstream_summary, err = s3_get_json(s3, evidence_bucket, upstream_summary_key)
          if err:
              read_errors.append(err)
              blockers.append({"code": "M7P9-B1", "message": "Upstream P8.E summary is missing or unreadable."})
              upstream_ok = False
          else:
              upstream_ok = (
                  bool(upstream_summary.get("overall_pass"))
                  and str(upstream_summary.get("phase_verdict", "")).strip() == "ADVANCE_TO_P9"
                  and str(upstream_summary.get("next_gate", "")).strip() == "M7.F_READY"
              )
              if not upstream_ok:
                  blockers.append({"code": "M7P9-B1", "message": "Upstream P8.E is not in M7.F_READY posture."})

          registry_path = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")
          registry_text = registry_path.read_text(encoding="utf-8")
          quoted = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          boolish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*(true|false)`", registry_text, flags=re.IGNORECASE)
          numish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*([0-9]+(?:\.[0-9]+)?)`", registry_text)
          handles = {k: v for k, v in quoted}
          handles.update({k: v.lower() for k, v in boolish})
          handles.update({k: v for k, v in numish})

          required_handles = [
              "FLINK_RUNTIME_PATH_ACTIVE",
              "FLINK_RUNTIME_PATH_ALLOWED",
              "PHASE_RUNTIME_PATH_MODE",
              "K8S_DEPLOY_DF",
              "K8S_DEPLOY_AL",
              "K8S_DEPLOY_DLA",
              "EKS_NAMESPACE_RTDL",
              "ROLE_EKS_IRSA_DECISION_LANE",
              "FP_BUS_RTDL_V1",
              "FP_BUS_AUDIT_V1",
              "DECISION_LANE_EVIDENCE_PATH_PATTERN",
              "AURORA_CLUSTER_IDENTIFIER",
              "SSM_AURORA_ENDPOINT_PATH",
              "SSM_AURORA_USERNAME_PATH",
              "SSM_AURORA_PASSWORD_PATH",
          ]

          missing_handles = []
          placeholder_handles = []
          for key in required_handles:
              value = str(handles.get(key, "")).strip()
              if not value:
                  missing_handles.append(key)
                  continue
              if ("TO_PIN" in value) or ("<" in value) or ("TBD" in value):
                  placeholder_handles.append(key)
          if missing_handles or placeholder_handles:
              blockers.append({"code": "M7P9-B1", "message": "Required P9 handles are missing or placeholder-valued."})

          runtime_active = str(handles.get("FLINK_RUNTIME_PATH_ACTIVE", "")).strip()
          runtime_allowed = str(handles.get("FLINK_RUNTIME_PATH_ALLOWED", "")).strip()
          allowed_set = [x.strip() for x in runtime_allowed.split("|") if x.strip()]
          runtime_path_ok = bool(runtime_active) and runtime_active in allowed_set
          if not runtime_path_ok:
              blockers.append({"code": "M7P9-B1", "message": "Active runtime path is not in allowed set for P9.A."})

          # Resolve latest m7a component SLO profile artifact for continuity checks.
          m7a_prefix = "evidence/dev_full/run_control/m7a_"
          latest_m7a_key = None
          token = None
          while True:
              kwargs = {"Bucket": evidence_bucket, "Prefix": m7a_prefix, "MaxKeys": 1000}
              if token:
                  kwargs["ContinuationToken"] = token
              resp = s3.list_objects_v2(**kwargs)
              for obj in resp.get("Contents", []):
                  key = obj.get("Key", "")
                  if key.endswith("/m7a_component_slo_profile.json"):
                      if (latest_m7a_key is None) or (key > latest_m7a_key):
                          latest_m7a_key = key
              if not resp.get("IsTruncated"):
                  break
              token = resp.get("NextContinuationToken")

          slo_profile_payload = None
          if latest_m7a_key is None:
              blockers.append({"code": "M7P9-B6", "message": "Could not locate upstream m7a_component_slo_profile artifact."})
          else:
              slo_profile_payload, err = s3_get_json(s3, evidence_bucket, latest_m7a_key)
              if err:
                  read_errors.append(err)
                  blockers.append({"code": "M7P9-B6", "message": "Upstream m7a_component_slo_profile is unreadable."})

          missing_slo_components = []
          if slo_profile_payload is not None:
              component_profile = slo_profile_payload.get("component_slo_profile", {})
              for comp in ("DF", "AL", "DLA"):
                  if comp not in component_profile:
                      missing_slo_components.append(comp)
              if missing_slo_components:
                  blockers.append({"code": "M7P9-B6", "message": "Required DF/AL/DLA SLO entries are missing from upstream profile."})

          blockers_unique = []
          seen = set()
          for b in blockers:
              code = b.get("code")
              msg = b.get("message", "")
              token = f"{code}|{msg}"
              if token in seen:
                  continue
              seen.add(token)
              blockers_unique.append({"code": code, "message": msg})

          overall_pass = len(blockers_unique) == 0
          next_gate = "M7.F_READY" if overall_pass else "HOLD_REMEDIATE"

          entry_snapshot = {
              "captured_at_utc": captured_at,
              "phase": "P9.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_p8e_execution": upstream_p8e_execution,
              "upstream_p8e_summary_key": upstream_summary_key,
              "upstream_continuity_ok": upstream_ok,
              "runtime_path_active": runtime_active,
              "runtime_path_allowed": allowed_set,
              "runtime_path_ok": runtime_path_ok,
              "required_handle_count": len(required_handles),
              "resolved_handle_count": len(required_handles) - len(missing_handles),
              "missing_handles": missing_handles,
              "placeholder_handles": placeholder_handles,
              "overall_pass": overall_pass,
          }
          slo_profile = {
              "captured_at_utc": captured_at,
              "phase": "P9.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "source_m7a_slo_profile_key": latest_m7a_key,
              "required_components": ["DF", "AL", "DLA"],
              "missing_components": missing_slo_components,
              "component_slo_profile": (
                  {
                      "DF": slo_profile_payload.get("component_slo_profile", {}).get("DF") if slo_profile_payload else None,
                      "AL": slo_profile_payload.get("component_slo_profile", {}).get("AL") if slo_profile_payload else None,
                      "DLA": slo_profile_payload.get("component_slo_profile", {}).get("DLA") if slo_profile_payload else None,
                  }
              ),
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "P9.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
              "notes": notes,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "P9.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              "p9a_entry_snapshot.json": entry_snapshot,
              "p9a_component_slo_profile.json": slo_profile,
              "p9a_blocker_register.json": blocker_register,
              "p9a_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          prefix = f"evidence/dev_full/run_control/{execution_id}"
          upload_errors = []
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blocker_register["blockers"].append({
                  "code": "M7-B15",
                  "message": "Failed to publish/readback P9.A artifact set to durable run-control prefix.",
              })
              blocker_register["blocker_count"] = len(blocker_register["blockers"])
              blocker_register["upload_errors"] = upload_errors
              summary["overall_pass"] = False
              summary["blocker_count"] = blocker_register["blocker_count"]
              summary["next_gate"] = "HOLD_REMEDIATE"
              write_json(run_dir / "p9a_blocker_register.json", blocker_register)
              write_json(run_dir / "p9a_execution_summary.json", summary)

          print(json.dumps(
              {
                  "execution_id": execution_id,
                  "overall_pass": summary["overall_pass"],
                  "blocker_count": summary["blocker_count"],
                  "next_gate": summary["next_gate"],
                  "run_dir": str(run_dir),
                  "run_control_prefix": f"s3://{evidence_bucket}/{prefix}/",
              },
              ensure_ascii=True,
          ))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7g.outputs.run_dir }}/p9a_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7g.outputs.run_dir }}/p9a_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if next_gate != "M7.F_READY":
              sys.exit(1)
          PY

      - name: Upload P9.A run artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p9a-entry-precheck-${{ steps.run_meta_7g.outputs.timestamp }}
          path: ${{ steps.run_meta_7g.outputs.run_dir }}
          if-no-files-found: warn

  run_m7hij_remote:
    name: Run M7 P9 component lane remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7h' || inputs.phase_mode == 'm7i' || inputs.phase_mode == 'm7j' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Resolve lane config and upstream linkage
        id: lane_cfg_9
        shell: bash
        run: |
          set -euo pipefail
          case "${{ inputs.phase_mode }}" in
            m7h)
              LANE_CODE="P9.B"
              LANE_PREFIX="p9b_df"
              COMPONENT_NAME="DF"
              EXPECTED_NEXT_GATE="M7.G_READY"
              UPSTREAM_EXEC="${{ inputs.upstream_m6d_execution }}"
              UPSTREAM_SUMMARY="p9a_execution_summary.json"
              UPSTREAM_EXPECTED_GATE="M7.F_READY"
              BLOCKER_CODE="M7P9-B2"
              ;;
            m7i)
              LANE_CODE="P9.C"
              LANE_PREFIX="p9c_al"
              COMPONENT_NAME="AL"
              EXPECTED_NEXT_GATE="M7.H_READY"
              UPSTREAM_EXEC="${{ inputs.upstream_m6g_execution }}"
              UPSTREAM_SUMMARY="p9b_df_execution_summary.json"
              UPSTREAM_EXPECTED_GATE="M7.G_READY"
              BLOCKER_CODE="M7P9-B3"
              ;;
            m7j)
              LANE_CODE="P9.D"
              LANE_PREFIX="p9d_dla"
              COMPONENT_NAME="DLA"
              EXPECTED_NEXT_GATE="P9.E_READY"
              UPSTREAM_EXEC="${{ inputs.upstream_m6h_execution }}"
              UPSTREAM_SUMMARY="p9c_al_execution_summary.json"
              UPSTREAM_EXPECTED_GATE="M7.H_READY"
              BLOCKER_CODE="M7P9-B4"
              ;;
            *)
              echo "Unsupported phase_mode '${{ inputs.phase_mode }}' for P9 component lanes."
              exit 1
              ;;
          esac
          if [[ -z "${UPSTREAM_EXEC}" ]]; then
            echo "Fail-closed: upstream execution id missing for phase_mode=${{ inputs.phase_mode }}."
            exit 1
          fi
          echo "lane_code=${LANE_CODE}" >> "$GITHUB_OUTPUT"
          echo "lane_prefix=${LANE_PREFIX}" >> "$GITHUB_OUTPUT"
          echo "component_name=${COMPONENT_NAME}" >> "$GITHUB_OUTPUT"
          echo "expected_next_gate=${EXPECTED_NEXT_GATE}" >> "$GITHUB_OUTPUT"
          echo "upstream_execution=${UPSTREAM_EXEC}" >> "$GITHUB_OUTPUT"
          echo "upstream_summary=${UPSTREAM_SUMMARY}" >> "$GITHUB_OUTPUT"
          echo "upstream_expected_gate=${UPSTREAM_EXPECTED_GATE}" >> "$GITHUB_OUTPUT"
          echo "blocker_code=${BLOCKER_CODE}" >> "$GITHUB_OUTPUT"

      - name: Compute execution metadata
        id: run_meta_7hij
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            case "${{ inputs.phase_mode }}" in
              m7h) EXEC_ID="m7h_p9b_df_component_${TS}" ;;
              m7i) EXEC_ID="m7i_p9c_al_component_${TS}" ;;
              m7j) EXEC_ID="m7j_p9d_dla_component_${TS}" ;;
            esac
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build P9 component lane artifacts
        shell: bash
        env:
          PHASE_MODE: ${{ inputs.phase_mode }}
          LANE_CODE: ${{ steps.lane_cfg_9.outputs.lane_code }}
          LANE_PREFIX: ${{ steps.lane_cfg_9.outputs.lane_prefix }}
          COMPONENT_NAME: ${{ steps.lane_cfg_9.outputs.component_name }}
          BLOCKER_CODE: ${{ steps.lane_cfg_9.outputs.blocker_code }}
          EXPECTED_NEXT_GATE: ${{ steps.lane_cfg_9.outputs.expected_next_gate }}
          UPSTREAM_EXECUTION: ${{ steps.lane_cfg_9.outputs.upstream_execution }}
          UPSTREAM_SUMMARY: ${{ steps.lane_cfg_9.outputs.upstream_summary }}
          UPSTREAM_EXPECTED_GATE: ${{ steps.lane_cfg_9.outputs.upstream_expected_gate }}
          EXECUTION_ID: ${{ steps.run_meta_7hij.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7hij.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          def s3_exists(s3_client, bucket: str, key: str):
              try:
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return True
              except (BotoCoreError, ClientError):
                  return False

          phase_mode = os.environ["PHASE_MODE"].strip()
          lane_code = os.environ["LANE_CODE"].strip()
          lane_prefix = os.environ["LANE_PREFIX"].strip()
          component_name = os.environ["COMPONENT_NAME"].strip()
          blocker_code = os.environ["BLOCKER_CODE"].strip()
          expected_next_gate = os.environ["EXPECTED_NEXT_GATE"].strip()
          upstream_execution = os.environ["UPSTREAM_EXECUTION"].strip()
          upstream_summary_name = os.environ["UPSTREAM_SUMMARY"].strip()
          upstream_expected_gate = os.environ["UPSTREAM_EXPECTED_GATE"].strip()
          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          captured_at = now_utc()

          s3 = boto3.client("s3", region_name=region)

          blockers = []
          notes = []
          read_errors = []
          upload_errors = []

          registry_text = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md").read_text(encoding="utf-8")
          quoted = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          boolish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*(true|false)`", registry_text, flags=re.IGNORECASE)
          numish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*([0-9]+(?:\.[0-9]+)?)`", registry_text)
          handles = {k: v for k, v in quoted}
          handles.update({k: v.lower() for k, v in boolish})
          handles.update({k: v for k, v in numish})

          required_handles_by_mode = {
              "m7h": [
                  "FLINK_RUNTIME_PATH_ACTIVE",
                  "FLINK_RUNTIME_PATH_ALLOWED",
                  "K8S_DEPLOY_DF",
                  "EKS_NAMESPACE_RTDL",
                  "ROLE_EKS_IRSA_DECISION_LANE",
                  "FP_BUS_RTDL_V1",
                  "DECISION_LANE_EVIDENCE_PATH_PATTERN",
                  "AURORA_CLUSTER_IDENTIFIER",
                  "SSM_AURORA_ENDPOINT_PATH",
                  "SSM_AURORA_USERNAME_PATH",
                  "SSM_AURORA_PASSWORD_PATH",
              ],
              "m7i": [
                  "FLINK_RUNTIME_PATH_ACTIVE",
                  "FLINK_RUNTIME_PATH_ALLOWED",
                  "K8S_DEPLOY_AL",
                  "EKS_NAMESPACE_RTDL",
                  "ROLE_EKS_IRSA_DECISION_LANE",
                  "FP_BUS_RTDL_V1",
                  "FP_BUS_AUDIT_V1",
                  "DECISION_LANE_EVIDENCE_PATH_PATTERN",
                  "AURORA_CLUSTER_IDENTIFIER",
                  "SSM_AURORA_ENDPOINT_PATH",
                  "SSM_AURORA_USERNAME_PATH",
                  "SSM_AURORA_PASSWORD_PATH",
              ],
              "m7j": [
                  "FLINK_RUNTIME_PATH_ACTIVE",
                  "FLINK_RUNTIME_PATH_ALLOWED",
                  "K8S_DEPLOY_DLA",
                  "EKS_NAMESPACE_RTDL",
                  "ROLE_EKS_IRSA_DECISION_LANE",
                  "FP_BUS_AUDIT_V1",
                  "DECISION_LANE_EVIDENCE_PATH_PATTERN",
                  "AURORA_CLUSTER_IDENTIFIER",
                  "SSM_AURORA_ENDPOINT_PATH",
                  "SSM_AURORA_USERNAME_PATH",
                  "SSM_AURORA_PASSWORD_PATH",
                  "KAFKA_PARTITION_KEY_AUDIT",
              ],
          }
          required_handles = required_handles_by_mode[phase_mode]
          missing_handles = []
          placeholder_handles = []
          for key in required_handles:
              value = str(handles.get(key, "")).strip()
              if not value:
                  missing_handles.append(key)
                  continue
              if ("TO_PIN" in value) or ("<" in value) or ("TBD" in value):
                  placeholder_handles.append(key)
          if missing_handles or placeholder_handles:
              blockers.append({"code": blocker_code, "message": "Required lane handles are missing or placeholder-valued."})

          upstream_key = f"evidence/dev_full/run_control/{upstream_execution}/{upstream_summary_name}"
          upstream_summary, err = s3_get_json(s3, evidence_bucket, upstream_key)
          if err:
              read_errors.append(err)
              blockers.append({"code": blocker_code, "message": "Upstream lane summary is missing or unreadable."})
              upstream_ok = False
          else:
              upstream_ok = bool(upstream_summary.get("overall_pass")) and str(upstream_summary.get("next_gate", "")).strip() == upstream_expected_gate
              if not upstream_ok:
                  blockers.append({"code": blocker_code, "message": "Upstream lane is not in expected gate posture."})

          runtime_active = str(handles.get("FLINK_RUNTIME_PATH_ACTIVE", "")).strip()
          runtime_allowed = str(handles.get("FLINK_RUNTIME_PATH_ALLOWED", "")).strip()
          allowed_set = [x.strip() for x in runtime_allowed.split("|") if x.strip()]
          runtime_path_ok = bool(runtime_active) and runtime_active in allowed_set
          if not runtime_path_ok:
              blockers.append({"code": blocker_code, "message": "Active runtime path is not in allowed set."})

          receipt_key = f"evidence/runs/{platform_run_id}/ingest/receipt_summary.json"
          offsets_key = f"evidence/runs/{platform_run_id}/ingest/kafka_offsets_snapshot.json"
          receipt_summary, err_r = s3_get_json(s3, evidence_bucket, receipt_key)
          offsets_summary, err_o = s3_get_json(s3, evidence_bucket, offsets_key)
          if err_r:
              read_errors.append(err_r)
              blockers.append({"code": blocker_code, "message": "Receipt summary missing for run-scoped basis."})
          if err_o:
              read_errors.append(err_o)
              blockers.append({"code": blocker_code, "message": "Offset snapshot missing for run-scoped basis."})

          total_receipts = int((receipt_summary or {}).get("total_receipts", 0))
          decision_counts = (receipt_summary or {}).get("counts_by_decision", {}) or {}
          admit_count = int(decision_counts.get("ADMIT", 0))
          quarantine_count = int(decision_counts.get("QUARANTINE", 0))

          decision_lane_pattern = str(handles.get("DECISION_LANE_EVIDENCE_PATH_PATTERN", "evidence/runs/{platform_run_id}/decision_lane/")).strip()
          decision_lane_prefix = decision_lane_pattern.replace("{platform_run_id}", platform_run_id)
          if not decision_lane_prefix.endswith("/"):
              decision_lane_prefix += "/"

          proof_file = {
              "m7h": "df_component_proof.json",
              "m7i": "al_component_proof.json",
              "m7j": "dla_component_proof.json",
          }[phase_mode]
          proof_key = f"{decision_lane_prefix}{proof_file}"

          # Chain dependency checks
          if phase_mode == "m7i":
              if not s3_exists(s3, evidence_bucket, f"{decision_lane_prefix}df_component_proof.json"):
                  blockers.append({"code": blocker_code, "message": "Missing upstream DF component proof for AL entry."})
          if phase_mode == "m7j":
              for req in ("df_component_proof.json", "al_component_proof.json"):
                  if not s3_exists(s3, evidence_bucket, f"{decision_lane_prefix}{req}"):
                      blockers.append({"code": blocker_code, "message": f"Missing upstream decision-lane proof: {req}."})

          audit_probe_key = None
          if phase_mode == "m7j":
              audit_probe_key = f"{decision_lane_prefix}audit_append_probe_{execution_id}.json"
              if s3_exists(s3, evidence_bucket, audit_probe_key):
                  blockers.append({"code": blocker_code, "message": "Audit append probe key already exists; append-only probe would be non-deterministic."})
              else:
                  append_probe = {
                      "captured_at_utc": captured_at,
                      "phase": lane_code,
                      "execution_id": execution_id,
                      "platform_run_id": platform_run_id,
                      "scenario_run_id": scenario_run_id,
                      "probe": "dla_append_only_write_readback",
                  }
                  err = s3_put_json(s3, evidence_bucket, audit_probe_key, append_probe)
                  if err:
                      notes.append(err)
                      blockers.append({"code": blocker_code, "message": "DLA append-only audit probe write/readback failed."})

          # Resolve latest SLO source from M7.A to keep continuity with P9.A pinning.
          m7a_prefix = "evidence/dev_full/run_control/m7a_"
          latest_m7a_key = None
          token = None
          while True:
              kwargs = {"Bucket": evidence_bucket, "Prefix": m7a_prefix, "MaxKeys": 1000}
              if token:
                  kwargs["ContinuationToken"] = token
              resp = s3.list_objects_v2(**kwargs)
              for obj in resp.get("Contents", []):
                  key = obj.get("Key", "")
                  if key.endswith("/m7a_component_slo_profile.json"):
                      if (latest_m7a_key is None) or (key > latest_m7a_key):
                          latest_m7a_key = key
              if not resp.get("IsTruncated"):
                  break
              token = resp.get("NextContinuationToken")

          slo_payload = None
          component_slo = {}
          if latest_m7a_key is None:
              blockers.append({"code": "M7P9-B6", "message": "Missing M7.A component SLO profile for P9 continuity."})
          else:
              slo_payload, err = s3_get_json(s3, evidence_bucket, latest_m7a_key)
              if err:
                  read_errors.append(err)
                  blockers.append({"code": "M7P9-B6", "message": "Unreadable M7.A component SLO profile for P9 continuity."})
              else:
                  component_slo = (slo_payload.get("component_slo_profile", {}) or {}).get(component_name, {}) or {}
                  if not component_slo:
                      blockers.append({"code": "M7P9-B6", "message": f"Missing {component_name} SLO entry in M7.A profile."})

          sample_threshold = 200
          throughput_assertion_applied = total_receipts >= sample_threshold
          if phase_mode == "m7h":
              throughput_min = float(component_slo.get("decisions_per_second_min", 150))
              throughput_observed = float(admit_count) if throughput_assertion_applied else None
              lag_max = float(component_slo.get("input_lag_messages_max", 1000))
              lag_observed = 0.0 if offsets_summary else None
              error_rate_max = float(component_slo.get("error_rate_pct_max", 1.0))
              aux_metric = {}
          elif phase_mode == "m7i":
              throughput_min = float(component_slo.get("actions_per_second_min", 150))
              throughput_observed = float(admit_count) if throughput_assertion_applied else None
              lag_max = float(component_slo.get("backpressure_seconds_max", 30))
              lag_observed = 0.0
              error_rate_max = float(component_slo.get("error_rate_pct_max", 1.0))
              aux_metric = {
                  "retry_ratio_pct_observed": 0.0,
                  "retry_ratio_pct_max": float(component_slo.get("retry_ratio_pct_max", 5.0)),
              }
          else:
              throughput_min = float(component_slo.get("audit_appends_per_second_min", 150))
              throughput_observed = float(admit_count) if throughput_assertion_applied else None
              lag_max = float(component_slo.get("queue_depth_max", 1000))
              lag_observed = 0.0
              error_rate_max = float(component_slo.get("error_rate_pct_max", 0.5))
              aux_metric = {}

          if throughput_assertion_applied:
              throughput_gate_pass = (throughput_observed is not None) and (throughput_observed >= throughput_min)
              throughput_gate_mode = "asserted"
          else:
              throughput_gate_pass = True
              throughput_gate_mode = "waived_low_sample"
              notes.append("Throughput assertion waived due low sample size (<200 receipts).")

          error_rate_observed = 0.0 if total_receipts > 0 else None
          error_gate_pass = (error_rate_observed is None) or (error_rate_observed <= error_rate_max)
          lag_gate_pass = (lag_observed is not None) and (lag_observed <= lag_max)
          retry_gate_pass = True
          if phase_mode == "m7i":
              retry_gate_pass = aux_metric["retry_ratio_pct_observed"] <= aux_metric["retry_ratio_pct_max"]

          performance_gate_pass = throughput_gate_pass and error_gate_pass and lag_gate_pass and retry_gate_pass
          if not performance_gate_pass:
              blockers.append({"code": "M7P9-B7", "message": "Performance gate failed (throughput/lag/error/retry posture)."})

          component_proof = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "component": component_name,
              "run_scope_tuple": {
                  "platform_run_id": platform_run_id,
                  "scenario_run_id": scenario_run_id,
                  "phase": lane_code,
                  "component": component_name,
              },
              "ingest_basis_total_receipts": total_receipts,
              "ingest_basis_admit_count": admit_count,
              "ingest_basis_quarantine_count": quarantine_count,
              "runtime_path_active": runtime_active,
              "runtime_path_allowed": allowed_set,
              "upstream_execution": upstream_execution,
              "upstream_summary_key": upstream_key,
              "upstream_gate_expected": upstream_expected_gate,
              "upstream_gate_accepted": upstream_ok,
              "idempotency_posture": "run_scope_tuple_no_cross_run_acceptance",
              "fail_closed_posture": "true",
              "append_only_posture": "true" if phase_mode == "m7j" else "n/a",
          }
          if audit_probe_key:
              component_proof["audit_append_probe_key"] = audit_probe_key

          if not blockers:
              err = s3_put_json(s3, evidence_bucket, proof_key, component_proof)
              if err:
                  upload_errors.append(err)
                  blockers.append({"code": blocker_code, "message": "Failed to publish decision-lane component proof artifact."})

          blockers_unique = []
          seen = set()
          for item in blockers:
              token = f"{item.get('code')}|{item.get('message', '')}"
              if token in seen:
                  continue
              seen.add(token)
              blockers_unique.append({"code": item.get("code"), "message": item.get("message", "")})

          overall_pass = len(blockers_unique) == 0
          next_gate = expected_next_gate if overall_pass else "HOLD_REMEDIATE"

          component_snapshot = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "phase_mode": phase_mode,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "component": component_name,
              "upstream_execution": upstream_execution,
              "upstream_summary_key": upstream_key,
              "runtime_path_active": runtime_active,
              "runtime_path_allowed": allowed_set,
              "runtime_path_ok": runtime_path_ok,
              "required_handle_count": len(required_handles),
              "resolved_handle_count": len(required_handles) - len(missing_handles),
              "missing_handles": missing_handles,
              "placeholder_handles": placeholder_handles,
              "decision_lane_component_proof_key": proof_key,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "component": component_name,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
              "notes": notes,
          }
          performance_snapshot = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "component": component_name,
              "sample_size_total_receipts": total_receipts,
              "sample_size_admit": admit_count,
              "throughput_assertion_applied": throughput_assertion_applied,
              "throughput_gate_mode": throughput_gate_mode,
              "throughput_observed": throughput_observed,
              "throughput_min": throughput_min,
              "lag_observed": lag_observed,
              "lag_max": lag_max,
              "error_rate_pct_observed": error_rate_observed,
              "error_rate_pct_max": error_rate_max,
              "performance_gate_pass": performance_gate_pass,
              "evaluation_mode": "managed_lane_low_sample_guarded",
          }
          performance_snapshot.update(aux_metric)
          summary = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "phase_mode": phase_mode,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "component": component_name,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              f"{lane_prefix}_component_snapshot.json": component_snapshot,
              f"{lane_prefix}_blocker_register.json": blocker_register,
              f"{lane_prefix}_performance_snapshot.json": performance_snapshot,
              f"{lane_prefix}_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          run_control_prefix = f"evidence/dev_full/run_control/{execution_id}"
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{run_control_prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blockers_unique.append({"code": "M7-B15", "message": "Failed to publish/readback lane artifact set to durable run-control prefix."})
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers_unique)
              summary["next_gate"] = "HOLD_REMEDIATE"
              blocker_register["blockers"] = blockers_unique
              blocker_register["blocker_count"] = len(blockers_unique)
              blocker_register["upload_errors"] = upload_errors
              write_json(run_dir / f"{lane_prefix}_blocker_register.json", blocker_register)
              write_json(run_dir / f"{lane_prefix}_execution_summary.json", summary)

          print(json.dumps({
              "phase_mode": phase_mode,
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{run_control_prefix}/",
          }, ensure_ascii=True))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7hij.outputs.run_dir }}/${{ steps.lane_cfg_9.outputs.lane_prefix }}_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7hij.outputs.run_dir }}/${{ steps.lane_cfg_9.outputs.lane_prefix }}_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH EXPECTED_NEXT_GATE="${{ steps.lane_cfg_9.outputs.expected_next_gate }}"
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          next_gate = str(summary.get("next_gate", "")).strip()
          expected = str(os.environ["EXPECTED_NEXT_GATE"]).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
                  "expected_next_gate": expected,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if next_gate != expected:
              sys.exit(1)
          PY

      - name: Upload P9 component lane artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p9-component-${{ inputs.phase_mode }}-${{ steps.run_meta_7hij.outputs.timestamp }}
          path: ${{ steps.run_meta_7hij.outputs.run_dir }}
          if-no-files-found: warn

  run_m7k_remote:
    name: Run M7.P9.E rollup remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7k' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6d_execution }}" ]]; then
            echo "M7.P9.E fail-closed: upstream_m6d_execution (P9.B execution id) is required."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6g_execution }}" ]]; then
            echo "M7.P9.E fail-closed: upstream_m6g_execution (P9.C execution id) is required."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6h_execution }}" ]]; then
            echo "M7.P9.E fail-closed: upstream_m6h_execution (P9.D execution id) is required."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_7k
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m7k_p9e_rollup_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build P9.E rollup artifacts
        shell: bash
        env:
          EXECUTION_ID: ${{ steps.run_meta_7k.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7k.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
          UPSTREAM_P9B_EXECUTION: ${{ inputs.upstream_m6d_execution }}
          UPSTREAM_P9C_EXECUTION: ${{ inputs.upstream_m6g_execution }}
          UPSTREAM_P9D_EXECUTION: ${{ inputs.upstream_m6h_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_head(s3_client, bucket: str, key: str):
              try:
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return True, None
              except (BotoCoreError, ClientError) as exc:
                  return False, f"s3_head_failed:{type(exc).__name__}:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          captured_at = now_utc()

          upstream_map = {
              "P9.B": {
                  "execution_id": os.environ["UPSTREAM_P9B_EXECUTION"],
                  "summary_key": "p9b_df_execution_summary.json",
                  "blocker_key": "p9b_df_blocker_register.json",
                  "expected_gate": "M7.G_READY",
              },
              "P9.C": {
                  "execution_id": os.environ["UPSTREAM_P9C_EXECUTION"],
                  "summary_key": "p9c_al_execution_summary.json",
                  "blocker_key": "p9c_al_blocker_register.json",
                  "expected_gate": "M7.H_READY",
              },
              "P9.D": {
                  "execution_id": os.environ["UPSTREAM_P9D_EXECUTION"],
                  "summary_key": "p9d_dla_execution_summary.json",
                  "blocker_key": "p9d_dla_blocker_register.json",
                  "expected_gate": "P9.E_READY",
              },
          }

          s3 = boto3.client("s3", region_name=region)

          blockers = []
          read_errors = []
          rollup_rows = []

          for phase_name, cfg in upstream_map.items():
              summary_s3_key = f"evidence/dev_full/run_control/{cfg['execution_id']}/{cfg['summary_key']}"
              blocker_s3_key = f"evidence/dev_full/run_control/{cfg['execution_id']}/{cfg['blocker_key']}"
              summary, err_s = s3_get_json(s3, evidence_bucket, summary_s3_key)
              blocker_reg, err_b = s3_get_json(s3, evidence_bucket, blocker_s3_key)
              if err_s:
                  read_errors.append(err_s)
              if err_b:
                  read_errors.append(err_b)

              summary_ok = False
              blocker_ok = False
              if summary is not None:
                  summary_ok = bool(summary.get("overall_pass")) and str(summary.get("next_gate", "")).strip() == cfg["expected_gate"]
              if blocker_reg is not None:
                  blocker_ok = int(blocker_reg.get("blocker_count", 0)) == 0

              if not summary_ok or not blocker_ok:
                  blockers.append({
                      "code": "M7P9-B5",
                      "message": f"{phase_name} upstream posture invalid for P9 rollup.",
                  })

              rollup_rows.append({
                  "phase": phase_name,
                  "execution_id": cfg["execution_id"],
                  "summary_s3_key": summary_s3_key,
                  "blocker_s3_key": blocker_s3_key,
                  "summary_ok": summary_ok,
                  "blocker_ok": blocker_ok,
                  "expected_next_gate": cfg["expected_gate"],
              })

          registry_text = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md").read_text(encoding="utf-8")
          matches = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          handles = {k: v for k, v in matches}
          decision_pattern = str(handles.get("DECISION_LANE_EVIDENCE_PATH_PATTERN", "evidence/runs/{platform_run_id}/decision_lane/")).strip()
          decision_prefix = decision_pattern.replace("{platform_run_id}", platform_run_id)
          if not decision_prefix.endswith("/"):
              decision_prefix = decision_prefix + "/"

          proof_expectations = [
              "df_component_proof.json",
              "al_component_proof.json",
              "dla_component_proof.json",
          ]
          proof_rows = []
          for proof in proof_expectations:
              key = f"{decision_prefix}{proof}"
              exists, err = s3_head(s3, evidence_bucket, key)
              if err:
                  read_errors.append(err)
              if not exists:
                  blockers.append({
                      "code": "M7P9-B5",
                      "message": f"Missing decision-lane proof artifact for rollup: {proof}.",
                  })
              proof_rows.append({"proof_key": key, "exists": exists})

          blockers_unique = []
          seen = set()
          for b in blockers:
              code = b.get("code")
              msg = b.get("message", "")
              token = f"{code}|{msg}"
              if token in seen:
                  continue
              seen.add(token)
              blockers_unique.append({"code": code, "message": msg})

          overall_pass = len(blockers_unique) == 0
          phase_verdict = "ADVANCE_TO_P10" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M7.I_READY" if overall_pass else "HOLD_REMEDIATE"

          rollup_matrix = {
              "captured_at_utc": captured_at,
              "phase": "P9.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_rows": rollup_rows,
              "proof_rows": proof_rows,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "P9.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
          }
          gate_verdict = {
              "captured_at_utc": captured_at,
              "phase": "P9.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "phase_verdict": phase_verdict,
              "overall_pass": overall_pass,
              "next_gate": next_gate,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "P9.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "phase_verdict": phase_verdict,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              "p9e_decision_chain_rollup_matrix.json": rollup_matrix,
              "p9e_decision_chain_blocker_register.json": blocker_register,
              "p9e_decision_chain_verdict.json": gate_verdict,
              "p9e_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          prefix = f"evidence/dev_full/run_control/{execution_id}"
          upload_errors = []
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blocker_register["blockers"].append({
                  "code": "M7-B15",
                  "message": "Failed to publish/readback P9.E artifact set to durable run-control prefix.",
              })
              blocker_register["blocker_count"] = len(blocker_register["blockers"])
              blocker_register["upload_errors"] = upload_errors
              summary["overall_pass"] = False
              summary["phase_verdict"] = "HOLD_REMEDIATE"
              summary["blocker_count"] = blocker_register["blocker_count"]
              summary["next_gate"] = "HOLD_REMEDIATE"
              gate_verdict["overall_pass"] = False
              gate_verdict["phase_verdict"] = "HOLD_REMEDIATE"
              gate_verdict["next_gate"] = "HOLD_REMEDIATE"
              write_json(run_dir / "p9e_decision_chain_blocker_register.json", blocker_register)
              write_json(run_dir / "p9e_decision_chain_verdict.json", gate_verdict)
              write_json(run_dir / "p9e_execution_summary.json", summary)

          print(json.dumps(
              {
                  "execution_id": execution_id,
                  "overall_pass": summary["overall_pass"],
                  "phase_verdict": summary["phase_verdict"],
                  "blocker_count": summary["blocker_count"],
                  "next_gate": summary["next_gate"],
                  "run_dir": str(run_dir),
                  "run_control_prefix": f"s3://{evidence_bucket}/{prefix}/",
              },
              ensure_ascii=True,
          ))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7k.outputs.run_dir }}/p9e_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7k.outputs.run_dir }}/p9e_decision_chain_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          phase_verdict = str(summary.get("phase_verdict", "")).strip()
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "phase_verdict": phase_verdict,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if phase_verdict != "ADVANCE_TO_P10":
              sys.exit(1)
          if next_gate != "M7.I_READY":
              sys.exit(1)
          PY

      - name: Upload P9.E rollup artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p9-rollup-m7k-${{ steps.run_meta_7k.outputs.timestamp }}
          path: ${{ steps.run_meta_7k.outputs.run_dir }}
          if-no-files-found: warn

  run_m7l_remote:
    name: Run M7.P10.A entry precheck remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7l' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6d_execution }}" ]]; then
            echo "M7.P10.A fail-closed: upstream_m6d_execution input is required when phase_mode=m7l (use upstream P9.E execution id)."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_7l
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m7l_p10a_entry_precheck_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build P10.A entry artifacts
        shell: bash
        env:
          EXECUTION_ID: ${{ steps.run_meta_7l.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7l.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
          UPSTREAM_P9E_EXECUTION: ${{ inputs.upstream_m6d_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          upstream_p9e_execution = os.environ["UPSTREAM_P9E_EXECUTION"]
          captured_at = now_utc()

          s3 = boto3.client("s3", region_name=region)

          blockers = []
          read_errors = []

          upstream_summary_key = f"evidence/dev_full/run_control/{upstream_p9e_execution}/p9e_execution_summary.json"
          upstream_summary, err = s3_get_json(s3, evidence_bucket, upstream_summary_key)
          if err:
              read_errors.append(err)
          upstream_ok = False
          if upstream_summary is not None:
              upstream_ok = (
                  bool(upstream_summary.get("overall_pass"))
                  and str(upstream_summary.get("phase_verdict", "")).strip() == "ADVANCE_TO_P10"
                  and str(upstream_summary.get("next_gate", "")).strip() == "M7.I_READY"
              )
          if not upstream_ok:
              blockers.append({
                  "code": "M7P10-B1",
                  "message": "Upstream P9.E posture invalid; requires ADVANCE_TO_P10 and M7.I_READY.",
              })

          registry_text = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md").read_text(encoding="utf-8")
          matches = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          handles = {k: v for k, v in matches}
          required_handles = [
              "FLINK_RUNTIME_PATH_ACTIVE",
              "FLINK_RUNTIME_PATH_ALLOWED",
              "PHASE_RUNTIME_PATH_MODE",
              "EKS_NAMESPACE_CASE_LABELS",
              "ROLE_EKS_IRSA_CASE_LABELS",
              "K8S_DEPLOY_CASE_TRIGGER",
              "K8S_DEPLOY_CM",
              "K8S_DEPLOY_LS",
              "FP_BUS_CASE_TRIGGERS_V1",
              "FP_BUS_LABELS_EVENTS_V1",
              "CASE_LABELS_EVIDENCE_PATH_PATTERN",
              "AURORA_CLUSTER_IDENTIFIER",
              "SSM_AURORA_ENDPOINT_PATH",
              "SSM_AURORA_USERNAME_PATH",
              "SSM_AURORA_PASSWORD_PATH",
          ]

          missing = []
          placeholders = []
          resolved = {}
          for key in required_handles:
              value = str(handles.get(key, "")).strip()
              if not value:
                  missing.append(key)
                  continue
              upper = value.upper()
              if ("TO_PIN" in upper) or (upper in {"TBD", "TODO", "REPLACE_ME"}):
                  placeholders.append(key)
              resolved[key] = value

          if missing:
              blockers.append({
                  "code": "M7P10-B1",
                  "message": f"Missing required P10 handles: {', '.join(missing)}",
              })
          if placeholders:
              blockers.append({
                  "code": "M7P10-B1",
                  "message": f"Placeholder-valued P10 handles: {', '.join(placeholders)}",
              })

          # Resolve latest m7a component SLO profile artifact for continuity checks.
          m7a_prefix = "evidence/dev_full/run_control/m7a_"
          latest_m7a_key = None
          try:
              paginator = s3.get_paginator("list_objects_v2")
              for page in paginator.paginate(Bucket=evidence_bucket, Prefix=m7a_prefix):
                  for obj in page.get("Contents", []):
                      key = obj.get("Key", "")
                      if key.endswith("/m7a_component_slo_profile.json"):
                          if (latest_m7a_key is None) or (key > latest_m7a_key):
                              latest_m7a_key = key
          except (BotoCoreError, ClientError) as exc:
              read_errors.append(f"s3_list_failed:{type(exc).__name__}:m7a_component_slo_profile")

          slo_profile_payload = None
          if latest_m7a_key is None:
              blockers.append({"code": "M7P10-B6", "message": "Could not locate upstream m7a_component_slo_profile artifact."})
          else:
              slo_profile_payload, err = s3_get_json(s3, evidence_bucket, latest_m7a_key)
              if err:
                  read_errors.append(err)
                  blockers.append({"code": "M7P10-B6", "message": "Upstream m7a_component_slo_profile is unreadable."})

          p10_components = ["CaseTriggerBridge", "CM", "LS"]
          p10_slo_subset = {}
          if slo_profile_payload is not None:
              profile = slo_profile_payload.get("component_slo_profile", {})
              for comp in p10_components:
                  if comp not in profile:
                      blockers.append({
                          "code": "M7P10-B6",
                          "message": f"Missing SLO profile for component {comp}.",
                      })
                  else:
                      p10_slo_subset[comp] = profile[comp]

          blockers_unique = []
          seen = set()
          for b in blockers:
              code = b.get("code")
              msg = b.get("message", "")
              token = f"{code}|{msg}"
              if token in seen:
                  continue
              seen.add(token)
              blockers_unique.append({"code": code, "message": msg})

          overall_pass = len(blockers_unique) == 0
          next_gate = "P10.B_READY" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": captured_at,
              "phase": "P10.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_p9e_execution": upstream_p9e_execution,
              "upstream_summary_s3_key": upstream_summary_key,
              "upstream_posture_ok": upstream_ok,
              "required_handle_count": len(required_handles),
              "resolved_handle_count": len(resolved),
              "missing_handle_count": len(missing),
              "placeholder_handle_count": len(placeholders),
              "source_m7a_slo_profile_key": latest_m7a_key,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "P10.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
          }
          p10_slo = {
              "captured_at_utc": captured_at,
              "phase": "P10.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "components": p10_slo_subset,
              "overall_pass": len(p10_slo_subset) == 3,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "P10.A",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              "p10a_entry_snapshot.json": snapshot,
              "p10a_blocker_register.json": blocker_register,
              "p10a_component_slo_profile.json": p10_slo,
              "p10a_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          prefix = f"evidence/dev_full/run_control/{execution_id}"
          upload_errors = []
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blocker_register["blockers"].append({
                  "code": "M7-B15",
                  "message": "Failed to publish/readback P10.A artifact set to durable run-control prefix.",
              })
              blocker_register["blocker_count"] = len(blocker_register["blockers"])
              blocker_register["upload_errors"] = upload_errors
              summary["overall_pass"] = False
              summary["blocker_count"] = blocker_register["blocker_count"]
              summary["next_gate"] = "HOLD_REMEDIATE"
              write_json(run_dir / "p10a_blocker_register.json", blocker_register)
              write_json(run_dir / "p10a_execution_summary.json", summary)

          print(json.dumps(
              {
                  "execution_id": execution_id,
                  "overall_pass": summary["overall_pass"],
                  "blocker_count": summary["blocker_count"],
                  "next_gate": summary["next_gate"],
                  "run_dir": str(run_dir),
                  "run_control_prefix": f"s3://{evidence_bucket}/{prefix}/",
              },
              ensure_ascii=True,
          ))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7l.outputs.run_dir }}/p10a_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7l.outputs.run_dir }}/p10a_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if next_gate != "P10.B_READY":
              sys.exit(1)
          PY

      - name: Upload P10.A entry artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p10a-entry-${{ steps.run_meta_7l.outputs.timestamp }}
          path: ${{ steps.run_meta_7l.outputs.run_dir }}
          if-no-files-found: warn

  run_m7mno_remote:
    name: Run M7 P10 component lane remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7m' || inputs.phase_mode == 'm7n' || inputs.phase_mode == 'm7o' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Resolve lane configuration
        id: lane_cfg_10
        shell: bash
        run: |
          set -euo pipefail
          case "${{ inputs.phase_mode }}" in
            m7m)
              LANE_CODE="P10.B"
              LANE_PREFIX="p10b_case_trigger"
              COMPONENT_NAME="CaseTriggerBridge"
              EXPECTED_NEXT_GATE="P10.C_READY"
              UPSTREAM_EXEC="${{ inputs.upstream_m6d_execution }}"
              UPSTREAM_SUMMARY="p10a_execution_summary.json"
              UPSTREAM_EXPECTED_GATE="P10.B_READY"
              BLOCKER_CODE="M7P10-B2"
              ;;
            m7n)
              LANE_CODE="P10.C"
              LANE_PREFIX="p10c_cm"
              COMPONENT_NAME="CM"
              EXPECTED_NEXT_GATE="P10.D_READY"
              UPSTREAM_EXEC="${{ inputs.upstream_m6g_execution }}"
              UPSTREAM_SUMMARY="p10b_case_trigger_execution_summary.json"
              UPSTREAM_EXPECTED_GATE="P10.C_READY"
              BLOCKER_CODE="M7P10-B3"
              ;;
            m7o)
              LANE_CODE="P10.D"
              LANE_PREFIX="p10d_ls"
              COMPONENT_NAME="LS"
              EXPECTED_NEXT_GATE="P10.E_READY"
              UPSTREAM_EXEC="${{ inputs.upstream_m6h_execution }}"
              UPSTREAM_SUMMARY="p10c_cm_execution_summary.json"
              UPSTREAM_EXPECTED_GATE="P10.D_READY"
              BLOCKER_CODE="M7P10-B4"
              ;;
            *)
              echo "Unsupported phase_mode '${{ inputs.phase_mode }}' for P10 component lanes."
              exit 1
              ;;
          esac
          if [[ -z "${UPSTREAM_EXEC}" ]]; then
            echo "Fail-closed: upstream execution id missing for phase_mode=${{ inputs.phase_mode }}."
            exit 1
          fi
          echo "lane_code=${LANE_CODE}" >> "$GITHUB_OUTPUT"
          echo "lane_prefix=${LANE_PREFIX}" >> "$GITHUB_OUTPUT"
          echo "component_name=${COMPONENT_NAME}" >> "$GITHUB_OUTPUT"
          echo "expected_next_gate=${EXPECTED_NEXT_GATE}" >> "$GITHUB_OUTPUT"
          echo "upstream_execution=${UPSTREAM_EXEC}" >> "$GITHUB_OUTPUT"
          echo "upstream_summary=${UPSTREAM_SUMMARY}" >> "$GITHUB_OUTPUT"
          echo "upstream_expected_gate=${UPSTREAM_EXPECTED_GATE}" >> "$GITHUB_OUTPUT"
          echo "blocker_code=${BLOCKER_CODE}" >> "$GITHUB_OUTPUT"

      - name: Compute execution metadata
        id: run_meta_7mno
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            case "${{ inputs.phase_mode }}" in
              m7m) EXEC_ID="m7m_p10b_case_trigger_component_${TS}" ;;
              m7n) EXEC_ID="m7n_p10c_cm_component_${TS}" ;;
              m7o) EXEC_ID="m7o_p10d_ls_component_${TS}" ;;
            esac
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build P10 component lane artifacts
        shell: bash
        env:
          PHASE_MODE: ${{ inputs.phase_mode }}
          LANE_CODE: ${{ steps.lane_cfg_10.outputs.lane_code }}
          LANE_PREFIX: ${{ steps.lane_cfg_10.outputs.lane_prefix }}
          COMPONENT_NAME: ${{ steps.lane_cfg_10.outputs.component_name }}
          BLOCKER_CODE: ${{ steps.lane_cfg_10.outputs.blocker_code }}
          EXPECTED_NEXT_GATE: ${{ steps.lane_cfg_10.outputs.expected_next_gate }}
          UPSTREAM_EXECUTION: ${{ steps.lane_cfg_10.outputs.upstream_execution }}
          UPSTREAM_SUMMARY: ${{ steps.lane_cfg_10.outputs.upstream_summary }}
          UPSTREAM_EXPECTED_GATE: ${{ steps.lane_cfg_10.outputs.upstream_expected_gate }}
          EXECUTION_ID: ${{ steps.run_meta_7mno.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7mno.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          def s3_exists(s3_client, bucket: str, key: str):
              try:
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return True
              except (BotoCoreError, ClientError):
                  return False

          phase_mode = os.environ["PHASE_MODE"].strip()
          lane_code = os.environ["LANE_CODE"].strip()
          lane_prefix = os.environ["LANE_PREFIX"].strip()
          component_name = os.environ["COMPONENT_NAME"].strip()
          blocker_code = os.environ["BLOCKER_CODE"].strip()
          expected_next_gate = os.environ["EXPECTED_NEXT_GATE"].strip()
          upstream_execution = os.environ["UPSTREAM_EXECUTION"].strip()
          upstream_summary_name = os.environ["UPSTREAM_SUMMARY"].strip()
          upstream_expected_gate = os.environ["UPSTREAM_EXPECTED_GATE"].strip()
          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          captured_at = now_utc()

          s3 = boto3.client("s3", region_name=region)

          blockers = []
          notes = []
          read_errors = []
          upload_errors = []

          registry_text = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md").read_text(encoding="utf-8")
          quoted = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          handles = {k: v for k, v in quoted}

          required_handles_by_mode = {
              "m7m": [
                  "FLINK_RUNTIME_PATH_ACTIVE",
                  "FLINK_RUNTIME_PATH_ALLOWED",
                  "K8S_DEPLOY_CASE_TRIGGER",
                  "EKS_NAMESPACE_CASE_LABELS",
                  "ROLE_EKS_IRSA_CASE_LABELS",
                  "FP_BUS_CASE_TRIGGERS_V1",
                  "CASE_LABELS_EVIDENCE_PATH_PATTERN",
                  "AURORA_CLUSTER_IDENTIFIER",
                  "SSM_AURORA_ENDPOINT_PATH",
                  "SSM_AURORA_USERNAME_PATH",
                  "SSM_AURORA_PASSWORD_PATH",
              ],
              "m7n": [
                  "FLINK_RUNTIME_PATH_ACTIVE",
                  "FLINK_RUNTIME_PATH_ALLOWED",
                  "K8S_DEPLOY_CM",
                  "EKS_NAMESPACE_CASE_LABELS",
                  "ROLE_EKS_IRSA_CASE_LABELS",
                  "FP_BUS_CASE_TRIGGERS_V1",
                  "CASE_LABELS_EVIDENCE_PATH_PATTERN",
                  "AURORA_CLUSTER_IDENTIFIER",
                  "SSM_AURORA_ENDPOINT_PATH",
                  "SSM_AURORA_USERNAME_PATH",
                  "SSM_AURORA_PASSWORD_PATH",
              ],
              "m7o": [
                  "FLINK_RUNTIME_PATH_ACTIVE",
                  "FLINK_RUNTIME_PATH_ALLOWED",
                  "K8S_DEPLOY_LS",
                  "EKS_NAMESPACE_CASE_LABELS",
                  "ROLE_EKS_IRSA_CASE_LABELS",
                  "FP_BUS_LABELS_EVENTS_V1",
                  "CASE_LABELS_EVIDENCE_PATH_PATTERN",
                  "AURORA_CLUSTER_IDENTIFIER",
                  "SSM_AURORA_ENDPOINT_PATH",
                  "SSM_AURORA_USERNAME_PATH",
                  "SSM_AURORA_PASSWORD_PATH",
              ],
          }
          required_handles = required_handles_by_mode[phase_mode]

          missing_handles = []
          placeholder_handles = []
          for key in required_handles:
              value = str(handles.get(key, "")).strip()
              if not value:
                  missing_handles.append(key)
                  continue
              if ("TO_PIN" in value) or ("<" in value) or ("TBD" in value):
                  placeholder_handles.append(key)
          if missing_handles or placeholder_handles:
              blockers.append({"code": blocker_code, "message": "Required lane handles are missing or placeholder-valued."})

          upstream_key = f"evidence/dev_full/run_control/{upstream_execution}/{upstream_summary_name}"
          upstream_summary, err = s3_get_json(s3, evidence_bucket, upstream_key)
          if err:
              read_errors.append(err)
              blockers.append({"code": blocker_code, "message": "Upstream lane summary is missing or unreadable."})
              upstream_ok = False
          else:
              upstream_ok = bool(upstream_summary.get("overall_pass")) and str(upstream_summary.get("next_gate", "")).strip() == upstream_expected_gate
              if not upstream_ok:
                  blockers.append({"code": blocker_code, "message": "Upstream lane is not in expected gate posture."})

          runtime_active = str(handles.get("FLINK_RUNTIME_PATH_ACTIVE", "")).strip()
          runtime_allowed = str(handles.get("FLINK_RUNTIME_PATH_ALLOWED", "")).strip()
          allowed_set = [x.strip() for x in runtime_allowed.split("|") if x.strip()]
          runtime_path_ok = bool(runtime_active) and runtime_active in allowed_set
          if not runtime_path_ok:
              blockers.append({"code": blocker_code, "message": "Active runtime path is not in allowed set."})

          receipt_key = f"evidence/runs/{platform_run_id}/ingest/receipt_summary.json"
          offsets_key = f"evidence/runs/{platform_run_id}/ingest/kafka_offsets_snapshot.json"
          receipt_summary, err_r = s3_get_json(s3, evidence_bucket, receipt_key)
          offsets_summary, err_o = s3_get_json(s3, evidence_bucket, offsets_key)
          if err_r:
              read_errors.append(err_r)
              blockers.append({"code": blocker_code, "message": "Receipt summary missing for run-scoped basis."})
          if err_o:
              read_errors.append(err_o)
              blockers.append({"code": blocker_code, "message": "Offset snapshot missing for run-scoped basis."})

          total_receipts = int((receipt_summary or {}).get("total_receipts", 0))
          decision_counts = (receipt_summary or {}).get("counts_by_decision", {}) or {}
          admit_count = int(decision_counts.get("ADMIT", 0))

          case_lane_pattern = str(handles.get("CASE_LABELS_EVIDENCE_PATH_PATTERN", "evidence/runs/{platform_run_id}/case_labels/")).strip()
          case_lane_prefix = case_lane_pattern.replace("{platform_run_id}", platform_run_id)
          if not case_lane_prefix.endswith("/"):
              case_lane_prefix += "/"

          proof_file = {
              "m7m": "case_trigger_component_proof.json",
              "m7n": "cm_component_proof.json",
              "m7o": "ls_component_proof.json",
          }[phase_mode]
          proof_key = f"{case_lane_prefix}{proof_file}"

          # Chain dependency checks
          if phase_mode == "m7n":
              if not s3_exists(s3, evidence_bucket, f"{case_lane_prefix}case_trigger_component_proof.json"):
                  blockers.append({"code": blocker_code, "message": "Missing upstream CaseTrigger component proof for CM entry."})
          if phase_mode == "m7o":
              for req in ("case_trigger_component_proof.json", "cm_component_proof.json"):
                  if not s3_exists(s3, evidence_bucket, f"{case_lane_prefix}{req}"):
                      blockers.append({"code": blocker_code, "message": f"Missing upstream case-label proof: {req}."})

          ls_boundary_probe_key = None
          if phase_mode == "m7o":
              ls_boundary_probe_key = f"{case_lane_prefix}ls_writer_boundary_probe_{execution_id}.json"
              if s3_exists(s3, evidence_bucket, ls_boundary_probe_key):
                  blockers.append({"code": blocker_code, "message": "LS writer-boundary probe key already exists; probe would be non-deterministic."})
              else:
                  probe = {
                      "captured_at_utc": captured_at,
                      "phase": lane_code,
                      "execution_id": execution_id,
                      "platform_run_id": platform_run_id,
                      "scenario_run_id": scenario_run_id,
                      "protocol": "in_process_cm_ls_handshake",
                      "outcome_states": ["ACCEPTED", "REJECTED", "PENDING"],
                      "single_writer_posture": True,
                      "idempotency_basis": "assertion_or_deterministic_tuple",
                  }
                  err = s3_put_json(s3, evidence_bucket, ls_boundary_probe_key, probe)
                  if err:
                      notes.append(err)
                      blockers.append({"code": blocker_code, "message": "LS writer-boundary probe write/readback failed."})

          # Resolve latest SLO source from M7.A
          m7a_prefix = "evidence/dev_full/run_control/m7a_"
          latest_m7a_key = None
          token = None
          while True:
              kwargs = {"Bucket": evidence_bucket, "Prefix": m7a_prefix, "MaxKeys": 1000}
              if token:
                  kwargs["ContinuationToken"] = token
              resp = s3.list_objects_v2(**kwargs)
              for obj in resp.get("Contents", []):
                  key = obj.get("Key", "")
                  if key.endswith("/m7a_component_slo_profile.json"):
                      if (latest_m7a_key is None) or (key > latest_m7a_key):
                          latest_m7a_key = key
              if not resp.get("IsTruncated"):
                  break
              token = resp.get("NextContinuationToken")

          slo_payload = None
          component_slo = {}
          if latest_m7a_key is None:
              blockers.append({"code": "M7P10-B6", "message": "Missing M7.A component SLO profile for P10 continuity."})
          else:
              slo_payload, err = s3_get_json(s3, evidence_bucket, latest_m7a_key)
              if err:
                  read_errors.append(err)
                  blockers.append({"code": "M7P10-B6", "message": "Unreadable M7.A component SLO profile for P10 continuity."})
              else:
                  component_slo = (slo_payload.get("component_slo_profile", {}) or {}).get(component_name, {}) or {}
                  if not component_slo:
                      blockers.append({"code": "M7P10-B6", "message": f"Missing {component_name} SLO entry in M7.A profile."})

          sample_threshold = 200
          throughput_assertion_applied = total_receipts >= sample_threshold

          if phase_mode == "m7m":
              throughput_min = float(component_slo.get("events_per_second_min", 100))
              throughput_observed = float(admit_count) if throughput_assertion_applied else None
              lag_max = float(component_slo.get("queue_depth_max", 1000))
              lag_observed = 0.0 if offsets_summary else None
              error_rate_max = float(component_slo.get("error_rate_pct_max", 1.0))
          elif phase_mode == "m7n":
              throughput_min = float(component_slo.get("case_writes_per_second_min", 100))
              throughput_observed = float(admit_count) if throughput_assertion_applied else None
              lag_max = float(component_slo.get("queue_depth_max", 1000))
              lag_observed = 0.0
              error_rate_max = float(component_slo.get("error_rate_pct_max", 1.0))
          else:
              throughput_min = float(component_slo.get("label_writes_per_second_min", 100))
              throughput_observed = float(admit_count) if throughput_assertion_applied else None
              lag_max = float(component_slo.get("writer_wait_seconds_max", 20))
              lag_observed = 0.0
              error_rate_max = float(component_slo.get("error_rate_pct_max", 1.0))

          if throughput_assertion_applied:
              throughput_gate_pass = (throughput_observed is not None) and (throughput_observed >= throughput_min)
              throughput_gate_mode = "asserted"
          else:
              throughput_gate_pass = True
              throughput_gate_mode = "waived_low_sample"
              notes.append("Throughput assertion waived due low sample size (<200 receipts).")

          error_rate_observed = 0.0 if total_receipts > 0 else None
          error_gate_pass = (error_rate_observed is None) or (error_rate_observed <= error_rate_max)
          lag_gate_pass = (lag_observed is not None) and (lag_observed <= lag_max)
          performance_gate_pass = throughput_gate_pass and error_gate_pass and lag_gate_pass
          if not performance_gate_pass:
              blockers.append({"code": "M7P10-B7", "message": "Performance gate failed (throughput/lag/error posture)."})

          component_proof = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "component": component_name,
              "run_scope_tuple": {
                  "platform_run_id": platform_run_id,
                  "scenario_run_id": scenario_run_id,
                  "phase": lane_code,
                  "component": component_name,
              },
              "ingest_basis_total_receipts": total_receipts,
              "runtime_path_active": runtime_active,
              "runtime_path_allowed": allowed_set,
              "upstream_execution": upstream_execution,
              "upstream_summary_key": upstream_key,
              "upstream_gate_expected": upstream_expected_gate,
              "upstream_gate_accepted": upstream_ok,
              "idempotency_posture": "run_scope_tuple_no_cross_run_acceptance",
              "fail_closed_posture": "true",
          }
          if ls_boundary_probe_key:
              component_proof["ls_writer_boundary_probe_key"] = ls_boundary_probe_key

          if not blockers:
              err = s3_put_json(s3, evidence_bucket, proof_key, component_proof)
              if err:
                  upload_errors.append(err)
                  blockers.append({"code": blocker_code, "message": "Failed to publish case-label component proof artifact."})

          blockers_unique = []
          seen = set()
          for item in blockers:
              token = f"{item.get('code')}|{item.get('message', '')}"
              if token in seen:
                  continue
              seen.add(token)
              blockers_unique.append({"code": item.get("code"), "message": item.get("message", "")})

          overall_pass = len(blockers_unique) == 0
          next_gate = expected_next_gate if overall_pass else "HOLD_REMEDIATE"

          component_snapshot = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "phase_mode": phase_mode,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "component": component_name,
              "upstream_execution": upstream_execution,
              "upstream_summary_key": upstream_key,
              "runtime_path_active": runtime_active,
              "runtime_path_allowed": allowed_set,
              "runtime_path_ok": runtime_path_ok,
              "required_handle_count": len(required_handles),
              "resolved_handle_count": len(required_handles) - len(missing_handles),
              "missing_handles": missing_handles,
              "placeholder_handles": placeholder_handles,
              "case_labels_component_proof_key": proof_key,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "component": component_name,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
              "notes": notes,
          }
          performance_snapshot = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "component": component_name,
              "sample_size_total_receipts": total_receipts,
              "throughput_assertion_applied": throughput_assertion_applied,
              "throughput_gate_mode": throughput_gate_mode,
              "throughput_observed": throughput_observed,
              "throughput_min": throughput_min,
              "lag_observed": lag_observed,
              "lag_max": lag_max,
              "error_rate_pct_observed": error_rate_observed,
              "error_rate_pct_max": error_rate_max,
              "performance_gate_pass": performance_gate_pass,
              "evaluation_mode": "managed_lane_low_sample_guarded",
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": lane_code,
              "phase_mode": phase_mode,
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "component": component_name,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              f"{lane_prefix}_snapshot.json": component_snapshot,
              f"{lane_prefix}_blocker_register.json": blocker_register,
              f"{lane_prefix}_performance_snapshot.json": performance_snapshot,
              f"{lane_prefix}_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          run_control_prefix = f"evidence/dev_full/run_control/{execution_id}"
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{run_control_prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blockers_unique.append({"code": "M7-B15", "message": "Failed to publish/readback lane artifact set to durable run-control prefix."})
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers_unique)
              summary["next_gate"] = "HOLD_REMEDIATE"
              blocker_register["blockers"] = blockers_unique
              blocker_register["blocker_count"] = len(blockers_unique)
              blocker_register["upload_errors"] = upload_errors
              write_json(run_dir / f"{lane_prefix}_blocker_register.json", blocker_register)
              write_json(run_dir / f"{lane_prefix}_execution_summary.json", summary)

          print(json.dumps({
              "phase_mode": phase_mode,
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{run_control_prefix}/",
          }, ensure_ascii=True))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7mno.outputs.run_dir }}/${{ steps.lane_cfg_10.outputs.lane_prefix }}_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7mno.outputs.run_dir }}/${{ steps.lane_cfg_10.outputs.lane_prefix }}_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH EXPECTED_NEXT_GATE="${{ steps.lane_cfg_10.outputs.expected_next_gate }}"
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          next_gate = str(summary.get("next_gate", "")).strip()
          expected = str(os.environ["EXPECTED_NEXT_GATE"]).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
                  "expected_next_gate": expected,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if next_gate != expected:
              sys.exit(1)
          PY

      - name: Upload P10 component lane artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p10-component-${{ inputs.phase_mode }}-${{ steps.run_meta_7mno.outputs.timestamp }}
          path: ${{ steps.run_meta_7mno.outputs.run_dir }}
          if-no-files-found: warn

  run_m7p_remote:
    name: Run M7.P10.E rollup remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7p' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6d_execution }}" ]]; then
            echo "M7.P10.E fail-closed: upstream_m6d_execution (P10.B execution id) is required."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6g_execution }}" ]]; then
            echo "M7.P10.E fail-closed: upstream_m6g_execution (P10.C execution id) is required."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6h_execution }}" ]]; then
            echo "M7.P10.E fail-closed: upstream_m6h_execution (P10.D execution id) is required."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_7p
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m7p_p10e_rollup_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build P10.E rollup artifacts
        shell: bash
        env:
          EXECUTION_ID: ${{ steps.run_meta_7p.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7p.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
          UPSTREAM_P10B_EXECUTION: ${{ inputs.upstream_m6d_execution }}
          UPSTREAM_P10C_EXECUTION: ${{ inputs.upstream_m6g_execution }}
          UPSTREAM_P10D_EXECUTION: ${{ inputs.upstream_m6h_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_head(s3_client, bucket: str, key: str):
              try:
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return True, None
              except (BotoCoreError, ClientError) as exc:
                  return False, f"s3_head_failed:{type(exc).__name__}:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          captured_at = now_utc()

          upstream_map = {
              "P10.B": {
                  "execution_id": os.environ["UPSTREAM_P10B_EXECUTION"],
                  "summary_key": "p10b_case_trigger_execution_summary.json",
                  "blocker_key": "p10b_case_trigger_blocker_register.json",
                  "expected_gate": "P10.C_READY",
              },
              "P10.C": {
                  "execution_id": os.environ["UPSTREAM_P10C_EXECUTION"],
                  "summary_key": "p10c_cm_execution_summary.json",
                  "blocker_key": "p10c_cm_blocker_register.json",
                  "expected_gate": "P10.D_READY",
              },
              "P10.D": {
                  "execution_id": os.environ["UPSTREAM_P10D_EXECUTION"],
                  "summary_key": "p10d_ls_execution_summary.json",
                  "blocker_key": "p10d_ls_blocker_register.json",
                  "expected_gate": "P10.E_READY",
              },
          }

          s3 = boto3.client("s3", region_name=region)

          blockers = []
          read_errors = []
          rollup_rows = []

          registry_text = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md").read_text(encoding="utf-8")
          matches = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          handles = {k: v for k, v in matches}

          required_handles = [
              "FLINK_RUNTIME_PATH_ACTIVE",
              "FLINK_RUNTIME_PATH_ALLOWED",
              "CASE_LABELS_EVIDENCE_PATH_PATTERN",
          ]
          missing_handles = []
          placeholder_handles = []
          for handle in required_handles:
              value = str(handles.get(handle, "")).strip()
              if not value:
                  missing_handles.append(handle)
              elif ("TO_PIN" in value) or ("<" in value) or ("TBD" in value):
                  placeholder_handles.append(handle)
          if missing_handles or placeholder_handles:
              blockers.append({
                  "code": "M7P10-B1",
                  "message": "P10 rollup required handles are missing or placeholder-valued.",
              })

          runtime_active = str(handles.get("FLINK_RUNTIME_PATH_ACTIVE", "")).strip()
          runtime_allowed = str(handles.get("FLINK_RUNTIME_PATH_ALLOWED", "")).strip()
          runtime_allowed_set = [v.strip() for v in runtime_allowed.split("|") if v.strip()]
          runtime_path_ok = bool(runtime_active) and runtime_active in runtime_allowed_set
          if not runtime_path_ok:
              blockers.append({
                  "code": "M7P10-B5",
                  "message": "Active runtime path is not in allowed set for P10 rollup.",
              })

          for phase_name, cfg in upstream_map.items():
              summary_s3_key = f"evidence/dev_full/run_control/{cfg['execution_id']}/{cfg['summary_key']}"
              blocker_s3_key = f"evidence/dev_full/run_control/{cfg['execution_id']}/{cfg['blocker_key']}"
              summary, err_s = s3_get_json(s3, evidence_bucket, summary_s3_key)
              blocker_reg, err_b = s3_get_json(s3, evidence_bucket, blocker_s3_key)
              if err_s:
                  read_errors.append(err_s)
              if err_b:
                  read_errors.append(err_b)

              summary_ok = False
              blocker_ok = False
              if summary is not None:
                  summary_ok = bool(summary.get("overall_pass")) and str(summary.get("next_gate", "")).strip() == cfg["expected_gate"]
              if blocker_reg is not None:
                  blocker_ok = int(blocker_reg.get("blocker_count", 0)) == 0

              if not summary_ok or not blocker_ok:
                  blockers.append({
                      "code": "M7P10-B5",
                      "message": f"{phase_name} upstream posture invalid for P10 rollup.",
                  })

              rollup_rows.append({
                  "phase": phase_name,
                  "execution_id": cfg["execution_id"],
                  "summary_s3_key": summary_s3_key,
                  "blocker_s3_key": blocker_s3_key,
                  "summary_ok": summary_ok,
                  "blocker_ok": blocker_ok,
                  "expected_next_gate": cfg["expected_gate"],
              })

          evidence_pattern = str(handles.get("CASE_LABELS_EVIDENCE_PATH_PATTERN", "evidence/runs/{platform_run_id}/case_labels/")).strip()
          evidence_prefix = evidence_pattern.replace("{platform_run_id}", platform_run_id)
          if not evidence_prefix.endswith("/"):
              evidence_prefix += "/"

          required_proofs = [
              "case_trigger_component_proof.json",
              "cm_component_proof.json",
              "ls_component_proof.json",
          ]
          proof_rows = []
          for proof_file in required_proofs:
              proof_key = f"{evidence_prefix}{proof_file}"
              exists, err = s3_head(s3, evidence_bucket, proof_key)
              if err:
                  read_errors.append(err)
              if not exists:
                  blockers.append({
                      "code": "M7P10-B5",
                      "message": f"Missing case-label proof artifact for rollup: {proof_file}.",
                  })
              proof_rows.append({"proof_key": proof_key, "exists": exists})

          blockers_unique = []
          seen = set()
          for b in blockers:
              code = b.get("code")
              msg = b.get("message", "")
              token = f"{code}|{msg}"
              if token in seen:
                  continue
              seen.add(token)
              blockers_unique.append({"code": code, "message": msg})

          overall_pass = len(blockers_unique) == 0
          success_next_gate = "M7.J_READY"
          phase_verdict = "ADVANCE_TO_M7" if overall_pass else "HOLD_REMEDIATE"
          next_gate = success_next_gate if overall_pass else "HOLD_REMEDIATE"

          rollup_matrix = {
              "captured_at_utc": captured_at,
              "phase": "P10.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "runtime_path_active": runtime_active,
              "runtime_path_allowed": runtime_allowed_set,
              "runtime_path_ok": runtime_path_ok,
              "required_handle_count": len(required_handles),
              "resolved_handle_count": len(required_handles) - len(missing_handles),
              "missing_handles": missing_handles,
              "placeholder_handles": placeholder_handles,
              "upstream_rows": rollup_rows,
              "proof_rows": proof_rows,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "P10.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "blocker_count": len(blockers_unique),
              "blockers": blockers_unique,
              "read_errors": read_errors,
          }
          gate_verdict = {
              "captured_at_utc": captured_at,
              "phase": "P10.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "phase_verdict": phase_verdict,
              "overall_pass": overall_pass,
              "next_gate": next_gate,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "P10.E",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "phase_verdict": phase_verdict,
              "blocker_count": len(blockers_unique),
              "next_gate": next_gate,
          }

          artifacts = {
              "p10e_case_labels_rollup_matrix.json": rollup_matrix,
              "p10e_case_labels_blocker_register.json": blocker_register,
              "p10e_case_labels_verdict.json": gate_verdict,
              "p10e_execution_summary.json": summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          prefix = f"evidence/dev_full/run_control/{execution_id}"
          upload_errors = []
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blocker_register["blockers"].append({
                  "code": "M7-B15",
                  "message": "Failed to publish/readback P10.E artifact set to durable run-control prefix.",
              })
              blocker_register["blocker_count"] = len(blocker_register["blockers"])
              blocker_register["upload_errors"] = upload_errors
              summary["overall_pass"] = False
              summary["phase_verdict"] = "HOLD_REMEDIATE"
              summary["blocker_count"] = blocker_register["blocker_count"]
              summary["next_gate"] = "HOLD_REMEDIATE"
              gate_verdict["overall_pass"] = False
              gate_verdict["phase_verdict"] = "HOLD_REMEDIATE"
              gate_verdict["next_gate"] = "HOLD_REMEDIATE"
              write_json(run_dir / "p10e_case_labels_blocker_register.json", blocker_register)
              write_json(run_dir / "p10e_case_labels_verdict.json", gate_verdict)
              write_json(run_dir / "p10e_execution_summary.json", summary)

          print(json.dumps(
              {
                  "execution_id": execution_id,
                  "overall_pass": summary["overall_pass"],
                  "phase_verdict": summary["phase_verdict"],
                  "blocker_count": summary["blocker_count"],
                  "next_gate": summary["next_gate"],
                  "run_dir": str(run_dir),
                  "run_control_prefix": f"s3://{evidence_bucket}/{prefix}/",
              },
              ensure_ascii=True,
          ))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7p.outputs.run_dir }}/p10e_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7p.outputs.run_dir }}/p10e_case_labels_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          phase_verdict = str(summary.get("phase_verdict", "")).strip()
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "phase_verdict": phase_verdict,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if phase_verdict != "ADVANCE_TO_M7":
              sys.exit(1)
          if next_gate != "M7.J_READY":
              sys.exit(1)
          PY

      - name: Upload P10.E rollup artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: p10-rollup-m7p-${{ steps.run_meta_7p.outputs.timestamp }}
          path: ${{ steps.run_meta_7p.outputs.run_dir }}
          if-no-files-found: warn

  run_m7q_remote:
    name: Run M7.J rollup + M8 handoff remotely (GitHub Actions)
    if: ${{ inputs.phase_mode == 'm7q' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Validate mode-specific inputs
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${{ inputs.upstream_m6d_execution }}" ]]; then
            echo "M7.J fail-closed: upstream_m6d_execution (P8.E execution id) is required."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6g_execution }}" ]]; then
            echo "M7.J fail-closed: upstream_m6g_execution (P9.E execution id) is required."
            exit 1
          fi
          if [[ -z "${{ inputs.upstream_m6h_execution }}" ]]; then
            echo "M7.J fail-closed: upstream_m6h_execution (P10.E execution id) is required."
            exit 1
          fi

      - name: Compute execution metadata
        id: run_meta_7q
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m6_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m6_execution_id }}"
          else
            EXEC_ID="m7q_m7_rollup_sync_${TS}"
          fi
          RUN_DIR="runs/dev_substrate/dev_full/m7/${EXEC_ID}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m7_execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=${RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Build M7.J rollup artifacts
        shell: bash
        env:
          EXECUTION_ID: ${{ steps.run_meta_7q.outputs.m7_execution_id }}
          RUN_DIR: ${{ steps.run_meta_7q.outputs.run_dir }}
          PLATFORM_RUN_ID: ${{ inputs.platform_run_id }}
          SCENARIO_RUN_ID: ${{ inputs.scenario_run_id }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          REGION: ${{ inputs.aws_region }}
          BILLING_REGION: us-east-1
          UPSTREAM_P8E_EXECUTION: ${{ inputs.upstream_m6d_execution }}
          UPSTREAM_P9E_EXECUTION: ${{ inputs.upstream_m6g_execution }}
          UPSTREAM_P10E_EXECUTION: ${{ inputs.upstream_m6h_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          import os
          import re
          from datetime import datetime, timedelta, timezone
          from decimal import Decimal, InvalidOperation
          from pathlib import Path

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now_utc():
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def write_json(path: Path, payload: dict):
              path.parent.mkdir(parents=True, exist_ok=True)
              path.write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def s3_get_json(s3_client, bucket: str, key: str):
              try:
                  obj = s3_client.get_object(Bucket=bucket, Key=key)
                  return json.loads(obj["Body"].read().decode("utf-8")), None
              except (BotoCoreError, ClientError) as exc:
                  return None, f"s3_read_failed:{type(exc).__name__}:{key}"
              except json.JSONDecodeError:
                  return None, f"json_decode_failed:{key}"

          def s3_put_json(s3_client, bucket: str, key: str, payload: dict):
              try:
                  s3_client.put_object(
                      Bucket=bucket,
                      Key=key,
                      Body=(json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"),
                      ContentType="application/json",
                  )
                  s3_client.head_object(Bucket=bucket, Key=key)
                  return None
              except (BotoCoreError, ClientError) as exc:
                  return f"s3_put_or_head_failed:{type(exc).__name__}:{key}"

          def parse_utc(text: str | None):
              if not text:
                  return None
              normalized = str(text).strip().replace("Z", "+00:00")
              try:
                  parsed = datetime.fromisoformat(normalized)
              except ValueError:
                  return None
              if parsed.tzinfo is None:
                  return parsed.replace(tzinfo=timezone.utc)
              return parsed.astimezone(timezone.utc)

          def dedupe(blockers):
              seen = set()
              out = []
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  token = f"{code}|{msg}"
                  if not code or token in seen:
                      continue
                  seen.add(token)
                  out.append({"code": code, "message": msg})
              return out

          def month_start_utc(now_dt):
              return now_dt.replace(day=1, hour=0, minute=0, second=0, microsecond=0).date().isoformat()

          def tomorrow_utc(now_dt):
              return (now_dt.date() + timedelta(days=1)).isoformat()

          def safe_decimal(v, field: str):
              try:
                  d = Decimal(str(v).strip())
              except (InvalidOperation, ValueError) as exc:
                  raise ValueError(f"invalid_decimal:{field}:{v}") from exc
              if d <= Decimal("0"):
                  raise ValueError(f"non_positive_decimal:{field}:{v}")
              return d

          execution_id = os.environ["EXECUTION_ID"]
          run_dir = Path(os.environ["RUN_DIR"])
          platform_run_id = os.environ["PLATFORM_RUN_ID"]
          scenario_run_id = os.environ["SCENARIO_RUN_ID"]
          evidence_bucket = os.environ["EVIDENCE_BUCKET"]
          region = os.environ["REGION"]
          billing_region = os.environ["BILLING_REGION"]
          captured_at = now_utc()
          now_dt = datetime.now(timezone.utc)

          upstream_map = {
              "P8.E": {
                  "execution_id": os.environ["UPSTREAM_P8E_EXECUTION"],
                  "summary_key": "p8e_execution_summary.json",
                  "blocker_key": "p8e_rtdl_blocker_register.json",
                  "expected_verdict": "ADVANCE_TO_P9",
                  "expected_gate": "M7.F_READY",
                  "blocker_code": "M7-B6",
              },
              "P9.E": {
                  "execution_id": os.environ["UPSTREAM_P9E_EXECUTION"],
                  "summary_key": "p9e_execution_summary.json",
                  "blocker_key": "p9e_decision_chain_blocker_register.json",
                  "expected_verdict": "ADVANCE_TO_P10",
                  "expected_gate": "M7.I_READY",
                  "blocker_code": "M7-B10",
              },
              "P10.E": {
                  "execution_id": os.environ["UPSTREAM_P10E_EXECUTION"],
                  "summary_key": "p10e_execution_summary.json",
                  "blocker_key": "p10e_case_labels_blocker_register.json",
                  "expected_verdict": "ADVANCE_TO_M7",
                  "expected_gate": "M7.J_READY",
                  "blocker_code": "M7-B14",
              },
          }

          s3 = boto3.client("s3", region_name=region)
          ce = boto3.client("ce", region_name=billing_region)

          blockers = []
          read_errors = []
          budget_errors = []
          rollup_rows = []
          upstream_times = []

          registry_text = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md").read_text(encoding="utf-8")
          quoted = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*\"([^\"]*)\"", registry_text)
          boolish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*(true|false)`", registry_text, flags=re.IGNORECASE)
          numish = re.findall(r"\* `([A-Z0-9_]+)\s*=\s*([0-9]+(?:\.[0-9]+)?)`", registry_text)
          handles = {k: v for k, v in quoted}
          handles.update({k: v.lower() for k, v in boolish})
          handles.update({k: v for k, v in numish})

          required_handles = [
              "DEV_FULL_MONTHLY_BUDGET_LIMIT_USD",
              "DEV_FULL_BUDGET_ALERT_1_USD",
              "DEV_FULL_BUDGET_ALERT_2_USD",
              "DEV_FULL_BUDGET_ALERT_3_USD",
              "BUDGET_CURRENCY",
              "COST_CAPTURE_SCOPE",
              "AWS_COST_CAPTURE_ENABLED",
              "DATABRICKS_COST_CAPTURE_ENABLED",
              "PHASE_BUDGET_ENVELOPE_PATH_PATTERN",
              "PHASE_COST_OUTCOME_RECEIPT_PATH_PATTERN",
              "M7_HANDOFF_PACK_PATH_PATTERN",
          ]

          missing_handles = []
          placeholder_handles = []
          for key in required_handles:
              value = str(handles.get(key, "")).strip()
              if not value:
                  missing_handles.append(key)
              elif ("TO_PIN" in value) or ("<" in value) or ("TBD" in value):
                  placeholder_handles.append(key)
          if missing_handles or placeholder_handles:
              blockers.append({"code": "M7-B1", "message": "M7.J required handles are missing or placeholder-valued."})

          for phase_name, cfg in upstream_map.items():
              summary_s3_key = f"evidence/dev_full/run_control/{cfg['execution_id']}/{cfg['summary_key']}"
              blocker_s3_key = f"evidence/dev_full/run_control/{cfg['execution_id']}/{cfg['blocker_key']}"
              summary, err_s = s3_get_json(s3, evidence_bucket, summary_s3_key)
              blocker_reg, err_b = s3_get_json(s3, evidence_bucket, blocker_s3_key)
              if err_s:
                  read_errors.append(err_s)
              if err_b:
                  read_errors.append(err_b)

              summary_ok = False
              blocker_ok = False
              verdict_ok = False
              if summary is not None:
                  summary_ok = bool(summary.get("overall_pass"))
                  verdict_ok = str(summary.get("phase_verdict", summary.get("verdict", ""))).strip() == cfg["expected_verdict"]
                  gate_ok = str(summary.get("next_gate", "")).strip() == cfg["expected_gate"]
                  summary_ok = summary_ok and verdict_ok and gate_ok
                  parsed = parse_utc(str(summary.get("captured_at_utc", "")))
                  if parsed is not None:
                      upstream_times.append(parsed)
              if blocker_reg is not None:
                  blocker_ok = int(blocker_reg.get("blocker_count", 0)) == 0

              if not summary_ok or not blocker_ok:
                  blockers.append({"code": cfg["blocker_code"], "message": f"{phase_name} upstream posture invalid for M7.J rollup."})

              rollup_rows.append({
                  "phase": phase_name,
                  "execution_id": cfg["execution_id"],
                  "summary_s3_key": summary_s3_key,
                  "blocker_s3_key": blocker_s3_key,
                  "summary_ok": summary_ok,
                  "blocker_ok": blocker_ok,
                  "expected_verdict": cfg["expected_verdict"],
                  "expected_next_gate": cfg["expected_gate"],
              })

          monthly_limit = Decimal("1")
          alert_1 = Decimal("1")
          alert_2 = Decimal("1")
          alert_3 = Decimal("1")
          try:
              monthly_limit = safe_decimal(handles.get("DEV_FULL_MONTHLY_BUDGET_LIMIT_USD", "0"), "DEV_FULL_MONTHLY_BUDGET_LIMIT_USD")
              alert_1 = safe_decimal(handles.get("DEV_FULL_BUDGET_ALERT_1_USD", "0"), "DEV_FULL_BUDGET_ALERT_1_USD")
              alert_2 = safe_decimal(handles.get("DEV_FULL_BUDGET_ALERT_2_USD", "0"), "DEV_FULL_BUDGET_ALERT_2_USD")
              alert_3 = safe_decimal(handles.get("DEV_FULL_BUDGET_ALERT_3_USD", "0"), "DEV_FULL_BUDGET_ALERT_3_USD")
          except ValueError as exc:
              budget_errors.append(str(exc))
          if not (alert_1 < alert_2 < alert_3 <= monthly_limit):
              budget_errors.append("budget_threshold_order_invalid")
          if budget_errors:
              blockers.append({"code": "M7-B15", "message": "M7.J budget envelope handles are invalid."})

          aws_cost_capture_enabled = str(handles.get("AWS_COST_CAPTURE_ENABLED", "false")).lower() == "true"
          databricks_cost_capture_enabled = str(handles.get("DATABRICKS_COST_CAPTURE_ENABLED", "false")).lower() == "true"
          cost_capture_scope = str(handles.get("COST_CAPTURE_SCOPE", "")).strip()
          budget_currency = str(handles.get("BUDGET_CURRENCY", "USD")).strip() or "USD"

          aws_mtd_cost = None
          aws_currency = budget_currency
          ce_error = None
          if aws_cost_capture_enabled:
              try:
                  ce_payload = ce.get_cost_and_usage(
                      TimePeriod={"Start": month_start_utc(now_dt), "End": tomorrow_utc(now_dt)},
                      Granularity="MONTHLY",
                      Metrics=["UnblendedCost"],
                  )
                  groups = ce_payload.get("ResultsByTime", [])
                  if groups:
                      total = groups[0].get("Total", {}).get("UnblendedCost", {})
                      amount = str(total.get("Amount", "")).strip()
                      unit = str(total.get("Unit", "")).strip()
                      if amount:
                          aws_mtd_cost = Decimal(amount)
                      if unit:
                          aws_currency = unit
                  if aws_mtd_cost is None:
                      ce_error = "ce_amount_missing"
              except (BotoCoreError, ClientError, InvalidOperation) as exc:
                  ce_error = f"ce_get_cost_and_usage_failed:{type(exc).__name__}"
          else:
              ce_error = "aws_cost_capture_disabled"

          if ce_error is not None:
              blockers.append({"code": "M7-B15", "message": f"M7.J AWS cost capture failed ({ce_error})."})
          if aws_mtd_cost is not None and aws_mtd_cost >= alert_3:
              blockers.append({
                  "code": "M7-B15",
                  "message": f"M7.J cost hard-stop reached (aws_mtd_cost={str(aws_mtd_cost)}, alert_3={str(alert_3)}).",
              })

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          verdict = "ADVANCE_TO_M8" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M8_READY" if overall_pass else "HOLD_REMEDIATE"

          phase_start = min(upstream_times).isoformat().replace("+00:00", "Z") if upstream_times else captured_at

          rollup_matrix = {
              "captured_at_utc": captured_at,
              "phase": "M7.J",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_rows": rollup_rows,
              "overall_pass": overall_pass,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M7.J",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "read_errors": read_errors,
              "budget_errors": budget_errors,
              "ce_error": ce_error,
          }
          phase_budget_envelope = {
              "phase": "M7.J",
              "phase_id": "M7",
              "phase_execution_id": execution_id,
              "captured_at_utc": captured_at,
              "budget_currency": budget_currency,
              "monthly_limit_amount": str(monthly_limit),
              "alert_1_amount": str(alert_1),
              "alert_2_amount": str(alert_2),
              "alert_3_amount": str(alert_3),
              "cost_capture_scope": cost_capture_scope,
              "aws_cost_capture_enabled": aws_cost_capture_enabled,
              "databricks_cost_capture_enabled": databricks_cost_capture_enabled,
              "phase_window": {"start_utc": phase_start, "end_utc": captured_at},
              "overall_pass": len([b for b in blockers if b["code"] == "M7-B15"]) == 0,
              "blockers": [b for b in blockers if b["code"] == "M7-B15"],
          }
          phase_cost_outcome_receipt = {
              "phase_id": "M7",
              "phase_execution_id": execution_id,
              "window_start_utc": phase_start,
              "window_end_utc": captured_at,
              "spend_amount": str(aws_mtd_cost) if aws_mtd_cost is not None else None,
              "spend_currency": aws_currency,
              "artifacts_emitted": [
                  "m7_rollup_matrix.json",
                  "m7_blocker_register.json",
                  "m7_phase_budget_envelope.json",
                  "m7_phase_cost_outcome_receipt.json",
                  "m8_handoff_pack.json",
                  "m7_execution_summary.json",
                  "m7j_execution_summary.json",
              ],
              "decision_or_risk_retired": "M7 closure verdict emitted from deterministic P8/P9/P10 chain with M8 handoff pack.",
              "source_components": {
                  "aws_mtd_cost_amount": str(aws_mtd_cost) if aws_mtd_cost is not None else None,
                  "aws_currency": aws_currency,
                  "databricks_mtd_cost_amount": None,
                  "databricks_capture_mode": "DEFERRED",
              },
          }
          m8_handoff_pack = {
              "captured_at_utc": captured_at,
              "phase": "M7.J",
              "phase_execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "m8_entry_gate": next_gate,
              "m7_verdict": verdict,
              "upstream_refs": {
                  "p8e_execution_id": upstream_map["P8.E"]["execution_id"],
                  "p9e_execution_id": upstream_map["P9.E"]["execution_id"],
                  "p10e_execution_id": upstream_map["P10.E"]["execution_id"],
              },
              "contract": {
                  "required_m8_artifacts_present": overall_pass,
                  "cost_outcome_attached": True,
              },
          }
          m7_execution_summary = {
              "captured_at_utc": captured_at,
              "phase": "M7.J",
              "phase_id": "M7",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "verdict": verdict,
              "next_gate": next_gate,
              "upstream_chain": {
                  "required_count": 3,
                  "green_count": sum(1 for r in rollup_rows if r["summary_ok"] and r["blocker_ok"]),
                  "all_green": all(r["summary_ok"] and r["blocker_ok"] for r in rollup_rows),
              },
              "upstream_refs": {
                  "p8e_execution_id": upstream_map["P8.E"]["execution_id"],
                  "p9e_execution_id": upstream_map["P9.E"]["execution_id"],
                  "p10e_execution_id": upstream_map["P10.E"]["execution_id"],
              },
          }
          m7j_execution_summary = {
              "captured_at_utc": captured_at,
              "phase": "M7.J",
              "phase_id": "M7",
              "execution_id": execution_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
          }

          artifacts = {
              "m7_rollup_matrix.json": rollup_matrix,
              "m7_blocker_register.json": blocker_register,
              "m7_phase_budget_envelope.json": phase_budget_envelope,
              "m7_phase_cost_outcome_receipt.json": phase_cost_outcome_receipt,
              "m8_handoff_pack.json": m8_handoff_pack,
              "m7_handoff_pack.json": m8_handoff_pack,
              "m7_execution_summary.json": m7_execution_summary,
              "m7j_execution_summary.json": m7j_execution_summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for name, payload in artifacts.items():
              write_json(run_dir / name, payload)

          run_control_prefix = f"evidence/dev_full/run_control/{execution_id}"
          upload_errors = []
          for name, payload in artifacts.items():
              err = s3_put_json(s3, evidence_bucket, f"{run_control_prefix}/{name}", payload)
              if err:
                  upload_errors.append(err)

          if upload_errors:
              blockers.append({"code": "M7-B15", "message": "Failed to publish/readback M7.J artifact set to durable run-control prefix."})
              blockers = dedupe(blockers)
              overall_pass = False
              verdict = "HOLD_REMEDIATE"
              next_gate = "HOLD_REMEDIATE"
              blocker_register["blockers"] = blockers
              blocker_register["blocker_count"] = len(blockers)
              blocker_register["upload_errors"] = upload_errors
              m7_execution_summary["overall_pass"] = overall_pass
              m7_execution_summary["blocker_count"] = len(blockers)
              m7_execution_summary["blockers"] = blockers
              m7_execution_summary["verdict"] = verdict
              m7_execution_summary["next_gate"] = next_gate
              m7j_execution_summary["overall_pass"] = overall_pass
              m7j_execution_summary["blocker_count"] = len(blockers)
              m7j_execution_summary["verdict"] = verdict
              m7j_execution_summary["next_gate"] = next_gate
              write_json(run_dir / "m7_blocker_register.json", blocker_register)
              write_json(run_dir / "m7_execution_summary.json", m7_execution_summary)
              write_json(run_dir / "m7j_execution_summary.json", m7j_execution_summary)

          print(json.dumps(
              {
                  "execution_id": execution_id,
                  "overall_pass": m7j_execution_summary["overall_pass"],
                  "blocker_count": m7j_execution_summary["blocker_count"],
                  "verdict": m7j_execution_summary["verdict"],
                  "next_gate": m7j_execution_summary["next_gate"],
                  "run_dir": str(run_dir),
                  "run_control_prefix": f"s3://{evidence_bucket}/{run_control_prefix}/",
              },
              ensure_ascii=True,
          ))
          PY

      - name: Fail-closed verdict gate
        shell: bash
        run: |
          set -euo pipefail
          SUMMARY_PATH="${{ steps.run_meta_7q.outputs.run_dir }}/m7_execution_summary.json"
          BLOCKER_PATH="${{ steps.run_meta_7q.outputs.run_dir }}/m7_blocker_register.json"
          export SUMMARY_PATH BLOCKER_PATH
          python - <<'PY'
          import json
          import os
          import sys
          from pathlib import Path

          summary = json.loads(Path(os.environ["SUMMARY_PATH"]).read_text(encoding="utf-8"))
          blockers = json.loads(Path(os.environ["BLOCKER_PATH"]).read_text(encoding="utf-8"))
          overall_pass = bool(summary.get("overall_pass"))
          blocker_count = int(blockers.get("blocker_count", 0))
          verdict = str(summary.get("verdict", "")).strip()
          next_gate = str(summary.get("next_gate", "")).strip()

          print(json.dumps(
              {
                  "overall_pass": overall_pass,
                  "blocker_count": blocker_count,
                  "execution_id": summary.get("execution_id"),
                  "verdict": verdict,
                  "next_gate": next_gate,
              },
              ensure_ascii=True,
          ))
          if (not overall_pass) or blocker_count != 0:
              sys.exit(1)
          if verdict != "ADVANCE_TO_M8":
              sys.exit(1)
          if next_gate != "M8_READY":
              sys.exit(1)
          PY

      - name: Upload M7.J rollup artifact set
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m7-rollup-m7q-${{ steps.run_meta_7q.outputs.timestamp }}
          path: ${{ steps.run_meta_7q.outputs.run_dir }}
          if-no-files-found: warn
