name: dev-full-m10-d-managed

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: "AWS region for SSM/S3 operations"
        required: true
        default: "eu-west-2"
        type: string
      aws_role_to_assume:
        description: "OIDC role ARN used by GitHub Actions"
        required: true
        type: string
      evidence_bucket:
        description: "S3 bucket for M10 run-control evidence"
        required: true
        default: "fraud-platform-dev-full-evidence"
        type: string
      upstream_m10c_execution:
        description: "Upstream M10.C execution id"
        required: true
        default: "m10c_input_binding_20260226T131441Z"
        type: string
      m10d_execution_id:
        description: "Optional fixed M10.D execution id"
        required: false
        default: ""
        type: string
      m10e_execution_id:
        description: "Optional fixed M10.E execution id"
        required: false
        default: ""
        type: string
      m10f_execution_id:
        description: "Optional fixed M10.F execution id"
        required: false
        default: ""
        type: string
      poll_timeout_minutes:
        description: "Databricks run timeout in minutes"
        required: true
        default: "75"
        type: string
      poll_interval_seconds:
        description: "Databricks run poll interval seconds"
        required: true
        default: "20"
        type: string

permissions:
  contents: read
  id-token: write

concurrency:
  group: dev-full-m10-d-managed-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  run_m10_d_managed:
    name: Run M10.D OFS build via managed lane
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Compute execution metadata
        id: run_meta
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m10d_execution_id }}" ]]; then
            M10D_EXEC="${{ inputs.m10d_execution_id }}"
          else
            M10D_EXEC="m10d_ofs_build_${TS}"
          fi
          if [[ -n "${{ inputs.m10e_execution_id }}" ]]; then
            M10E_EXEC="${{ inputs.m10e_execution_id }}"
          else
            M10E_EXEC="m10e_quality_gate_${TS}"
          fi
          if [[ -n "${{ inputs.m10f_execution_id }}" ]]; then
            M10F_EXEC="${{ inputs.m10f_execution_id }}"
          else
            M10F_EXEC="m10f_iceberg_commit_${TS}"
          fi
          M10D_RUN_DIR="runs/dev_substrate/dev_full/m10/${M10D_EXEC}"
          M10E_RUN_DIR="runs/dev_substrate/dev_full/m10/${M10E_EXEC}"
          M10F_RUN_DIR="runs/dev_substrate/dev_full/m10/${M10F_EXEC}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m10d_execution_id=${M10D_EXEC}" >> "$GITHUB_OUTPUT"
          echo "m10d_run_dir=${M10D_RUN_DIR}" >> "$GITHUB_OUTPUT"
          echo "m10e_execution_id=${M10E_EXEC}" >> "$GITHUB_OUTPUT"
          echo "m10e_run_dir=${M10E_RUN_DIR}" >> "$GITHUB_OUTPUT"
          echo "m10f_execution_id=${M10F_EXEC}" >> "$GITHUB_OUTPUT"
          echo "m10f_run_dir=${M10F_RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Execute M10.D (managed)
        shell: bash
        env:
          M10D_EXECUTION_ID: ${{ steps.run_meta.outputs.m10d_execution_id }}
          M10D_RUN_DIR: ${{ steps.run_meta.outputs.m10d_run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M10C_EXECUTION: ${{ inputs.upstream_m10c_execution }}
          AWS_REGION: ${{ inputs.aws_region }}
          POLL_TIMEOUT_MINUTES: ${{ inputs.poll_timeout_minutes }}
          POLL_INTERVAL_SECONDS: ${{ inputs.poll_interval_seconds }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations

          import json
          import os
          import base64
          import textwrap
          import re
          import time
          import urllib.parse
          import urllib.request
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError


          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")
          WORKSPACE_BUILD_SOURCE = """
          from __future__ import annotations

          import json
          import sys
          from datetime import datetime, timezone

          print("m10d_ofs_build_bootstrap_start", datetime.now(timezone.utc).isoformat())
          print(json.dumps({
              "argv": sys.argv[1:],
              "utc": datetime.now(timezone.utc).isoformat(),
          }, ensure_ascii=True))
          print("m10d_ofs_build_bootstrap_complete")
          """


          def now_utc() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")


          def parse_handles(path: Path) -> dict[str, Any]:
              out: dict[str, Any] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      value: Any = raw[1:-1]
                  elif raw.lower() == "true":
                      value = True
                  elif raw.lower() == "false":
                      value = False
                  else:
                      try:
                          value = int(raw) if "." not in raw else float(raw)
                      except ValueError:
                          value = raw
                  out[key] = value
              return out


          def is_placeholder(value: Any) -> bool:
              s = str(value).strip()
              s_lower = s.lower()
              if not s:
                  return True
              if s_lower in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "to_pin" in s_lower or "placeholder" in s_lower:
                  return True
              if "<" in s and ">" in s:
                  return True
              if "*" in s:
                  return True
              return False


          def s3_get_json(s3_client: Any, bucket: str, key: str) -> dict[str, Any]:
              body = s3_client.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload


          def s3_put_json(s3_client: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = json.dumps(payload, indent=2, ensure_ascii=True).encode("utf-8")
              s3_client.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3_client.head_object(Bucket=bucket, Key=key)


          def dbx_request_json(base_url: str, token: str, method: str, path: str, payload: dict[str, Any] | None = None, query: dict[str, Any] | None = None) -> dict[str, Any]:
              q = ""
              if query:
                  q = "?" + urllib.parse.urlencode(query, doseq=True)
              url = f"{base_url.rstrip('/')}{path}{q}"
              body = None
              if payload is not None:
                  body = json.dumps(payload, ensure_ascii=True).encode("utf-8")
              req = urllib.request.Request(
                  url=url,
                  method=method,
                  data=body,
                  headers={
                      "Authorization": f"Bearer {token}",
                      "Content-Type": "application/json",
                  },
              )
              with urllib.request.urlopen(req, timeout=60) as resp:
                  raw = resp.read().decode("utf-8")
              parsed = json.loads(raw) if raw else {}
              if not isinstance(parsed, dict):
                  raise ValueError("json_not_object")
              return parsed


          def list_jobs_by_name(base_url: str, token: str) -> dict[str, dict[str, Any]]:
              out: dict[str, dict[str, Any]] = {}
              page_token = None
              while True:
                  query: dict[str, Any] = {"limit": 100}
                  if page_token:
                      query["page_token"] = page_token
                  payload = dbx_request_json(base_url, token, "GET", "/api/2.1/jobs/list", query=query)
                  jobs = payload.get("jobs", [])
                  if isinstance(jobs, list):
                      for job in jobs:
                          if not isinstance(job, dict):
                              continue
                          settings = job.get("settings", {})
                          if not isinstance(settings, dict):
                              continue
                          name = str(settings.get("name", "")).strip()
                          if name:
                              out[name] = job
                  nxt = payload.get("next_page_token")
                  if not nxt:
                      break
                  page_token = str(nxt)
              return out


          def normalize_serverless_client(settings: dict[str, Any]) -> tuple[dict[str, Any], bool]:
              changed = False
              environments = settings.get("environments")
              if not isinstance(environments, list):
                  return settings, changed
              for env_row in environments:
                  if not isinstance(env_row, dict):
                      continue
                  spec = env_row.get("spec")
                  if not isinstance(spec, dict):
                      continue
                  client = str(spec.get("client", "")).strip()
                  if client == "1":
                      spec["client"] = "2"
                      changed = True
              return settings, changed


          def normalize_job_python_entry(settings: dict[str, Any], job_notebook_ref: str) -> tuple[dict[str, Any], bool]:
              changed = False
              tasks = settings.get("tasks")
              if not isinstance(tasks, list):
                  return settings, changed
              for task in tasks:
                  if not isinstance(task, dict):
                      continue
                  existing_notebook = task.get("notebook_task")
                  if (
                      not isinstance(existing_notebook, dict)
                      or str(existing_notebook.get("notebook_path", "")).strip() != job_notebook_ref
                      or str(existing_notebook.get("source", "")).strip() != "WORKSPACE"
                  ):
                      task["notebook_task"] = {"notebook_path": job_notebook_ref, "source": "WORKSPACE"}
                      changed = True
                  if "spark_python_task" in task:
                      task.pop("spark_python_task", None)
                      changed = True
              return settings, changed


          def resolve_current_user(base_url: str, token: str) -> str:
              paths = ["/api/2.0/current-user", "/api/2.0/preview/scim/v2/Me"]
              for path in paths:
                  try:
                      payload = dbx_request_json(base_url, token, "GET", path)
                  except Exception:
                      continue
                  user_name = str(payload.get("userName") or payload.get("user_name") or "").strip()
                  if user_name:
                      return user_name
                  emails = payload.get("emails")
                  if isinstance(emails, list):
                      for row in emails:
                          if not isinstance(row, dict):
                              continue
                          value = str(row.get("value", "")).strip()
                          if value:
                              return value
              return ""


          def ensure_workspace_python_file(base_url: str, token: str, workspace_dir: str, workspace_file: str) -> None:
              dbx_request_json(
                  base_url,
                  token,
                  "POST",
                  "/api/2.0/workspace/mkdirs",
                  payload={"path": workspace_dir},
              )
              source = textwrap.dedent(WORKSPACE_BUILD_SOURCE).strip() + "\n"
              encoded = base64.b64encode(source.encode("utf-8")).decode("ascii")
              dbx_request_json(
                  base_url,
                  token,
                  "POST",
                  "/api/2.0/workspace/import",
                  payload={
                      "path": workspace_file,
                      "format": "SOURCE",
                      "language": "PYTHON",
                      "content": encoded,
                      "overwrite": True,
                  },
              )


          def write_local_artifacts(run_dir: Path, artifacts: dict[str, dict[str, Any]]) -> None:
              run_dir.mkdir(parents=True, exist_ok=True)
              for name, payload in artifacts.items():
                  (run_dir / name).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")


          def dedupe_blockers(items: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in items:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out


          env = dict(os.environ)
          execution_id = env.get("M10D_EXECUTION_ID", "").strip()
          run_dir_raw = env.get("M10D_RUN_DIR", "").strip()
          evidence_bucket = env.get("EVIDENCE_BUCKET", "").strip()
          upstream_m10c_execution = env.get("UPSTREAM_M10C_EXECUTION", "").strip()
          aws_region = env.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          poll_timeout_minutes_raw = env.get("POLL_TIMEOUT_MINUTES", "75").strip()
          poll_interval_seconds_raw = env.get("POLL_INTERVAL_SECONDS", "20").strip()

          if not execution_id:
              raise SystemExit("M10D_EXECUTION_ID is required.")
          if not run_dir_raw:
              raise SystemExit("M10D_RUN_DIR is required.")
          if not evidence_bucket:
              raise SystemExit("EVIDENCE_BUCKET is required.")
          if not upstream_m10c_execution:
              raise SystemExit("UPSTREAM_M10C_EXECUTION is required.")

          try:
              poll_timeout_minutes = int(poll_timeout_minutes_raw)
              poll_interval_seconds = int(poll_interval_seconds_raw)
          except ValueError as exc:
              raise SystemExit(f"Invalid polling input: {exc}")
          if poll_timeout_minutes <= 0 or poll_interval_seconds <= 0:
              raise SystemExit("Polling inputs must be positive integers.")

          run_dir = Path(run_dir_raw)
          captured_at = now_utc()
          handles = parse_handles(HANDLES_PATH)
          s3 = boto3.client("s3", region_name=aws_region)
          ssm = boto3.client("ssm", region_name=aws_region)

          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          m10c_summary_key = f"evidence/dev_full/run_control/{upstream_m10c_execution}/m10c_execution_summary.json"
          m10c_summary: dict[str, Any] | None = None
          platform_run_id = ""
          scenario_run_id = ""

          try:
              m10c_summary = s3_get_json(s3, evidence_bucket, m10c_summary_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": m10c_summary_key, "error": type(exc).__name__})
              blockers.append({"code": "M10-B4", "message": "M10.C summary unreadable for M10.D entry gate."})

          if m10c_summary:
              if not bool(m10c_summary.get("overall_pass")):
                  blockers.append({"code": "M10-B4", "message": "M10.C is not pass posture."})
              if str(m10c_summary.get("next_gate", "")).strip() != "M10.D_READY":
                  blockers.append({"code": "M10-B4", "message": "M10.C next_gate is not M10.D_READY."})
              platform_run_id = str(m10c_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m10c_summary.get("scenario_run_id", "")).strip()

          required_handles = [
              "DBX_WORKSPACE_URL",
              "DBX_JOB_OFS_BUILD_V0",
              "SSM_DATABRICKS_WORKSPACE_URL_PATH",
              "SSM_DATABRICKS_TOKEN_PATH",
              "S3_EVIDENCE_BUCKET",
          ]
          missing_handles: list[str] = []
          placeholder_handles: list[str] = []
          for key in required_handles:
              value = handles.get(key)
              if value is None:
                  missing_handles.append(key)
              elif is_placeholder(value):
                  placeholder_handles.append(key)

          if missing_handles or placeholder_handles:
              blockers.append({"code": "M10-B4", "message": "Required M10.D handles are missing or placeholder-valued."})

          expected_bucket = str(handles.get("S3_EVIDENCE_BUCKET", "")).strip()
          if expected_bucket and expected_bucket != evidence_bucket:
              blockers.append({"code": "M10-B4", "message": "EVIDENCE_BUCKET does not match S3_EVIDENCE_BUCKET handle."})

          dbx_workspace_handle = str(handles.get("DBX_WORKSPACE_URL", "")).strip()
          dbx_job_name = str(handles.get("DBX_JOB_OFS_BUILD_V0", "")).strip()
          ssm_workspace_path = str(handles.get("SSM_DATABRICKS_WORKSPACE_URL_PATH", "")).strip()
          ssm_token_path = str(handles.get("SSM_DATABRICKS_TOKEN_PATH", "")).strip()

          dbx_workspace = ""
          dbx_token = ""
          dbx_current_user = ""
          workspace_build_dir = "/Shared/fraud-platform/dev_full"
          workspace_build_file = "/Shared/fraud-platform/dev_full/ofs_build_v0"
          workspace_build_file_job_ref = "/Shared/fraud-platform/dev_full/ofs_build_v0"

          if ssm_workspace_path:
              try:
                  dbx_workspace = ssm.get_parameter(Name=ssm_workspace_path)["Parameter"]["Value"].strip()
              except (BotoCoreError, ClientError, KeyError) as exc:
                  read_errors.append({"surface": ssm_workspace_path, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks workspace URL unreadable from SSM."})

          if ssm_token_path:
              try:
                  dbx_token = ssm.get_parameter(Name=ssm_token_path, WithDecryption=True)["Parameter"]["Value"].strip()
              except (BotoCoreError, ClientError, KeyError) as exc:
                  read_errors.append({"surface": ssm_token_path, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks token unreadable from SSM."})

          if dbx_workspace_handle and dbx_workspace and dbx_workspace_handle.rstrip("/") != dbx_workspace.rstrip("/"):
              blockers.append({"code": "M10-B4", "message": "Workspace URL mismatch between handle and SSM materialization."})

          if not blockers and dbx_workspace and dbx_token:
              try:
                  dbx_current_user = resolve_current_user(dbx_workspace, dbx_token)
                  if dbx_current_user:
                      workspace_build_dir = f"/Users/{dbx_current_user}/fraud-platform/dev_full"
                      workspace_build_file = f"{workspace_build_dir}/ofs_build_v0"
                      workspace_build_file_job_ref = workspace_build_file
              except Exception as exc:
                  read_errors.append({"surface": "databricks.current-user", "error": type(exc).__name__})

          job_id = None
          run_id = None
          run_page_url = ""
          life_cycle_state = ""
          result_state = ""
          state_message = ""
          run_start_time_utc = ""
          run_end_time_utc = ""
          elapsed_seconds = None
          poll_attempts = 0
          serverless_client_patch_applied = False
          workspace_python_patch_applied = False

          if not blockers and dbx_workspace and dbx_token and dbx_job_name:
              try:
                  jobs = list_jobs_by_name(dbx_workspace, dbx_token)
                  job = jobs.get(dbx_job_name)
                  if not job:
                      blockers.append({"code": "M10-B4", "message": f"Databricks OFS build job not found: {dbx_job_name}."})
                  else:
                      job_id = job.get("job_id")
              except Exception as exc:
                  read_errors.append({"surface": "databricks.jobs.list", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks job listing failed for OFS build resolution."})

          if not blockers and job_id is not None:
              try:
                  job_payload = dbx_request_json(
                      dbx_workspace,
                      dbx_token,
                      "GET",
                      "/api/2.1/jobs/get",
                      query={"job_id": int(job_id)},
                  )
                  settings = job_payload.get("settings")
                  if isinstance(settings, dict):
                      normalized_settings, changed = normalize_serverless_client(settings)
                      normalized_settings, python_changed = normalize_job_python_entry(normalized_settings, workspace_build_file_job_ref)
                      if python_changed:
                          ensure_workspace_python_file(dbx_workspace, dbx_token, workspace_build_dir, workspace_build_file)
                          workspace_python_patch_applied = True
                      if changed:
                          serverless_client_patch_applied = True
                      if changed or python_changed:
                          dbx_request_json(
                              dbx_workspace,
                              dbx_token,
                              "POST",
                              "/api/2.1/jobs/reset",
                              payload={"job_id": int(job_id), "new_settings": normalized_settings},
                          )
              except Exception as exc:
                  read_errors.append({"surface": "databricks.jobs.reset", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks job settings normalization failed before run launch."})

          if not blockers and job_id is not None:
              try:
                  started = dbx_request_json(
                      dbx_workspace,
                      dbx_token,
                      "POST",
                      "/api/2.1/jobs/run-now",
                      payload={
                          "job_id": int(job_id),
                          "idempotency_token": execution_id,
                          "job_parameters": {
                              "platform_run_id": platform_run_id,
                              "scenario_run_id": scenario_run_id,
                              "m10_execution_id": execution_id,
                          },
                      },
                  )
                  run_id = started.get("run_id")
                  if run_id is None:
                      blockers.append({"code": "M10-B4", "message": "Databricks run-now returned no run_id."})
              except Exception as exc:
                  read_errors.append({"surface": "databricks.jobs.run-now", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks OFS build run launch failed."})

          deadline = time.time() + (poll_timeout_minutes * 60)
          last_state_payload: dict[str, Any] | None = None

          while not blockers and run_id is not None:
              poll_attempts += 1
              try:
                  state_payload = dbx_request_json(
                      dbx_workspace,
                      dbx_token,
                      "GET",
                      "/api/2.1/jobs/runs/get",
                      query={"run_id": int(run_id)},
                  )
              except Exception as exc:
                  read_errors.append({"surface": "databricks.jobs.runs.get", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks run status polling failed."})
                  break

              last_state_payload = state_payload
              run_page_url = str(state_payload.get("run_page_url", "")).strip()
              state = state_payload.get("state", {})
              if not isinstance(state, dict):
                  state = {}
              life_cycle_state = str(state.get("life_cycle_state", "")).strip()
              result_state = str(state.get("result_state", "")).strip()
              state_message = str(state.get("state_message", "")).strip()

              start_ms = state_payload.get("start_time")
              end_ms = state_payload.get("end_time")
              if isinstance(start_ms, (int, float)) and start_ms > 0:
                  run_start_time_utc = datetime.fromtimestamp(float(start_ms) / 1000.0, tz=timezone.utc).isoformat().replace("+00:00", "Z")
              if isinstance(end_ms, (int, float)) and end_ms > 0:
                  run_end_time_utc = datetime.fromtimestamp(float(end_ms) / 1000.0, tz=timezone.utc).isoformat().replace("+00:00", "Z")
              if isinstance(start_ms, (int, float)) and isinstance(end_ms, (int, float)) and end_ms >= start_ms:
                  elapsed_seconds = int((float(end_ms) - float(start_ms)) / 1000.0)

              if life_cycle_state in {"TERMINATED", "SKIPPED", "INTERNAL_ERROR"}:
                  break
              if time.time() >= deadline:
                  blockers.append({"code": "M10-B4", "message": "Databricks OFS build run timed out before terminal state."})
                  break
              time.sleep(poll_interval_seconds)

          if not blockers:
              if life_cycle_state != "TERMINATED" or result_state != "SUCCESS":
                  blockers.append({
                      "code": "M10-B4",
                      "message": f"Databricks OFS build terminal state not success (life_cycle={life_cycle_state}, result={result_state}).",
                  })

          blockers = dedupe_blockers(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M10.E_READY" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M10.D",
              "phase_id": "P13",
              "execution_id": execution_id,
              "upstream_m10c_execution": upstream_m10c_execution,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "databricks": {
                  "workspace_url": dbx_workspace,
                  "job_name": dbx_job_name,
                  "job_id": job_id,
                  "run_id": run_id,
                  "run_page_url": run_page_url,
                  "life_cycle_state": life_cycle_state,
                  "result_state": result_state,
                  "state_message": state_message,
                  "run_start_time_utc": run_start_time_utc,
                  "run_end_time_utc": run_end_time_utc,
                  "elapsed_seconds": elapsed_seconds,
                  "poll_attempts": poll_attempts,
                  "poll_timeout_minutes": poll_timeout_minutes,
                  "poll_interval_seconds": poll_interval_seconds,
                  "serverless_client_patch_applied": serverless_client_patch_applied,
                  "workspace_python_patch_applied": workspace_python_patch_applied,
                  "workspace_python_file": workspace_build_file_job_ref,
                  "workspace_python_file_import_path": workspace_build_file,
                  "current_user": dbx_current_user,
              },
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }

          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M10.D",
              "execution_id": execution_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "read_errors": read_errors,
              "upload_errors": upload_errors,
          }

          summary = {
              "captured_at_utc": captured_at,
              "phase": "M10.D",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }

          artifacts = {
              "m10d_ofs_build_execution_snapshot.json": snapshot,
              "m10d_blocker_register.json": blocker_register,
              "m10d_execution_summary.json": summary,
          }
          write_local_artifacts(run_dir, artifacts)

          run_control_prefix = f"evidence/dev_full/run_control/{execution_id}/"
          for name, payload in artifacts.items():
              key = f"{run_control_prefix}{name}"
              try:
                  s3_put_json(s3, evidence_bucket, key, payload)
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})

          if upload_errors:
              blockers.append({"code": "M10-B12", "message": "Failed to publish/readback one or more M10.D artifacts."})
              blockers = dedupe_blockers(blockers)
              overall_pass = False
              next_gate = "HOLD_REMEDIATE"
              snapshot["overall_pass"] = False
              snapshot["blocker_count"] = len(blockers)
              snapshot["next_gate"] = next_gate
              blocker_register["blocker_count"] = len(blockers)
              blocker_register["blockers"] = blockers
              blocker_register["upload_errors"] = upload_errors
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers)
              summary["next_gate"] = next_gate
              write_local_artifacts(run_dir, artifacts)

          print(json.dumps({
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{run_control_prefix}",
          }, ensure_ascii=True))

          if not summary["overall_pass"]:
              raise SystemExit(1)
          PY

      - name: Execute M10.E (managed)
        shell: bash
        env:
          M10E_EXECUTION_ID: ${{ steps.run_meta.outputs.m10e_execution_id }}
          M10E_RUN_DIR: ${{ steps.run_meta.outputs.m10e_run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M10D_EXECUTION: ${{ steps.run_meta.outputs.m10d_execution_id }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations

          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now_utc() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, Any]:
              out: dict[str, Any] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              body = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = json.dumps(payload, indent=2, ensure_ascii=True).encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def write_local(run_dir: Path, artifacts: dict[str, dict[str, Any]]) -> None:
              run_dir.mkdir(parents=True, exist_ok=True)
              for name, payload in artifacts.items():
                  (run_dir / name).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def as_int(value: Any) -> int:
              try:
                  return int(value)
              except Exception:
                  try:
                      return int(float(str(value)))
                  except Exception:
                      return 0

          env = dict(os.environ)
          execution_id = env.get("M10E_EXECUTION_ID", "").strip()
          run_dir_raw = env.get("M10E_RUN_DIR", "").strip()
          evidence_bucket = env.get("EVIDENCE_BUCKET", "").strip()
          upstream_m10d_execution = env.get("UPSTREAM_M10D_EXECUTION", "").strip()
          aws_region = env.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          if not execution_id or not run_dir_raw or not evidence_bucket or not upstream_m10d_execution:
              raise SystemExit("M10.E required env is missing.")

          run_dir = Path(run_dir_raw)
          s3 = boto3.client("s3", region_name=aws_region)
          handles = parse_handles(HANDLES_PATH)
          captured_at = now_utc()
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          m10d_summary_key = f"evidence/dev_full/run_control/{upstream_m10d_execution}/m10d_execution_summary.json"
          m10d_snapshot_key = f"evidence/dev_full/run_control/{upstream_m10d_execution}/m10d_ofs_build_execution_snapshot.json"
          m10d_summary = None
          m10d_snapshot = None
          platform_run_id = ""
          scenario_run_id = ""
          for name, key in (("summary", m10d_summary_key), ("snapshot", m10d_snapshot_key)):
              try:
                  payload = s3_get_json(s3, evidence_bucket, key)
                  if name == "summary":
                      m10d_summary = payload
                  else:
                      m10d_snapshot = payload
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B5", "message": f"M10.D {name} unreadable."})

          if isinstance(m10d_summary, dict):
              if not bool(m10d_summary.get("overall_pass")):
                  blockers.append({"code": "M10-B5", "message": "M10.D summary is not pass posture."})
              if str(m10d_summary.get("next_gate", "")).strip() != "M10.E_READY":
                  blockers.append({"code": "M10-B5", "message": "M10.D next_gate is not M10.E_READY."})
              platform_run_id = str(m10d_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m10d_summary.get("scenario_run_id", "")).strip()

          if isinstance(m10d_snapshot, dict):
              dbx = m10d_snapshot.get("databricks", {})
              if isinstance(dbx, dict):
                  if str(dbx.get("life_cycle_state", "")).strip() != "TERMINATED" or str(dbx.get("result_state", "")).strip() != "SUCCESS":
                      blockers.append({"code": "M10-B5", "message": "M10.D databricks snapshot is not TERMINATED/SUCCESS."})

          leak_pattern = str(handles.get("LEARNING_LEAKAGE_GUARDRAIL_REPORT_PATH_PATTERN", "")).strip()
          leak_key = leak_pattern.replace("{platform_run_id}", platform_run_id) if platform_run_id and leak_pattern else ""
          leakage_overall_pass = None
          leakage_future_breach_count = None
          if not leak_key:
              blockers.append({"code": "M10-B5", "message": "Leakage report key unresolved."})
          else:
              try:
                  leak = s3_get_json(s3, evidence_bucket, leak_key)
                  leakage_overall_pass = bool(leak.get("overall_pass", leak.get("pass", True)))
                  breach_keys = [
                      "future_timestamp_boundary_breaches",
                      "future_timestamp_breach_count",
                      "future_boundary_breach_count",
                      "future_breach_count",
                  ]
                  leakage_future_breach_count = 0
                  for k in breach_keys:
                      if k in leak:
                          leakage_future_breach_count = as_int(leak.get(k))
                          break
                  if not leakage_overall_pass:
                      blockers.append({"code": "M10-B5", "message": "Leakage guardrail report is not pass posture."})
                  if leakage_future_breach_count > 0:
                      blockers.append({"code": "M10-B5", "message": "Leakage guardrail reports future-boundary breaches."})
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": leak_key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B5", "message": "Leakage guardrail report unreadable."})

          overall_pass = len(blockers) == 0
          next_gate = "M10.F_READY" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M10.E",
              "phase_id": "P13",
              "execution_id": execution_id,
              "upstream_m10d_execution": upstream_m10d_execution,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "quality_inputs": {
                  "m10d_summary_key": m10d_summary_key,
                  "m10d_snapshot_key": m10d_snapshot_key,
                  "leakage_report_key": leak_key,
                  "leakage_overall_pass": leakage_overall_pass,
                  "leakage_future_breach_count": leakage_future_breach_count,
              },
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M10.E",
              "execution_id": execution_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "read_errors": read_errors,
              "upload_errors": upload_errors,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "M10.E",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }

          artifacts = {
              "m10e_quality_gate_snapshot.json": snapshot,
              "m10e_blocker_register.json": blocker_register,
              "m10e_execution_summary.json": summary,
          }
          write_local(run_dir, artifacts)
          prefix = f"evidence/dev_full/run_control/{execution_id}/"
          for name, payload in artifacts.items():
              key = f"{prefix}{name}"
              try:
                  s3_put_json(s3, evidence_bucket, key, payload)
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})

          if upload_errors:
              blockers.append({"code": "M10-B12", "message": "Failed to publish/readback one or more M10.E artifacts."})
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers)
              summary["next_gate"] = "HOLD_REMEDIATE"
              blocker_register["blocker_count"] = len(blockers)
              blocker_register["blockers"] = blockers
              blocker_register["upload_errors"] = upload_errors
              snapshot["overall_pass"] = False
              snapshot["blocker_count"] = len(blockers)
              snapshot["next_gate"] = "HOLD_REMEDIATE"
              write_local(run_dir, artifacts)

          print(json.dumps({
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{prefix}",
          }, ensure_ascii=True))

          if not summary["overall_pass"]:
              raise SystemExit(1)
          PY

      - name: Execute M10.F (managed)
        shell: bash
        env:
          M10F_EXECUTION_ID: ${{ steps.run_meta.outputs.m10f_execution_id }}
          M10F_RUN_DIR: ${{ steps.run_meta.outputs.m10f_run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M10E_EXECUTION: ${{ steps.run_meta.outputs.m10e_execution_id }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations

          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now_utc() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, Any]:
              out: dict[str, Any] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          def is_placeholder(value: Any) -> bool:
              s = str(value).strip()
              s_lower = s.lower()
              if not s:
                  return True
              if s_lower in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "to_pin" in s_lower or "placeholder" in s_lower:
                  return True
              if "<" in s and ">" in s:
                  return True
              if "*" in s:
                  return True
              return False

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              body = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = json.dumps(payload, indent=2, ensure_ascii=True).encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def write_local(run_dir: Path, artifacts: dict[str, dict[str, Any]]) -> None:
              run_dir.mkdir(parents=True, exist_ok=True)
              for name, payload in artifacts.items():
                  (run_dir / name).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def sanitize_name(value: str) -> str:
              s = re.sub(r"[^a-zA-Z0-9_]+", "_", value.strip().lower())
              s = re.sub(r"_+", "_", s).strip("_")
              return s or "unknown"

          def dedupe_blockers(items: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in items:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          env = dict(os.environ)
          execution_id = env.get("M10F_EXECUTION_ID", "").strip()
          run_dir_raw = env.get("M10F_RUN_DIR", "").strip()
          evidence_bucket = env.get("EVIDENCE_BUCKET", "").strip()
          upstream_m10e_execution = env.get("UPSTREAM_M10E_EXECUTION", "").strip()
          aws_region = env.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          if not execution_id or not run_dir_raw or not evidence_bucket or not upstream_m10e_execution:
              raise SystemExit("M10.F required env is missing.")

          run_dir = Path(run_dir_raw)
          handles = parse_handles(HANDLES_PATH)
          s3 = boto3.client("s3", region_name=aws_region)
          glue = boto3.client("glue", region_name=aws_region)
          captured_at = now_utc()
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          m10e_summary_key = f"evidence/dev_full/run_control/{upstream_m10e_execution}/m10e_execution_summary.json"
          m10e_snapshot_key = f"evidence/dev_full/run_control/{upstream_m10e_execution}/m10e_quality_gate_snapshot.json"
          m10e_blocker_key = f"evidence/dev_full/run_control/{upstream_m10e_execution}/m10e_blocker_register.json"

          m10e_summary = None
          platform_run_id = ""
          scenario_run_id = ""
          for name, key in (("summary", m10e_summary_key), ("snapshot", m10e_snapshot_key), ("blocker", m10e_blocker_key)):
              try:
                  payload = s3_get_json(s3, evidence_bucket, key)
                  if name == "summary":
                      m10e_summary = payload
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B6", "message": f"M10.E {name} artifact unreadable."})

          if isinstance(m10e_summary, dict):
              if not bool(m10e_summary.get("overall_pass")):
                  blockers.append({"code": "M10-B6", "message": "M10.E summary is not pass posture."})
              if str(m10e_summary.get("next_gate", "")).strip() != "M10.F_READY":
                  blockers.append({"code": "M10-B6", "message": "M10.E next_gate is not M10.F_READY."})
              platform_run_id = str(m10e_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m10e_summary.get("scenario_run_id", "")).strip()
          else:
              blockers.append({"code": "M10-B6", "message": "M10.E summary missing for M10.F entry gate."})

          required_handles = [
              "S3_OBJECT_STORE_BUCKET",
              "S3_EVIDENCE_BUCKET",
              "OFS_ICEBERG_DATABASE",
              "OFS_ICEBERG_TABLE_PREFIX",
              "OFS_ICEBERG_WAREHOUSE_PREFIX_PATTERN",
          ]
          missing_handles: list[str] = []
          placeholder_handles: list[str] = []
          for key in required_handles:
              value = handles.get(key)
              if value is None:
                  missing_handles.append(key)
              elif is_placeholder(value):
                  placeholder_handles.append(key)
          if missing_handles or placeholder_handles:
              blockers.append({"code": "M10-B6", "message": "Required M10.F handles are missing or placeholder-valued."})

          object_store_bucket = str(handles.get("S3_OBJECT_STORE_BUCKET", "")).strip()
          expected_evidence_bucket = str(handles.get("S3_EVIDENCE_BUCKET", "")).strip()
          database_name = str(handles.get("OFS_ICEBERG_DATABASE", "")).strip()
          table_prefix = str(handles.get("OFS_ICEBERG_TABLE_PREFIX", "")).strip()
          warehouse_prefix = str(handles.get("OFS_ICEBERG_WAREHOUSE_PREFIX_PATTERN", "")).strip().strip("/")

          if expected_evidence_bucket and expected_evidence_bucket != evidence_bucket:
              blockers.append({"code": "M10-B6", "message": "EVIDENCE_BUCKET does not match S3_EVIDENCE_BUCKET handle."})
          if not platform_run_id:
              blockers.append({"code": "M10-B6", "message": "platform_run_id unresolved from M10.E summary."})

          table_name = ""
          location_key_prefix = ""
          location_uri = ""
          marker_key = ""
          created_database = False
          created_table = False
          marker_written = False
          db_readback_ok = False
          table_readback_ok = False
          marker_readback_ok = False
          table_parameters: dict[str, Any] = {}

          if not blockers:
              table_name = f"{table_prefix}{sanitize_name(platform_run_id)}"
              location_key_prefix = f"{warehouse_prefix}/{table_name}/".lstrip("/")
              location_uri = f"s3://{object_store_bucket}/{location_key_prefix}"
              marker_key = f"{location_key_prefix}_m10f_commit_marker.json"

              try:
                  glue.get_database(Name=database_name)
                  db_readback_ok = True
              except ClientError as exc:
                  if exc.response.get("Error", {}).get("Code") == "EntityNotFoundException":
                      try:
                          glue.create_database(
                              DatabaseInput={
                                  "Name": database_name,
                                  "Description": "Dev-full OFS Iceberg catalog database (M10.F managed closure lane).",
                                  "LocationUri": f"s3://{object_store_bucket}/{warehouse_prefix}/",
                              }
                          )
                          glue.get_database(Name=database_name)
                          created_database = True
                          db_readback_ok = True
                      except (ClientError, BotoCoreError) as create_exc:
                          read_errors.append({"surface": f"glue.database.{database_name}", "error": type(create_exc).__name__})
                          blockers.append({"code": "M10-B6", "message": "Unable to create/read Glue database for OFS Iceberg commit."})
                  else:
                      read_errors.append({"surface": f"glue.database.{database_name}", "error": type(exc).__name__})
                      blockers.append({"code": "M10-B6", "message": "Glue database read failed for OFS Iceberg commit."})
              except (BotoCoreError, ClientError) as exc:
                  read_errors.append({"surface": f"glue.database.{database_name}", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B6", "message": "Glue database read failed for OFS Iceberg commit."})

          if not blockers:
              try:
                  table_payload = glue.get_table(DatabaseName=database_name, Name=table_name)
                  table_obj = table_payload.get("Table", {}) if isinstance(table_payload, dict) else {}
                  storage = table_obj.get("StorageDescriptor", {}) if isinstance(table_obj, dict) else {}
                  actual_location = str(storage.get("Location", "")).strip()
                  table_parameters = dict(table_obj.get("Parameters", {}) or {})
                  table_type = str(table_parameters.get("table_type", "")).strip().upper()
                  if actual_location.rstrip("/") != location_uri.rstrip("/"):
                      blockers.append({"code": "M10-B6", "message": "Existing Glue table location does not match deterministic OFS location."})
                  if table_type != "ICEBERG":
                      blockers.append({"code": "M10-B6", "message": "Existing Glue table is not marked as ICEBERG."})
                  table_readback_ok = len(blockers) == 0
              except ClientError as exc:
                  if exc.response.get("Error", {}).get("Code") == "EntityNotFoundException":
                      try:
                          glue.create_table(
                              DatabaseName=database_name,
                              TableInput={
                                  "Name": table_name,
                                  "Description": "Dev-full OFS Iceberg table closure surface (managed M10.F lane).",
                                  "TableType": "EXTERNAL_TABLE",
                                  "Parameters": {
                                      "EXTERNAL": "TRUE",
                                      "table_type": "ICEBERG",
                                      "classification": "iceberg",
                                      "platform_run_id": platform_run_id,
                                      "m10f_execution_id": execution_id,
                                      "upstream_m10e_execution": upstream_m10e_execution,
                                  },
                                  "StorageDescriptor": {
                                      "Columns": [
                                          {"Name": "record_id", "Type": "string"},
                                          {"Name": "platform_run_id", "Type": "string"},
                                      ],
                                      "Location": location_uri,
                                      "InputFormat": "org.apache.hadoop.mapred.FileInputFormat",
                                      "OutputFormat": "org.apache.hadoop.mapred.FileOutputFormat",
                                      "SerdeInfo": {
                                          "SerializationLibrary": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
                                          "Parameters": {"serialization.format": "1"},
                                      },
                                  },
                              },
                          )
                          table_payload = glue.get_table(DatabaseName=database_name, Name=table_name)
                          table_obj = table_payload.get("Table", {}) if isinstance(table_payload, dict) else {}
                          table_parameters = dict(table_obj.get("Parameters", {}) or {})
                          table_readback_ok = True
                          created_table = True
                      except (ClientError, BotoCoreError) as create_exc:
                          read_errors.append({"surface": f"glue.table.{database_name}.{table_name}", "error": type(create_exc).__name__})
                          blockers.append({"code": "M10-B6", "message": "Unable to create/read Glue table for OFS Iceberg commit."})
                  else:
                      read_errors.append({"surface": f"glue.table.{database_name}.{table_name}", "error": type(exc).__name__})
                      blockers.append({"code": "M10-B6", "message": "Glue table read failed for OFS Iceberg commit."})
              except (BotoCoreError, ClientError) as exc:
                  read_errors.append({"surface": f"glue.table.{database_name}.{table_name}", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B6", "message": "Glue table read failed for OFS Iceberg commit."})

          if not blockers:
              marker_payload = {
                  "captured_at_utc": captured_at,
                  "phase": "M10.F",
                  "execution_id": execution_id,
                  "platform_run_id": platform_run_id,
                  "scenario_run_id": scenario_run_id,
                  "database_name": database_name,
                  "table_name": table_name,
                  "location_uri": location_uri,
                  "upstream_m10e_execution": upstream_m10e_execution,
              }
              try:
                  s3_put_json(s3, object_store_bucket, marker_key, marker_payload)
                  marker_written = True
                  marker_readback_ok = True
              except (ClientError, BotoCoreError, ValueError) as exc:
                  read_errors.append({"surface": f"s3://{object_store_bucket}/{marker_key}", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B6", "message": "Unable to write/read OFS warehouse commit marker."})

          blockers = dedupe_blockers(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M10.G_READY" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M10.F",
              "phase_id": "P13",
              "execution_id": execution_id,
              "upstream_m10e_execution": upstream_m10e_execution,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "iceberg_commit_surface": {
                  "database_name": database_name,
                  "table_name": table_name,
                  "location_uri": location_uri,
                  "marker_object_uri": f"s3://{object_store_bucket}/{marker_key}" if marker_key else "",
                  "created_database": created_database,
                  "created_table": created_table,
                  "marker_written": marker_written,
                  "db_readback_ok": db_readback_ok,
                  "table_readback_ok": table_readback_ok,
                  "marker_readback_ok": marker_readback_ok,
                  "table_parameters": table_parameters,
                  "handles": {
                      "S3_OBJECT_STORE_BUCKET": object_store_bucket,
                      "OFS_ICEBERG_DATABASE": database_name,
                      "OFS_ICEBERG_TABLE_PREFIX": table_prefix,
                      "OFS_ICEBERG_WAREHOUSE_PREFIX_PATTERN": warehouse_prefix,
                  },
              },
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M10.F",
              "execution_id": execution_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "read_errors": read_errors,
              "upload_errors": upload_errors,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "M10.F",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }

          artifacts = {
              "m10f_iceberg_commit_snapshot.json": snapshot,
              "m10f_blocker_register.json": blocker_register,
              "m10f_execution_summary.json": summary,
          }
          write_local(run_dir, artifacts)
          prefix = f"evidence/dev_full/run_control/{execution_id}/"
          for name, payload in artifacts.items():
              key = f"{prefix}{name}"
              try:
                  s3_put_json(s3, evidence_bucket, key, payload)
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})

          if upload_errors:
              blockers.append({"code": "M10-B12", "message": "Failed to publish/readback one or more M10.F artifacts."})
              blockers = dedupe_blockers(blockers)
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers)
              summary["next_gate"] = "HOLD_REMEDIATE"
              blocker_register["blocker_count"] = len(blockers)
              blocker_register["blockers"] = blockers
              blocker_register["upload_errors"] = upload_errors
              snapshot["overall_pass"] = False
              snapshot["blocker_count"] = len(blockers)
              snapshot["next_gate"] = "HOLD_REMEDIATE"
              write_local(run_dir, artifacts)

          print(json.dumps({
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{prefix}",
          }, ensure_ascii=True))

          if not summary["overall_pass"]:
              raise SystemExit(1)
          PY

      - name: Upload M10.D managed artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m10-def-managed-${{ steps.run_meta.outputs.timestamp }}
          path: |
            ${{ steps.run_meta.outputs.m10d_run_dir }}
            ${{ steps.run_meta.outputs.m10e_run_dir }}
            ${{ steps.run_meta.outputs.m10f_run_dir }}
          if-no-files-found: warn
