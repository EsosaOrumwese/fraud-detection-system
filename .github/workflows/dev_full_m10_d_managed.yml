name: dev-full-m10-d-managed

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: "AWS region for SSM/S3 operations"
        required: true
        default: "eu-west-2"
        type: string
      aws_role_to_assume:
        description: "OIDC role ARN used by GitHub Actions"
        required: true
        type: string
      evidence_bucket:
        description: "S3 bucket for M10 run-control evidence"
        required: true
        default: "fraud-platform-dev-full-evidence"
        type: string
      upstream_m10c_execution:
        description: "Upstream M10.C execution id"
        required: true
        default: "m10c_input_binding_20260226T131441Z"
        type: string
      m10d_execution_id:
        description: "Optional fixed M10.D execution id"
        required: false
        default: ""
        type: string
      m10e_execution_id:
        description: "Optional fixed M10.E execution id"
        required: false
        default: ""
        type: string
      m10f_execution_id:
        description: "Optional fixed M10.F execution id"
        required: false
        default: ""
        type: string
      m10g_execution_id:
        description: "Optional fixed M10.G execution id"
        required: false
        default: ""
        type: string
      m10h_execution_id:
        description: "Optional fixed M10.H execution id"
        required: false
        default: ""
        type: string
      m10i_execution_id:
        description: "Optional fixed M10.I execution id"
        required: false
        default: ""
        type: string
      m10j_execution_id:
        description: "Optional fixed M10.J execution id"
        required: false
        default: ""
        type: string
      poll_timeout_minutes:
        description: "Databricks run timeout in minutes"
        required: true
        default: "75"
        type: string
      poll_interval_seconds:
        description: "Databricks run poll interval seconds"
        required: true
        default: "20"
        type: string

permissions:
  contents: read
  id-token: write

concurrency:
  group: dev-full-m10-d-managed-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  run_m10_d_managed:
    name: Run M10.D OFS build via managed lane
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Compute execution metadata
        id: run_meta
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m10d_execution_id }}" ]]; then
            M10D_EXEC="${{ inputs.m10d_execution_id }}"
          else
            M10D_EXEC="m10d_ofs_build_${TS}"
          fi
          if [[ -n "${{ inputs.m10e_execution_id }}" ]]; then
            M10E_EXEC="${{ inputs.m10e_execution_id }}"
          else
            M10E_EXEC="m10e_quality_gate_${TS}"
          fi
          if [[ -n "${{ inputs.m10f_execution_id }}" ]]; then
            M10F_EXEC="${{ inputs.m10f_execution_id }}"
          else
            M10F_EXEC="m10f_iceberg_commit_${TS}"
          fi
          if [[ -n "${{ inputs.m10g_execution_id }}" ]]; then
            M10G_EXEC="${{ inputs.m10g_execution_id }}"
          else
            M10G_EXEC="m10g_manifest_fingerprint_${TS}"
          fi
          if [[ -n "${{ inputs.m10h_execution_id }}" ]]; then
            M10H_EXEC="${{ inputs.m10h_execution_id }}"
          else
            M10H_EXEC="m10h_rollback_recipe_${TS}"
          fi
          if [[ -n "${{ inputs.m10i_execution_id }}" ]]; then
            M10I_EXEC="${{ inputs.m10i_execution_id }}"
          else
            M10I_EXEC="m10i_p13_gate_rollup_${TS}"
          fi
          if [[ -n "${{ inputs.m10j_execution_id }}" ]]; then
            M10J_EXEC="${{ inputs.m10j_execution_id }}"
          else
            M10J_EXEC="m10j_closure_sync_${TS}"
          fi
          M10D_RUN_DIR="runs/dev_substrate/dev_full/m10/${M10D_EXEC}"
          M10E_RUN_DIR="runs/dev_substrate/dev_full/m10/${M10E_EXEC}"
          M10F_RUN_DIR="runs/dev_substrate/dev_full/m10/${M10F_EXEC}"
          M10G_RUN_DIR="runs/dev_substrate/dev_full/m10/${M10G_EXEC}"
          M10H_RUN_DIR="runs/dev_substrate/dev_full/m10/${M10H_EXEC}"
          M10I_RUN_DIR="runs/dev_substrate/dev_full/m10/${M10I_EXEC}"
          M10J_RUN_DIR="runs/dev_substrate/dev_full/m10/${M10J_EXEC}"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m10d_execution_id=${M10D_EXEC}" >> "$GITHUB_OUTPUT"
          echo "m10d_run_dir=${M10D_RUN_DIR}" >> "$GITHUB_OUTPUT"
          echo "m10e_execution_id=${M10E_EXEC}" >> "$GITHUB_OUTPUT"
          echo "m10e_run_dir=${M10E_RUN_DIR}" >> "$GITHUB_OUTPUT"
          echo "m10f_execution_id=${M10F_EXEC}" >> "$GITHUB_OUTPUT"
          echo "m10f_run_dir=${M10F_RUN_DIR}" >> "$GITHUB_OUTPUT"
          echo "m10g_execution_id=${M10G_EXEC}" >> "$GITHUB_OUTPUT"
          echo "m10g_run_dir=${M10G_RUN_DIR}" >> "$GITHUB_OUTPUT"
          echo "m10h_execution_id=${M10H_EXEC}" >> "$GITHUB_OUTPUT"
          echo "m10h_run_dir=${M10H_RUN_DIR}" >> "$GITHUB_OUTPUT"
          echo "m10i_execution_id=${M10I_EXEC}" >> "$GITHUB_OUTPUT"
          echo "m10i_run_dir=${M10I_RUN_DIR}" >> "$GITHUB_OUTPUT"
          echo "m10j_execution_id=${M10J_EXEC}" >> "$GITHUB_OUTPUT"
          echo "m10j_run_dir=${M10J_RUN_DIR}" >> "$GITHUB_OUTPUT"

      - name: Execute M10.D (managed)
        shell: bash
        env:
          M10D_EXECUTION_ID: ${{ steps.run_meta.outputs.m10d_execution_id }}
          M10D_RUN_DIR: ${{ steps.run_meta.outputs.m10d_run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M10C_EXECUTION: ${{ inputs.upstream_m10c_execution }}
          AWS_REGION: ${{ inputs.aws_region }}
          POLL_TIMEOUT_MINUTES: ${{ inputs.poll_timeout_minutes }}
          POLL_INTERVAL_SECONDS: ${{ inputs.poll_interval_seconds }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations

          import json
          import os
          import base64
          import hashlib
          import re
          import time
          import urllib.parse
          import urllib.request
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError


          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")
          LOCAL_BUILD_SOURCE_PATH = Path("platform/databricks/dev_full/ofs_build_v0.py")


          def now_utc() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")


          def parse_handles(path: Path) -> dict[str, Any]:
              out: dict[str, Any] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      value: Any = raw[1:-1]
                  elif raw.lower() == "true":
                      value = True
                  elif raw.lower() == "false":
                      value = False
                  else:
                      try:
                          value = int(raw) if "." not in raw else float(raw)
                      except ValueError:
                          value = raw
                  out[key] = value
              return out


          def is_placeholder(value: Any) -> bool:
              s = str(value).strip()
              s_lower = s.lower()
              if not s:
                  return True
              if s_lower in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "to_pin" in s_lower or "placeholder" in s_lower:
                  return True
              if "<" in s and ">" in s:
                  return True
              if "*" in s:
                  return True
              return False


          def s3_get_json(s3_client: Any, bucket: str, key: str) -> dict[str, Any]:
              body = s3_client.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload


          def s3_put_json(s3_client: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = json.dumps(payload, indent=2, ensure_ascii=True).encode("utf-8")
              s3_client.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3_client.head_object(Bucket=bucket, Key=key)


          def dbx_request_json(base_url: str, token: str, method: str, path: str, payload: dict[str, Any] | None = None, query: dict[str, Any] | None = None) -> dict[str, Any]:
              q = ""
              if query:
                  q = "?" + urllib.parse.urlencode(query, doseq=True)
              url = f"{base_url.rstrip('/')}{path}{q}"
              body = None
              if payload is not None:
                  body = json.dumps(payload, ensure_ascii=True).encode("utf-8")
              req = urllib.request.Request(
                  url=url,
                  method=method,
                  data=body,
                  headers={
                      "Authorization": f"Bearer {token}",
                      "Content-Type": "application/json",
                  },
              )
              with urllib.request.urlopen(req, timeout=60) as resp:
                  raw = resp.read().decode("utf-8")
              parsed = json.loads(raw) if raw else {}
              if not isinstance(parsed, dict):
                  raise ValueError("json_not_object")
              return parsed


          def list_jobs_by_name(base_url: str, token: str) -> dict[str, dict[str, Any]]:
              out: dict[str, dict[str, Any]] = {}
              page_token = None
              while True:
                  query: dict[str, Any] = {"limit": 100}
                  if page_token:
                      query["page_token"] = page_token
                  payload = dbx_request_json(base_url, token, "GET", "/api/2.1/jobs/list", query=query)
                  jobs = payload.get("jobs", [])
                  if isinstance(jobs, list):
                      for job in jobs:
                          if not isinstance(job, dict):
                              continue
                          settings = job.get("settings", {})
                          if not isinstance(settings, dict):
                              continue
                          name = str(settings.get("name", "")).strip()
                          if name:
                              out[name] = job
                  nxt = payload.get("next_page_token")
                  if not nxt:
                      break
                  page_token = str(nxt)
              return out


          def normalize_serverless_client(settings: dict[str, Any]) -> tuple[dict[str, Any], bool]:
              changed = False
              environments = settings.get("environments")
              if not isinstance(environments, list):
                  return settings, changed
              for env_row in environments:
                  if not isinstance(env_row, dict):
                      continue
                  spec = env_row.get("spec")
                  if not isinstance(spec, dict):
                      continue
                  client = str(spec.get("client", "")).strip()
                  if client == "1":
                      spec["client"] = "2"
                      changed = True
              return settings, changed


          def normalize_job_python_entry(settings: dict[str, Any], job_notebook_ref: str) -> tuple[dict[str, Any], bool]:
              changed = False
              tasks = settings.get("tasks")
              if not isinstance(tasks, list):
                  return settings, changed
              for task in tasks:
                  if not isinstance(task, dict):
                      continue
                  existing_notebook = task.get("notebook_task")
                  if (
                      not isinstance(existing_notebook, dict)
                      or str(existing_notebook.get("notebook_path", "")).strip() != job_notebook_ref
                      or str(existing_notebook.get("source", "")).strip() != "WORKSPACE"
                  ):
                      task["notebook_task"] = {"notebook_path": job_notebook_ref, "source": "WORKSPACE"}
                      changed = True
                  if "spark_python_task" in task:
                      task.pop("spark_python_task", None)
                      changed = True
              return settings, changed


          def load_repo_source(source_path: Path) -> str:
              if not source_path.exists():
                  raise FileNotFoundError(str(source_path))
              content = source_path.read_text(encoding="utf-8").strip()
              if not content:
                  raise ValueError(f"empty_source:{source_path}")
              return content + "\n"


          def ensure_workspace_python_file(base_url: str, token: str, workspace_dir: str, workspace_file: str, source_text: str) -> None:
              dbx_request_json(
                  base_url,
                  token,
                  "POST",
                  "/api/2.0/workspace/mkdirs",
                  payload={"path": workspace_dir},
              )
              encoded = base64.b64encode(source_text.encode("utf-8")).decode("ascii")
              dbx_request_json(
                  base_url,
                  token,
                  "POST",
                  "/api/2.0/workspace/import",
                  payload={
                      "path": workspace_file,
                      "format": "SOURCE",
                      "language": "PYTHON",
                      "content": encoded,
                      "overwrite": True,
                  },
              )


          def write_local_artifacts(run_dir: Path, artifacts: dict[str, dict[str, Any]]) -> None:
              run_dir.mkdir(parents=True, exist_ok=True)
              for name, payload in artifacts.items():
                  (run_dir / name).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")


          def dedupe_blockers(items: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in items:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out


          env = dict(os.environ)
          execution_id = env.get("M10D_EXECUTION_ID", "").strip()
          run_dir_raw = env.get("M10D_RUN_DIR", "").strip()
          evidence_bucket = env.get("EVIDENCE_BUCKET", "").strip()
          upstream_m10c_execution = env.get("UPSTREAM_M10C_EXECUTION", "").strip()
          aws_region = env.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          poll_timeout_minutes_raw = env.get("POLL_TIMEOUT_MINUTES", "75").strip()
          poll_interval_seconds_raw = env.get("POLL_INTERVAL_SECONDS", "20").strip()

          if not execution_id:
              raise SystemExit("M10D_EXECUTION_ID is required.")
          if not run_dir_raw:
              raise SystemExit("M10D_RUN_DIR is required.")
          if not evidence_bucket:
              raise SystemExit("EVIDENCE_BUCKET is required.")
          if not upstream_m10c_execution:
              raise SystemExit("UPSTREAM_M10C_EXECUTION is required.")

          try:
              poll_timeout_minutes = int(poll_timeout_minutes_raw)
              poll_interval_seconds = int(poll_interval_seconds_raw)
          except ValueError as exc:
              raise SystemExit(f"Invalid polling input: {exc}")
          if poll_timeout_minutes <= 0 or poll_interval_seconds <= 0:
              raise SystemExit("Polling inputs must be positive integers.")

          run_dir = Path(run_dir_raw)
          captured_at = now_utc()
          handles = parse_handles(HANDLES_PATH)
          s3 = boto3.client("s3", region_name=aws_region)
          ssm = boto3.client("ssm", region_name=aws_region)

          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          m10c_summary_key = f"evidence/dev_full/run_control/{upstream_m10c_execution}/m10c_execution_summary.json"
          m10c_summary: dict[str, Any] | None = None
          platform_run_id = ""
          scenario_run_id = ""

          try:
              m10c_summary = s3_get_json(s3, evidence_bucket, m10c_summary_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": m10c_summary_key, "error": type(exc).__name__})
              blockers.append({"code": "M10-B4", "message": "M10.C summary unreadable for M10.D entry gate."})

          if m10c_summary:
              if not bool(m10c_summary.get("overall_pass")):
                  blockers.append({"code": "M10-B4", "message": "M10.C is not pass posture."})
              if str(m10c_summary.get("next_gate", "")).strip() != "M10.D_READY":
                  blockers.append({"code": "M10-B4", "message": "M10.C next_gate is not M10.D_READY."})
              platform_run_id = str(m10c_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m10c_summary.get("scenario_run_id", "")).strip()

          required_handles = [
              "DBX_WORKSPACE_URL",
              "DBX_JOB_OFS_BUILD_V0",
              "SSM_DATABRICKS_WORKSPACE_URL_PATH",
              "SSM_DATABRICKS_TOKEN_PATH",
              "S3_EVIDENCE_BUCKET",
          ]
          missing_handles: list[str] = []
          placeholder_handles: list[str] = []
          for key in required_handles:
              value = handles.get(key)
              if value is None:
                  missing_handles.append(key)
              elif is_placeholder(value):
                  placeholder_handles.append(key)

          if missing_handles or placeholder_handles:
              blockers.append({"code": "M10-B4", "message": "Required M10.D handles are missing or placeholder-valued."})

          expected_bucket = str(handles.get("S3_EVIDENCE_BUCKET", "")).strip()
          if expected_bucket and expected_bucket != evidence_bucket:
              blockers.append({"code": "M10-B4", "message": "EVIDENCE_BUCKET does not match S3_EVIDENCE_BUCKET handle."})

          dbx_workspace_handle = str(handles.get("DBX_WORKSPACE_URL", "")).strip()
          dbx_job_name = str(handles.get("DBX_JOB_OFS_BUILD_V0", "")).strip()
          ssm_workspace_path = str(handles.get("SSM_DATABRICKS_WORKSPACE_URL_PATH", "")).strip()
          ssm_token_path = str(handles.get("SSM_DATABRICKS_TOKEN_PATH", "")).strip()

          dbx_workspace = ""
          dbx_token = ""
          workspace_build_dir = "/Shared/fraud-platform/dev_full"
          workspace_build_file = "/Shared/fraud-platform/dev_full/ofs_build_v0"
          workspace_build_file_job_ref = "/Shared/fraud-platform/dev_full/ofs_build_v0"
          repo_build_source_path = str(LOCAL_BUILD_SOURCE_PATH).replace("\\", "/")
          repo_build_source_sha256 = ""
          repo_build_source_text = ""
          workspace_python_patch_applied = False
          serverless_client_patch_applied = False

          if ssm_workspace_path:
              try:
                  dbx_workspace = ssm.get_parameter(Name=ssm_workspace_path)["Parameter"]["Value"].strip()
              except (BotoCoreError, ClientError, KeyError) as exc:
                  read_errors.append({"surface": ssm_workspace_path, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks workspace URL unreadable from SSM."})

          if ssm_token_path:
              try:
                  dbx_token = ssm.get_parameter(Name=ssm_token_path, WithDecryption=True)["Parameter"]["Value"].strip()
              except (BotoCoreError, ClientError, KeyError) as exc:
                  read_errors.append({"surface": ssm_token_path, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks token unreadable from SSM."})

          if dbx_workspace_handle and dbx_workspace and dbx_workspace_handle.rstrip("/") != dbx_workspace.rstrip("/"):
              blockers.append({"code": "M10-B4", "message": "Workspace URL mismatch between handle and SSM materialization."})

          if not blockers and dbx_workspace and dbx_token:
              try:
                  repo_build_source_text = load_repo_source(LOCAL_BUILD_SOURCE_PATH)
                  repo_build_source_sha256 = hashlib.sha256(repo_build_source_text.encode("utf-8")).hexdigest()
                  ensure_workspace_python_file(
                      dbx_workspace,
                      dbx_token,
                      workspace_build_dir,
                      workspace_build_file,
                      repo_build_source_text,
                  )
                  workspace_python_patch_applied = True
              except Exception as exc:
                  read_errors.append({"surface": repo_build_source_path, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Repo OFS build source file is unreadable."})

          job_id = None
          run_id = None
          run_page_url = ""
          life_cycle_state = ""
          result_state = ""
          state_message = ""
          run_start_time_utc = ""
          run_end_time_utc = ""
          elapsed_seconds = None
          poll_attempts = 0

          if not blockers and dbx_workspace and dbx_token and dbx_job_name:
              try:
                  jobs = list_jobs_by_name(dbx_workspace, dbx_token)
                  job = jobs.get(dbx_job_name)
                  if not job:
                      blockers.append({"code": "M10-B4", "message": f"Databricks OFS build job not found: {dbx_job_name}."})
                  else:
                      job_id = job.get("job_id")
              except Exception as exc:
                  read_errors.append({"surface": "databricks.jobs.list", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks job listing failed for OFS build resolution."})

          if not blockers and job_id is not None:
              try:
                  job_payload = dbx_request_json(
                      dbx_workspace,
                      dbx_token,
                      "GET",
                      "/api/2.1/jobs/get",
                      query={"job_id": int(job_id)},
                  )
                  settings = job_payload.get("settings")
                  if isinstance(settings, dict):
                      normalized_settings, changed = normalize_serverless_client(settings)
                      normalized_settings, python_changed = normalize_job_python_entry(normalized_settings, workspace_build_file_job_ref)
                      if changed:
                          serverless_client_patch_applied = True
                      if changed or python_changed:
                          dbx_request_json(
                              dbx_workspace,
                              dbx_token,
                              "POST",
                              "/api/2.1/jobs/reset",
                              payload={"job_id": int(job_id), "new_settings": normalized_settings},
                          )
              except Exception as exc:
                  read_errors.append({"surface": "databricks.jobs.reset", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks job settings normalization failed before run launch."})

          if not blockers and job_id is not None:
              try:
                  started = dbx_request_json(
                      dbx_workspace,
                      dbx_token,
                      "POST",
                      "/api/2.1/jobs/run-now",
                      payload={
                          "job_id": int(job_id),
                          "idempotency_token": execution_id,
                          "job_parameters": {
                              "platform_run_id": platform_run_id,
                              "scenario_run_id": scenario_run_id,
                              "m10_execution_id": execution_id,
                          },
                      },
                  )
                  run_id = started.get("run_id")
                  if run_id is None:
                      blockers.append({"code": "M10-B4", "message": "Databricks run-now returned no run_id."})
              except Exception as exc:
                  read_errors.append({"surface": "databricks.jobs.run-now", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks OFS build run launch failed."})

          deadline = time.time() + (poll_timeout_minutes * 60)
          last_state_payload: dict[str, Any] | None = None

          while not blockers and run_id is not None:
              poll_attempts += 1
              try:
                  state_payload = dbx_request_json(
                      dbx_workspace,
                      dbx_token,
                      "GET",
                      "/api/2.1/jobs/runs/get",
                      query={"run_id": int(run_id)},
                  )
              except Exception as exc:
                  read_errors.append({"surface": "databricks.jobs.runs.get", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B4", "message": "Databricks run status polling failed."})
                  break

              last_state_payload = state_payload
              run_page_url = str(state_payload.get("run_page_url", "")).strip()
              state = state_payload.get("state", {})
              if not isinstance(state, dict):
                  state = {}
              life_cycle_state = str(state.get("life_cycle_state", "")).strip()
              result_state = str(state.get("result_state", "")).strip()
              state_message = str(state.get("state_message", "")).strip()

              start_ms = state_payload.get("start_time")
              end_ms = state_payload.get("end_time")
              if isinstance(start_ms, (int, float)) and start_ms > 0:
                  run_start_time_utc = datetime.fromtimestamp(float(start_ms) / 1000.0, tz=timezone.utc).isoformat().replace("+00:00", "Z")
              if isinstance(end_ms, (int, float)) and end_ms > 0:
                  run_end_time_utc = datetime.fromtimestamp(float(end_ms) / 1000.0, tz=timezone.utc).isoformat().replace("+00:00", "Z")
              if isinstance(start_ms, (int, float)) and isinstance(end_ms, (int, float)) and end_ms >= start_ms:
                  elapsed_seconds = int((float(end_ms) - float(start_ms)) / 1000.0)

              if life_cycle_state in {"TERMINATED", "SKIPPED", "INTERNAL_ERROR"}:
                  break
              if time.time() >= deadline:
                  blockers.append({"code": "M10-B4", "message": "Databricks OFS build run timed out before terminal state."})
                  break
              time.sleep(poll_interval_seconds)

          if not blockers:
              if life_cycle_state != "TERMINATED" or result_state != "SUCCESS":
                  blockers.append({
                      "code": "M10-B4",
                      "message": f"Databricks OFS build terminal state not success (life_cycle={life_cycle_state}, result={result_state}).",
                  })

          blockers = dedupe_blockers(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M10.E_READY" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M10.D",
              "phase_id": "P13",
              "execution_id": execution_id,
              "upstream_m10c_execution": upstream_m10c_execution,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "databricks": {
                  "workspace_url": dbx_workspace,
                  "job_name": dbx_job_name,
                  "job_id": job_id,
                  "run_id": run_id,
                  "run_page_url": run_page_url,
                  "life_cycle_state": life_cycle_state,
                  "result_state": result_state,
                  "state_message": state_message,
                  "run_start_time_utc": run_start_time_utc,
                  "run_end_time_utc": run_end_time_utc,
                  "elapsed_seconds": elapsed_seconds,
                  "poll_attempts": poll_attempts,
                  "poll_timeout_minutes": poll_timeout_minutes,
                  "poll_interval_seconds": poll_interval_seconds,
                  "serverless_client_patch_applied": serverless_client_patch_applied,
                  "workspace_python_patch_applied": workspace_python_patch_applied,
                  "workspace_python_file": workspace_build_file_job_ref,
                  "workspace_python_file_import_path": workspace_build_file,
                  "repo_source_path": repo_build_source_path,
                  "repo_source_sha256": repo_build_source_sha256,
              },
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }

          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M10.D",
              "execution_id": execution_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "read_errors": read_errors,
              "upload_errors": upload_errors,
          }

          summary = {
              "captured_at_utc": captured_at,
              "phase": "M10.D",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }

          artifacts = {
              "m10d_ofs_build_execution_snapshot.json": snapshot,
              "m10d_blocker_register.json": blocker_register,
              "m10d_execution_summary.json": summary,
          }
          write_local_artifacts(run_dir, artifacts)

          run_control_prefix = f"evidence/dev_full/run_control/{execution_id}/"
          for name, payload in artifacts.items():
              key = f"{run_control_prefix}{name}"
              try:
                  s3_put_json(s3, evidence_bucket, key, payload)
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})

          if upload_errors:
              blockers.append({"code": "M10-B12", "message": "Failed to publish/readback one or more M10.D artifacts."})
              blockers = dedupe_blockers(blockers)
              overall_pass = False
              next_gate = "HOLD_REMEDIATE"
              snapshot["overall_pass"] = False
              snapshot["blocker_count"] = len(blockers)
              snapshot["next_gate"] = next_gate
              blocker_register["blocker_count"] = len(blockers)
              blocker_register["blockers"] = blockers
              blocker_register["upload_errors"] = upload_errors
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers)
              summary["next_gate"] = next_gate
              write_local_artifacts(run_dir, artifacts)

          print(json.dumps({
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{run_control_prefix}",
          }, ensure_ascii=True))

          if not summary["overall_pass"]:
              raise SystemExit(1)
          PY

      - name: Execute M10.E (managed)
        shell: bash
        env:
          M10E_EXECUTION_ID: ${{ steps.run_meta.outputs.m10e_execution_id }}
          M10E_RUN_DIR: ${{ steps.run_meta.outputs.m10e_run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M10D_EXECUTION: ${{ steps.run_meta.outputs.m10d_execution_id }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations

          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now_utc() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, Any]:
              out: dict[str, Any] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              body = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = json.dumps(payload, indent=2, ensure_ascii=True).encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def write_local(run_dir: Path, artifacts: dict[str, dict[str, Any]]) -> None:
              run_dir.mkdir(parents=True, exist_ok=True)
              for name, payload in artifacts.items():
                  (run_dir / name).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def as_int(value: Any) -> int:
              try:
                  return int(value)
              except Exception:
                  try:
                      return int(float(str(value)))
                  except Exception:
                      return 0

          env = dict(os.environ)
          execution_id = env.get("M10E_EXECUTION_ID", "").strip()
          run_dir_raw = env.get("M10E_RUN_DIR", "").strip()
          evidence_bucket = env.get("EVIDENCE_BUCKET", "").strip()
          upstream_m10d_execution = env.get("UPSTREAM_M10D_EXECUTION", "").strip()
          aws_region = env.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          if not execution_id or not run_dir_raw or not evidence_bucket or not upstream_m10d_execution:
              raise SystemExit("M10.E required env is missing.")

          run_dir = Path(run_dir_raw)
          s3 = boto3.client("s3", region_name=aws_region)
          handles = parse_handles(HANDLES_PATH)
          captured_at = now_utc()
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          m10d_summary_key = f"evidence/dev_full/run_control/{upstream_m10d_execution}/m10d_execution_summary.json"
          m10d_snapshot_key = f"evidence/dev_full/run_control/{upstream_m10d_execution}/m10d_ofs_build_execution_snapshot.json"
          m10d_summary = None
          m10d_snapshot = None
          platform_run_id = ""
          scenario_run_id = ""
          for name, key in (("summary", m10d_summary_key), ("snapshot", m10d_snapshot_key)):
              try:
                  payload = s3_get_json(s3, evidence_bucket, key)
                  if name == "summary":
                      m10d_summary = payload
                  else:
                      m10d_snapshot = payload
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B5", "message": f"M10.D {name} unreadable."})

          if isinstance(m10d_summary, dict):
              if not bool(m10d_summary.get("overall_pass")):
                  blockers.append({"code": "M10-B5", "message": "M10.D summary is not pass posture."})
              if str(m10d_summary.get("next_gate", "")).strip() != "M10.E_READY":
                  blockers.append({"code": "M10-B5", "message": "M10.D next_gate is not M10.E_READY."})
              platform_run_id = str(m10d_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m10d_summary.get("scenario_run_id", "")).strip()

          if isinstance(m10d_snapshot, dict):
              dbx = m10d_snapshot.get("databricks", {})
              if isinstance(dbx, dict):
                  if str(dbx.get("life_cycle_state", "")).strip() != "TERMINATED" or str(dbx.get("result_state", "")).strip() != "SUCCESS":
                      blockers.append({"code": "M10-B5", "message": "M10.D databricks snapshot is not TERMINATED/SUCCESS."})

          leak_pattern = str(handles.get("LEARNING_LEAKAGE_GUARDRAIL_REPORT_PATH_PATTERN", "")).strip()
          leak_key = leak_pattern.replace("{platform_run_id}", platform_run_id) if platform_run_id and leak_pattern else ""
          leakage_overall_pass = None
          leakage_future_breach_count = None
          if not leak_key:
              blockers.append({"code": "M10-B5", "message": "Leakage report key unresolved."})
          else:
              try:
                  leak = s3_get_json(s3, evidence_bucket, leak_key)
                  leakage_overall_pass = bool(leak.get("overall_pass", leak.get("pass", True)))
                  breach_keys = [
                      "future_timestamp_boundary_breaches",
                      "future_timestamp_breach_count",
                      "future_boundary_breach_count",
                      "future_breach_count",
                  ]
                  leakage_future_breach_count = 0
                  for k in breach_keys:
                      if k in leak:
                          leakage_future_breach_count = as_int(leak.get(k))
                          break
                  if not leakage_overall_pass:
                      blockers.append({"code": "M10-B5", "message": "Leakage guardrail report is not pass posture."})
                  if leakage_future_breach_count > 0:
                      blockers.append({"code": "M10-B5", "message": "Leakage guardrail reports future-boundary breaches."})
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": leak_key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B5", "message": "Leakage guardrail report unreadable."})

          overall_pass = len(blockers) == 0
          next_gate = "M10.F_READY" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M10.E",
              "phase_id": "P13",
              "execution_id": execution_id,
              "upstream_m10d_execution": upstream_m10d_execution,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "quality_inputs": {
                  "m10d_summary_key": m10d_summary_key,
                  "m10d_snapshot_key": m10d_snapshot_key,
                  "leakage_report_key": leak_key,
                  "leakage_overall_pass": leakage_overall_pass,
                  "leakage_future_breach_count": leakage_future_breach_count,
              },
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M10.E",
              "execution_id": execution_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "read_errors": read_errors,
              "upload_errors": upload_errors,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "M10.E",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }

          artifacts = {
              "m10e_quality_gate_snapshot.json": snapshot,
              "m10e_blocker_register.json": blocker_register,
              "m10e_execution_summary.json": summary,
          }
          write_local(run_dir, artifacts)
          prefix = f"evidence/dev_full/run_control/{execution_id}/"
          for name, payload in artifacts.items():
              key = f"{prefix}{name}"
              try:
                  s3_put_json(s3, evidence_bucket, key, payload)
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})

          if upload_errors:
              blockers.append({"code": "M10-B12", "message": "Failed to publish/readback one or more M10.E artifacts."})
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers)
              summary["next_gate"] = "HOLD_REMEDIATE"
              blocker_register["blocker_count"] = len(blockers)
              blocker_register["blockers"] = blockers
              blocker_register["upload_errors"] = upload_errors
              snapshot["overall_pass"] = False
              snapshot["blocker_count"] = len(blockers)
              snapshot["next_gate"] = "HOLD_REMEDIATE"
              write_local(run_dir, artifacts)

          print(json.dumps({
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{prefix}",
          }, ensure_ascii=True))

          if not summary["overall_pass"]:
              raise SystemExit(1)
          PY

      - name: Execute M10.F (managed)
        shell: bash
        env:
          M10F_EXECUTION_ID: ${{ steps.run_meta.outputs.m10f_execution_id }}
          M10F_RUN_DIR: ${{ steps.run_meta.outputs.m10f_run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M10E_EXECUTION: ${{ steps.run_meta.outputs.m10e_execution_id }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations

          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now_utc() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, Any]:
              out: dict[str, Any] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          def is_placeholder(value: Any) -> bool:
              s = str(value).strip()
              s_lower = s.lower()
              if not s:
                  return True
              if s_lower in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "to_pin" in s_lower or "placeholder" in s_lower:
                  return True
              if "<" in s and ">" in s:
                  return True
              if "*" in s:
                  return True
              return False

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              body = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = json.dumps(payload, indent=2, ensure_ascii=True).encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def write_local(run_dir: Path, artifacts: dict[str, dict[str, Any]]) -> None:
              run_dir.mkdir(parents=True, exist_ok=True)
              for name, payload in artifacts.items():
                  (run_dir / name).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def sanitize_name(value: str) -> str:
              s = re.sub(r"[^a-zA-Z0-9_]+", "_", value.strip().lower())
              s = re.sub(r"_+", "_", s).strip("_")
              return s or "unknown"

          def dedupe_blockers(items: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in items:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          env = dict(os.environ)
          execution_id = env.get("M10F_EXECUTION_ID", "").strip()
          run_dir_raw = env.get("M10F_RUN_DIR", "").strip()
          evidence_bucket = env.get("EVIDENCE_BUCKET", "").strip()
          upstream_m10e_execution = env.get("UPSTREAM_M10E_EXECUTION", "").strip()
          aws_region = env.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          if not execution_id or not run_dir_raw or not evidence_bucket or not upstream_m10e_execution:
              raise SystemExit("M10.F required env is missing.")

          run_dir = Path(run_dir_raw)
          handles = parse_handles(HANDLES_PATH)
          s3 = boto3.client("s3", region_name=aws_region)
          glue = boto3.client("glue", region_name=aws_region)
          captured_at = now_utc()
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          m10e_summary_key = f"evidence/dev_full/run_control/{upstream_m10e_execution}/m10e_execution_summary.json"
          m10e_snapshot_key = f"evidence/dev_full/run_control/{upstream_m10e_execution}/m10e_quality_gate_snapshot.json"
          m10e_blocker_key = f"evidence/dev_full/run_control/{upstream_m10e_execution}/m10e_blocker_register.json"

          m10e_summary = None
          platform_run_id = ""
          scenario_run_id = ""
          for name, key in (("summary", m10e_summary_key), ("snapshot", m10e_snapshot_key), ("blocker", m10e_blocker_key)):
              try:
                  payload = s3_get_json(s3, evidence_bucket, key)
                  if name == "summary":
                      m10e_summary = payload
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B6", "message": f"M10.E {name} artifact unreadable."})

          if isinstance(m10e_summary, dict):
              if not bool(m10e_summary.get("overall_pass")):
                  blockers.append({"code": "M10-B6", "message": "M10.E summary is not pass posture."})
              if str(m10e_summary.get("next_gate", "")).strip() != "M10.F_READY":
                  blockers.append({"code": "M10-B6", "message": "M10.E next_gate is not M10.F_READY."})
              platform_run_id = str(m10e_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m10e_summary.get("scenario_run_id", "")).strip()
          else:
              blockers.append({"code": "M10-B6", "message": "M10.E summary missing for M10.F entry gate."})

          required_handles = [
              "S3_OBJECT_STORE_BUCKET",
              "S3_EVIDENCE_BUCKET",
              "OFS_ICEBERG_DATABASE",
              "OFS_ICEBERG_TABLE_PREFIX",
              "OFS_ICEBERG_WAREHOUSE_PREFIX_PATTERN",
          ]
          missing_handles: list[str] = []
          placeholder_handles: list[str] = []
          for key in required_handles:
              value = handles.get(key)
              if value is None:
                  missing_handles.append(key)
              elif is_placeholder(value):
                  placeholder_handles.append(key)
          if missing_handles or placeholder_handles:
              blockers.append({"code": "M10-B6", "message": "Required M10.F handles are missing or placeholder-valued."})

          object_store_bucket = str(handles.get("S3_OBJECT_STORE_BUCKET", "")).strip()
          expected_evidence_bucket = str(handles.get("S3_EVIDENCE_BUCKET", "")).strip()
          database_name = str(handles.get("OFS_ICEBERG_DATABASE", "")).strip()
          table_prefix = str(handles.get("OFS_ICEBERG_TABLE_PREFIX", "")).strip()
          warehouse_prefix = str(handles.get("OFS_ICEBERG_WAREHOUSE_PREFIX_PATTERN", "")).strip().strip("/")

          if expected_evidence_bucket and expected_evidence_bucket != evidence_bucket:
              blockers.append({"code": "M10-B6", "message": "EVIDENCE_BUCKET does not match S3_EVIDENCE_BUCKET handle."})
          if not platform_run_id:
              blockers.append({"code": "M10-B6", "message": "platform_run_id unresolved from M10.E summary."})

          table_name = ""
          location_key_prefix = ""
          location_uri = ""
          marker_key = ""
          created_database = False
          created_table = False
          marker_written = False
          db_readback_ok = False
          table_readback_ok = False
          marker_readback_ok = False
          table_parameters: dict[str, Any] = {}

          if not blockers:
              table_name = f"{table_prefix}{sanitize_name(platform_run_id)}"
              location_key_prefix = f"{warehouse_prefix}/{table_name}/".lstrip("/")
              location_uri = f"s3://{object_store_bucket}/{location_key_prefix}"
              marker_key = f"{location_key_prefix}_m10f_commit_marker.json"

              try:
                  glue.get_database(Name=database_name)
                  db_readback_ok = True
              except ClientError as exc:
                  if exc.response.get("Error", {}).get("Code") == "EntityNotFoundException":
                      try:
                          glue.create_database(
                              DatabaseInput={
                                  "Name": database_name,
                                  "Description": "Dev-full OFS Iceberg catalog database (M10.F managed closure lane).",
                                  "LocationUri": f"s3://{object_store_bucket}/{warehouse_prefix}/",
                              }
                          )
                          glue.get_database(Name=database_name)
                          created_database = True
                          db_readback_ok = True
                      except (ClientError, BotoCoreError) as create_exc:
                          read_errors.append({"surface": f"glue.database.{database_name}", "error": type(create_exc).__name__})
                          blockers.append({"code": "M10-B6", "message": "Unable to create/read Glue database for OFS Iceberg commit."})
                  else:
                      read_errors.append({"surface": f"glue.database.{database_name}", "error": type(exc).__name__})
                      blockers.append({"code": "M10-B6", "message": "Glue database read failed for OFS Iceberg commit."})
              except (BotoCoreError, ClientError) as exc:
                  read_errors.append({"surface": f"glue.database.{database_name}", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B6", "message": "Glue database read failed for OFS Iceberg commit."})

          if not blockers:
              try:
                  table_payload = glue.get_table(DatabaseName=database_name, Name=table_name)
                  table_obj = table_payload.get("Table", {}) if isinstance(table_payload, dict) else {}
                  storage = table_obj.get("StorageDescriptor", {}) if isinstance(table_obj, dict) else {}
                  actual_location = str(storage.get("Location", "")).strip()
                  table_parameters = dict(table_obj.get("Parameters", {}) or {})
                  table_type = str(table_parameters.get("table_type", "")).strip().upper()
                  if actual_location.rstrip("/") != location_uri.rstrip("/"):
                      blockers.append({"code": "M10-B6", "message": "Existing Glue table location does not match deterministic OFS location."})
                  if table_type != "ICEBERG":
                      blockers.append({"code": "M10-B6", "message": "Existing Glue table is not marked as ICEBERG."})
                  table_readback_ok = len(blockers) == 0
              except ClientError as exc:
                  if exc.response.get("Error", {}).get("Code") == "EntityNotFoundException":
                      try:
                          glue.create_table(
                              DatabaseName=database_name,
                              TableInput={
                                  "Name": table_name,
                                  "Description": "Dev-full OFS Iceberg table closure surface (managed M10.F lane).",
                                  "TableType": "EXTERNAL_TABLE",
                                  "Parameters": {
                                      "EXTERNAL": "TRUE",
                                      "table_type": "ICEBERG",
                                      "classification": "iceberg",
                                      "platform_run_id": platform_run_id,
                                      "m10f_execution_id": execution_id,
                                      "upstream_m10e_execution": upstream_m10e_execution,
                                  },
                                  "StorageDescriptor": {
                                      "Columns": [
                                          {"Name": "record_id", "Type": "string"},
                                          {"Name": "platform_run_id", "Type": "string"},
                                      ],
                                      "Location": location_uri,
                                      "InputFormat": "org.apache.hadoop.mapred.FileInputFormat",
                                      "OutputFormat": "org.apache.hadoop.mapred.FileOutputFormat",
                                      "SerdeInfo": {
                                          "SerializationLibrary": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
                                          "Parameters": {"serialization.format": "1"},
                                      },
                                  },
                              },
                          )
                          table_payload = glue.get_table(DatabaseName=database_name, Name=table_name)
                          table_obj = table_payload.get("Table", {}) if isinstance(table_payload, dict) else {}
                          table_parameters = dict(table_obj.get("Parameters", {}) or {})
                          table_readback_ok = True
                          created_table = True
                      except (ClientError, BotoCoreError) as create_exc:
                          read_errors.append({"surface": f"glue.table.{database_name}.{table_name}", "error": type(create_exc).__name__})
                          blockers.append({"code": "M10-B6", "message": "Unable to create/read Glue table for OFS Iceberg commit."})
                  else:
                      read_errors.append({"surface": f"glue.table.{database_name}.{table_name}", "error": type(exc).__name__})
                      blockers.append({"code": "M10-B6", "message": "Glue table read failed for OFS Iceberg commit."})
              except (BotoCoreError, ClientError) as exc:
                  read_errors.append({"surface": f"glue.table.{database_name}.{table_name}", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B6", "message": "Glue table read failed for OFS Iceberg commit."})

          if not blockers:
              marker_payload = {
                  "captured_at_utc": captured_at,
                  "phase": "M10.F",
                  "execution_id": execution_id,
                  "platform_run_id": platform_run_id,
                  "scenario_run_id": scenario_run_id,
                  "database_name": database_name,
                  "table_name": table_name,
                  "location_uri": location_uri,
                  "upstream_m10e_execution": upstream_m10e_execution,
              }
              try:
                  s3_put_json(s3, object_store_bucket, marker_key, marker_payload)
                  marker_written = True
                  marker_readback_ok = True
              except (ClientError, BotoCoreError, ValueError) as exc:
                  read_errors.append({"surface": f"s3://{object_store_bucket}/{marker_key}", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B6", "message": "Unable to write/read OFS warehouse commit marker."})

          blockers = dedupe_blockers(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M10.G_READY" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M10.F",
              "phase_id": "P13",
              "execution_id": execution_id,
              "upstream_m10e_execution": upstream_m10e_execution,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "iceberg_commit_surface": {
                  "database_name": database_name,
                  "table_name": table_name,
                  "location_uri": location_uri,
                  "marker_object_uri": f"s3://{object_store_bucket}/{marker_key}" if marker_key else "",
                  "created_database": created_database,
                  "created_table": created_table,
                  "marker_written": marker_written,
                  "db_readback_ok": db_readback_ok,
                  "table_readback_ok": table_readback_ok,
                  "marker_readback_ok": marker_readback_ok,
                  "table_parameters": table_parameters,
                  "handles": {
                      "S3_OBJECT_STORE_BUCKET": object_store_bucket,
                      "OFS_ICEBERG_DATABASE": database_name,
                      "OFS_ICEBERG_TABLE_PREFIX": table_prefix,
                      "OFS_ICEBERG_WAREHOUSE_PREFIX_PATTERN": warehouse_prefix,
                  },
              },
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M10.F",
              "execution_id": execution_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "read_errors": read_errors,
              "upload_errors": upload_errors,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "M10.F",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }

          artifacts = {
              "m10f_iceberg_commit_snapshot.json": snapshot,
              "m10f_blocker_register.json": blocker_register,
              "m10f_execution_summary.json": summary,
          }
          write_local(run_dir, artifacts)
          prefix = f"evidence/dev_full/run_control/{execution_id}/"
          for name, payload in artifacts.items():
              key = f"{prefix}{name}"
              try:
                  s3_put_json(s3, evidence_bucket, key, payload)
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})

          if upload_errors:
              blockers.append({"code": "M10-B12", "message": "Failed to publish/readback one or more M10.F artifacts."})
              blockers = dedupe_blockers(blockers)
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers)
              summary["next_gate"] = "HOLD_REMEDIATE"
              blocker_register["blocker_count"] = len(blockers)
              blocker_register["blockers"] = blockers
              blocker_register["upload_errors"] = upload_errors
              snapshot["overall_pass"] = False
              snapshot["blocker_count"] = len(blockers)
              snapshot["next_gate"] = "HOLD_REMEDIATE"
              write_local(run_dir, artifacts)

          print(json.dumps({
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{prefix}",
          }, ensure_ascii=True))

          if not summary["overall_pass"]:
              raise SystemExit(1)
          PY

      - name: Execute M10.G (managed)
        shell: bash
        env:
          M10G_EXECUTION_ID: ${{ steps.run_meta.outputs.m10g_execution_id }}
          M10G_RUN_DIR: ${{ steps.run_meta.outputs.m10g_run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M10F_EXECUTION: ${{ steps.run_meta.outputs.m10f_execution_id }}
          AWS_REGION: ${{ inputs.aws_region }}
          GITHUB_SHA: ${{ github.sha }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations

          import hashlib
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now_utc() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, Any]:
              out: dict[str, Any] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          def is_placeholder(value: Any) -> bool:
              s = str(value).strip()
              s_lower = s.lower()
              if not s:
                  return True
              if s_lower in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "to_pin" in s_lower or "placeholder" in s_lower:
                  return True
              if "<" in s and ">" in s:
                  return True
              if "*" in s:
                  return True
              return False

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              body = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = json.dumps(payload, indent=2, ensure_ascii=True).encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def write_local(run_dir: Path, artifacts: dict[str, dict[str, Any]]) -> None:
              run_dir.mkdir(parents=True, exist_ok=True)
              for name, payload in artifacts.items():
                  (run_dir / name).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def dedupe_blockers(items: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in items:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          def as_int(value: Any) -> int:
              try:
                  return int(value)
              except Exception:
                  try:
                      return int(float(str(value)))
                  except Exception:
                      return 0

          def resolve_pattern(pattern: str, platform_run_id: str) -> str:
              return pattern.replace("{platform_run_id}", platform_run_id).lstrip("/")

          env = dict(os.environ)
          execution_id = env.get("M10G_EXECUTION_ID", "").strip()
          run_dir_raw = env.get("M10G_RUN_DIR", "").strip()
          evidence_bucket = env.get("EVIDENCE_BUCKET", "").strip()
          upstream_m10f_execution = env.get("UPSTREAM_M10F_EXECUTION", "").strip()
          aws_region = env.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          github_sha = env.get("GITHUB_SHA", "").strip()
          if not execution_id or not run_dir_raw or not evidence_bucket or not upstream_m10f_execution:
              raise SystemExit("M10.G required env is missing.")

          run_dir = Path(run_dir_raw)
          s3 = boto3.client("s3", region_name=aws_region)
          handles = parse_handles(HANDLES_PATH)
          captured_at = now_utc()
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          m10f_summary_key = f"evidence/dev_full/run_control/{upstream_m10f_execution}/m10f_execution_summary.json"
          m10f_snapshot_key = f"evidence/dev_full/run_control/{upstream_m10f_execution}/m10f_iceberg_commit_snapshot.json"
          m10f_blocker_key = f"evidence/dev_full/run_control/{upstream_m10f_execution}/m10f_blocker_register.json"
          m10f_summary = None
          m10f_snapshot = None
          platform_run_id = ""
          scenario_run_id = ""
          for name, key in (("summary", m10f_summary_key), ("snapshot", m10f_snapshot_key), ("blocker", m10f_blocker_key)):
              try:
                  payload = s3_get_json(s3, evidence_bucket, key)
                  if name == "summary":
                      m10f_summary = payload
                  elif name == "snapshot":
                      m10f_snapshot = payload
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B7", "message": f"M10.F {name} artifact unreadable."})

          if isinstance(m10f_summary, dict):
              if not bool(m10f_summary.get("overall_pass")):
                  blockers.append({"code": "M10-B7", "message": "M10.F summary is not pass posture."})
              if str(m10f_summary.get("next_gate", "")).strip() != "M10.G_READY":
                  blockers.append({"code": "M10-B7", "message": "M10.F next_gate is not M10.G_READY."})
              platform_run_id = str(m10f_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m10f_summary.get("scenario_run_id", "")).strip()
          else:
              blockers.append({"code": "M10-B7", "message": "M10.F summary missing for M10.G entry gate."})

          required_handles = [
              "S3_EVIDENCE_BUCKET",
              "OFS_MANIFEST_PATH_PATTERN",
              "OFS_FINGERPRINT_PATH_PATTERN",
              "OFS_TIME_BOUND_AUDIT_PATH_PATTERN",
              "DATASET_FINGERPRINT_REQUIRED_FIELDS",
          ]
          missing_handles: list[str] = []
          placeholder_handles: list[str] = []
          for key in required_handles:
              value = handles.get(key)
              if value is None:
                  missing_handles.append(key)
              elif is_placeholder(value):
                  placeholder_handles.append(key)
          if missing_handles or placeholder_handles:
              blockers.append({"code": "M10-B7", "message": "Required M10.G handles are missing or placeholder-valued."})

          expected_evidence_bucket = str(handles.get("S3_EVIDENCE_BUCKET", "")).strip()
          if expected_evidence_bucket and expected_evidence_bucket != evidence_bucket:
              blockers.append({"code": "M10-B7", "message": "EVIDENCE_BUCKET does not match S3_EVIDENCE_BUCKET handle."})
          if not platform_run_id:
              blockers.append({"code": "M10-B7", "message": "platform_run_id unresolved from M10.F summary."})

          manifest_key = ""
          fingerprint_key = ""
          audit_key = ""

          m10e_execution_id = ""
          m10d_execution_id = ""
          m10c_execution_id = ""
          m10e_snapshot = {}
          m10d_snapshot = {}
          m10c_snapshot = {}
          replay_receipt = {}
          leakage_report = {}
          m9d_policy_snapshot = {}

          if not blockers:
              manifest_pattern = str(handles.get("OFS_MANIFEST_PATH_PATTERN", "")).strip()
              fingerprint_pattern = str(handles.get("OFS_FINGERPRINT_PATH_PATTERN", "")).strip()
              audit_pattern = str(handles.get("OFS_TIME_BOUND_AUDIT_PATH_PATTERN", "")).strip()
              manifest_key = resolve_pattern(manifest_pattern, platform_run_id)
              fingerprint_key = resolve_pattern(fingerprint_pattern, platform_run_id)
              audit_key = resolve_pattern(audit_pattern, platform_run_id)
              if not manifest_key or not fingerprint_key or not audit_key:
                  blockers.append({"code": "M10-B7", "message": "Run-scoped M10.G publish keys unresolved from handle patterns."})

          if not blockers and isinstance(m10f_snapshot, dict):
              m10e_execution_id = str(m10f_snapshot.get("upstream_m10e_execution", "")).strip()
              if not m10e_execution_id:
                  blockers.append({"code": "M10-B7", "message": "M10.F snapshot missing upstream_m10e_execution."})
          elif not blockers:
              blockers.append({"code": "M10-B7", "message": "M10.F snapshot missing for lineage traversal."})

          if not blockers:
              m10e_snapshot_key = f"evidence/dev_full/run_control/{m10e_execution_id}/m10e_quality_gate_snapshot.json"
              try:
                  m10e_snapshot = s3_get_json(s3, evidence_bucket, m10e_snapshot_key)
                  m10d_execution_id = str(m10e_snapshot.get("upstream_m10d_execution", "")).strip()
                  if not m10d_execution_id:
                      blockers.append({"code": "M10-B7", "message": "M10.E snapshot missing upstream_m10d_execution."})
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": m10e_snapshot_key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B7", "message": "M10.E snapshot unreadable during M10.G lineage traversal."})

          if not blockers:
              m10d_snapshot_key = f"evidence/dev_full/run_control/{m10d_execution_id}/m10d_ofs_build_execution_snapshot.json"
              try:
                  m10d_snapshot = s3_get_json(s3, evidence_bucket, m10d_snapshot_key)
                  m10c_execution_id = str(m10d_snapshot.get("upstream_m10c_execution", "")).strip()
                  if not m10c_execution_id:
                      blockers.append({"code": "M10-B7", "message": "M10.D snapshot missing upstream_m10c_execution."})
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": m10d_snapshot_key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B7", "message": "M10.D snapshot unreadable during M10.G lineage traversal."})

          if not blockers:
              m10c_snapshot_key = f"evidence/dev_full/run_control/{m10c_execution_id}/m10c_input_binding_snapshot.json"
              try:
                  m10c_snapshot = s3_get_json(s3, evidence_bucket, m10c_snapshot_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": m10c_snapshot_key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B7", "message": "M10.C snapshot unreadable during M10.G lineage traversal."})

          replay_receipt_key = ""
          leakage_report_key = ""
          m9d_policy_key = ""
          if not blockers and isinstance(m10c_snapshot, dict):
              refs = m10c_snapshot.get("required_evidence_refs", {})
              if not isinstance(refs, dict):
                  refs = {}
              replay_receipt_key = str(refs.get("m9c_replay_basis_ref", "")).strip()
              leakage_report_key = str(refs.get("m9e_leakage_report_ref", "")).strip()
              m9d_policy_key = str(refs.get("m9d_policy_ref", "")).strip()
              if not replay_receipt_key or not leakage_report_key or not m9d_policy_key:
                  blockers.append({"code": "M10-B7", "message": "M10.C required evidence refs are incomplete for M10.G."})

          for name, key in (("replay_receipt", replay_receipt_key), ("leakage_report", leakage_report_key), ("m9d_policy", m9d_policy_key)):
              if blockers:
                  break
              try:
                  payload = s3_get_json(s3, evidence_bucket, key)
                  if name == "replay_receipt":
                      replay_receipt = payload
                  elif name == "leakage_report":
                      leakage_report = payload
                  else:
                      m9d_policy_snapshot = payload
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B7", "message": f"M10.G required {name} artifact unreadable."})

          replay_basis = str(replay_receipt.get("replay_basis_fingerprint", "")).strip()
          if not replay_basis:
              replay_basis = str(m10c_snapshot.get("replay_basis_fingerprint", "")).strip() if isinstance(m10c_snapshot, dict) else ""
          feature_asof_utc = str(m9d_policy_snapshot.get("feature_asof_utc", "")).strip()
          label_asof_utc = str(m9d_policy_snapshot.get("label_asof_utc", "")).strip()
          label_maturity_days = m9d_policy_snapshot.get("label_maturity_days")
          feature_def_set = str(((m10d_snapshot.get("databricks", {}) if isinstance(m10d_snapshot, dict) else {}) or {}).get("job_name", "")).strip()
          replay_mode = str(replay_receipt.get("replay_basis_mode", "")).strip()
          if not replay_mode:
              replay_mode = str(((m10c_snapshot.get("resolved_handle_values", {}) if isinstance(m10c_snapshot, dict) else {}) or {}).get("LEARNING_REPLAY_BASIS_MODE", "")).strip()
          join_scope = f"platform_run_id={platform_run_id}|scenario_run_id={scenario_run_id}|replay_mode={replay_mode or 'unknown'}"
          cohort_filters = "none"
          ofs_code_release_id = github_sha or execution_id
          mf_code_release_id = "P14_NOT_BUILT"

          leakage_overall_pass = bool(leakage_report.get("overall_pass", leakage_report.get("pass", True))) if isinstance(leakage_report, dict) else False
          leakage_future_breach_count = 0
          if isinstance(leakage_report, dict):
              for k in (
                  "future_timestamp_boundary_breaches",
                  "future_timestamp_breach_count",
                  "future_boundary_breach_count",
                  "future_breach_count",
              ):
                  if k in leakage_report:
                      leakage_future_breach_count = as_int(leakage_report.get(k))
                      break

          required_fields_raw = str(handles.get("DATASET_FINGERPRINT_REQUIRED_FIELDS", "")).strip()
          required_fields = [x.strip() for x in required_fields_raw.split(",") if x.strip()]
          required_values: dict[str, Any] = {
              "replay_basis": replay_basis,
              "feature_asof_utc": feature_asof_utc,
              "label_asof_utc": label_asof_utc,
              "label_maturity_days": label_maturity_days,
              "feature_def_set": feature_def_set,
              "join_scope": join_scope,
              "cohort_filters": cohort_filters,
              "ofs_code_release_id": ofs_code_release_id,
              "mf_code_release_id": mf_code_release_id,
          }
          missing_required_fields: list[str] = []
          required_field_map: dict[str, Any] = {}
          for key in required_fields:
              value = required_values.get(key)
              if value is None or (isinstance(value, str) and not value.strip()):
                  missing_required_fields.append(key)
              else:
                  required_field_map[key] = value
          if missing_required_fields:
              blockers.append({"code": "M10-B7", "message": f"Dataset fingerprint required fields unresolved: {', '.join(missing_required_fields)}"})

          ordered_material = [f"{k}={required_field_map[k]}" for k in required_fields if k in required_field_map]
          fingerprint_input = "\n".join(ordered_material)
          fingerprint_sha256 = hashlib.sha256(fingerprint_input.encode("utf-8")).hexdigest() if ordered_material else ""
          if not fingerprint_sha256:
              blockers.append({"code": "M10-B7", "message": "Unable to compute dataset fingerprint digest."})

          if not leakage_overall_pass or leakage_future_breach_count > 0:
              blockers.append({"code": "M10-B7", "message": "Time-bound/leakage audit not pass posture for OFS artifact publication."})

          iceberg_surface = (m10f_snapshot.get("iceberg_commit_surface", {}) if isinstance(m10f_snapshot, dict) else {}) or {}
          manifest_payload = {
              "captured_at_utc": captured_at,
              "phase": "M10.G",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "dataset_id": f"ofs::{platform_run_id}",
              "status": "COMMITTED" if len(blockers) == 0 else "HOLD_REMEDIATE",
              "upstream_refs": {
                  "m10f_execution_id": upstream_m10f_execution,
                  "m10e_execution_id": m10e_execution_id,
                  "m10d_execution_id": m10d_execution_id,
                  "m10c_execution_id": m10c_execution_id,
                  "m10f_summary_key": m10f_summary_key,
                  "m10f_snapshot_key": m10f_snapshot_key,
                  "m10c_snapshot_key": f"evidence/dev_full/run_control/{m10c_execution_id}/m10c_input_binding_snapshot.json" if m10c_execution_id else "",
                  "replay_receipt_key": replay_receipt_key,
                  "m9d_policy_key": m9d_policy_key,
                  "leakage_report_key": leakage_report_key,
              },
              "table_surface": {
                  "database_name": str(iceberg_surface.get("database_name", "")).strip(),
                  "table_name": str(iceberg_surface.get("table_name", "")).strip(),
                  "location_uri": str(iceberg_surface.get("location_uri", "")).strip(),
                  "marker_object_uri": str(iceberg_surface.get("marker_object_uri", "")).strip(),
              },
              "replay_basis_fingerprint": replay_basis,
              "feature_asof_utc": feature_asof_utc,
              "label_asof_utc": label_asof_utc,
              "label_maturity_days": label_maturity_days,
              "fingerprint_ref": f"s3://{evidence_bucket}/{fingerprint_key}" if fingerprint_key else "",
              "time_bound_audit_ref": f"s3://{evidence_bucket}/{audit_key}" if audit_key else "",
          }

          fingerprint_payload = {
              "captured_at_utc": captured_at,
              "phase": "M10.G",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "required_fields_order": required_fields,
              "required_field_values": required_field_map,
              "fingerprint_input": fingerprint_input,
              "fingerprint_sha256": fingerprint_sha256,
              "status": "COMMITTED" if len(blockers) == 0 else "HOLD_REMEDIATE",
          }

          time_bound_audit_payload = {
              "captured_at_utc": captured_at,
              "phase": "M10.G",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "temporal_boundaries": {
                  "feature_asof_utc": feature_asof_utc,
                  "label_asof_utc": label_asof_utc,
                  "label_maturity_days": label_maturity_days,
              },
              "leakage_inputs": {
                  "leakage_overall_pass": leakage_overall_pass,
                  "leakage_future_breach_count": leakage_future_breach_count,
                  "source_ref": leakage_report_key,
              },
              "overall_pass": leakage_overall_pass and leakage_future_breach_count == 0,
              "policy": "fail_closed",
              "status": "COMMITTED" if (leakage_overall_pass and leakage_future_breach_count == 0) else "HOLD_REMEDIATE",
          }

          overall_pass = len(blockers) == 0
          next_gate = "M10.H_READY" if overall_pass else "HOLD_REMEDIATE"
          snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M10.G",
              "phase_id": "P13",
              "execution_id": execution_id,
              "upstream_m10f_execution": upstream_m10f_execution,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "publish_targets": {
                  "manifest_key": manifest_key,
                  "fingerprint_key": fingerprint_key,
                  "time_bound_audit_key": audit_key,
              },
              "transitive_lineage": {
                  "m10e_execution_id": m10e_execution_id,
                  "m10d_execution_id": m10d_execution_id,
                  "m10c_execution_id": m10c_execution_id,
              },
              "fingerprint_sha256": fingerprint_sha256,
              "time_bound_audit_overall_pass": bool(time_bound_audit_payload.get("overall_pass")),
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M10.G",
              "execution_id": execution_id,
              "blocker_count": len(blockers),
              "blockers": dedupe_blockers(blockers),
              "read_errors": read_errors,
              "upload_errors": upload_errors,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "M10.G",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
              "manifest_key": manifest_key,
              "fingerprint_key": fingerprint_key,
              "time_bound_audit_key": audit_key,
          }

          run_control_artifacts = {
              "m10g_manifest_fingerprint_snapshot.json": snapshot,
              "m10g_blocker_register.json": blocker_register,
              "m10g_execution_summary.json": summary,
          }
          write_local(run_dir, run_control_artifacts)

          run_scoped_artifacts = {
              manifest_key: manifest_payload,
              fingerprint_key: fingerprint_payload,
              audit_key: time_bound_audit_payload,
          }
          for key, payload in run_scoped_artifacts.items():
              if key:
                  try:
                      s3_put_json(s3, evidence_bucket, key, payload)
                  except (BotoCoreError, ClientError, ValueError) as exc:
                      upload_errors.append({"surface": key, "error": type(exc).__name__})

          run_control_prefix = f"evidence/dev_full/run_control/{execution_id}/"
          for name, payload in run_control_artifacts.items():
              key = f"{run_control_prefix}{name}"
              try:
                  s3_put_json(s3, evidence_bucket, key, payload)
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})

          if upload_errors:
              blockers.append({"code": "M10-B12", "message": "Failed to publish/readback one or more M10.G artifacts."})
              blockers = dedupe_blockers(blockers)
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers)
              summary["next_gate"] = "HOLD_REMEDIATE"
              blocker_register["blocker_count"] = len(blockers)
              blocker_register["blockers"] = blockers
              blocker_register["upload_errors"] = upload_errors
              snapshot["overall_pass"] = False
              snapshot["blocker_count"] = len(blockers)
              snapshot["next_gate"] = "HOLD_REMEDIATE"
              write_local(run_dir, run_control_artifacts)

          print(json.dumps({
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{run_control_prefix}",
          }, ensure_ascii=True))

          if not summary["overall_pass"]:
              raise SystemExit(1)
          PY

      - name: Execute M10.H (managed)
        shell: bash
        env:
          M10H_EXECUTION_ID: ${{ steps.run_meta.outputs.m10h_execution_id }}
          M10H_RUN_DIR: ${{ steps.run_meta.outputs.m10h_run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M10G_EXECUTION: ${{ steps.run_meta.outputs.m10g_execution_id }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations

          import hashlib
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now_utc() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, Any]:
              out: dict[str, Any] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          def is_placeholder(value: Any) -> bool:
              s = str(value).strip()
              s_lower = s.lower()
              if not s:
                  return True
              if s_lower in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "to_pin" in s_lower or "placeholder" in s_lower:
                  return True
              if "<" in s and ">" in s:
                  return True
              if "*" in s:
                  return True
              return False

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              body = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = json.dumps(payload, indent=2, ensure_ascii=True).encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def write_local(run_dir: Path, artifacts: dict[str, dict[str, Any]]) -> None:
              run_dir.mkdir(parents=True, exist_ok=True)
              for name, payload in artifacts.items():
                  (run_dir / name).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def dedupe_blockers(items: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in items:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          def parse_s3_uri(uri: str) -> tuple[str, str]:
              s = uri.strip()
              if not s.startswith("s3://"):
                  return "", ""
              rem = s[5:]
              if "/" not in rem:
                  return rem, ""
              bucket, key = rem.split("/", 1)
              return bucket, key

          env = dict(os.environ)
          execution_id = env.get("M10H_EXECUTION_ID", "").strip()
          run_dir_raw = env.get("M10H_RUN_DIR", "").strip()
          evidence_bucket = env.get("EVIDENCE_BUCKET", "").strip()
          upstream_m10g_execution = env.get("UPSTREAM_M10G_EXECUTION", "").strip()
          aws_region = env.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          if not execution_id or not run_dir_raw or not evidence_bucket or not upstream_m10g_execution:
              raise SystemExit("M10.H required env is missing.")

          run_dir = Path(run_dir_raw)
          s3 = boto3.client("s3", region_name=aws_region)
          glue = boto3.client("glue", region_name=aws_region)
          handles = parse_handles(HANDLES_PATH)
          captured_at = now_utc()
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          m10g_summary_key = f"evidence/dev_full/run_control/{upstream_m10g_execution}/m10g_execution_summary.json"
          m10g_snapshot_key = f"evidence/dev_full/run_control/{upstream_m10g_execution}/m10g_manifest_fingerprint_snapshot.json"
          m10g_blocker_key = f"evidence/dev_full/run_control/{upstream_m10g_execution}/m10g_blocker_register.json"
          m10g_summary = None
          m10g_snapshot = None
          platform_run_id = ""
          scenario_run_id = ""
          for name, key in (("summary", m10g_summary_key), ("snapshot", m10g_snapshot_key), ("blocker", m10g_blocker_key)):
              try:
                  payload = s3_get_json(s3, evidence_bucket, key)
                  if name == "summary":
                      m10g_summary = payload
                  elif name == "snapshot":
                      m10g_snapshot = payload
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B8", "message": f"M10.G {name} artifact unreadable."})

          if isinstance(m10g_summary, dict):
              if not bool(m10g_summary.get("overall_pass")):
                  blockers.append({"code": "M10-B8", "message": "M10.G summary is not pass posture."})
              if str(m10g_summary.get("next_gate", "")).strip() != "M10.H_READY":
                  blockers.append({"code": "M10-B8", "message": "M10.G next_gate is not M10.H_READY."})
              platform_run_id = str(m10g_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m10g_summary.get("scenario_run_id", "")).strip()
          else:
              blockers.append({"code": "M10-B8", "message": "M10.G summary missing for M10.H entry gate."})

          if is_placeholder(handles.get("S3_EVIDENCE_BUCKET")):
              blockers.append({"code": "M10-B8", "message": "S3_EVIDENCE_BUCKET handle unresolved for M10.H."})
          expected_bucket = str(handles.get("S3_EVIDENCE_BUCKET", "")).strip()
          if expected_bucket and expected_bucket != evidence_bucket:
              blockers.append({"code": "M10-B8", "message": "EVIDENCE_BUCKET does not match S3_EVIDENCE_BUCKET handle."})

          manifest_key = str((m10g_summary or {}).get("manifest_key", "")).strip() if isinstance(m10g_summary, dict) else ""
          fingerprint_key = str((m10g_summary or {}).get("fingerprint_key", "")).strip() if isinstance(m10g_summary, dict) else ""
          audit_key = str((m10g_summary or {}).get("time_bound_audit_key", "")).strip() if isinstance(m10g_summary, dict) else ""
          if not manifest_key or not fingerprint_key or not audit_key:
              blockers.append({"code": "M10-B8", "message": "M10.G summary missing run-scoped OFS artifact keys."})

          manifest = {}
          fingerprint = {}
          time_bound_audit = {}
          for name, key in (("manifest", manifest_key), ("fingerprint", fingerprint_key), ("time_bound_audit", audit_key)):
              if blockers:
                  break
              try:
                  payload = s3_get_json(s3, evidence_bucket, key)
                  if name == "manifest":
                      manifest = payload
                  elif name == "fingerprint":
                      fingerprint = payload
                  else:
                      time_bound_audit = payload
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B8", "message": f"M10.H required {name} artifact unreadable."})

          if not blockers and not bool(time_bound_audit.get("overall_pass")):
              blockers.append({"code": "M10-B8", "message": "Time-bound audit is not pass posture for rollback closure."})

          m10f_execution_id = ""
          m10f_snapshot = {}
          if not blockers and isinstance(m10g_snapshot, dict):
              m10f_execution_id = str(m10g_snapshot.get("upstream_m10f_execution", "")).strip()
              if not m10f_execution_id:
                  blockers.append({"code": "M10-B8", "message": "M10.G snapshot missing upstream_m10f_execution."})
          elif not blockers:
              blockers.append({"code": "M10-B8", "message": "M10.G snapshot missing for rollback lineage checks."})

          if not blockers:
              m10f_snapshot_key = f"evidence/dev_full/run_control/{m10f_execution_id}/m10f_iceberg_commit_snapshot.json"
              try:
                  m10f_snapshot = s3_get_json(s3, evidence_bucket, m10f_snapshot_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": m10f_snapshot_key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B8", "message": "M10.F snapshot unreadable for rollback target validation."})

          iceberg_surface = (m10f_snapshot.get("iceberg_commit_surface", {}) if isinstance(m10f_snapshot, dict) else {}) or {}
          database_name = str(iceberg_surface.get("database_name", "")).strip()
          table_name = str(iceberg_surface.get("table_name", "")).strip()
          table_location = str(iceberg_surface.get("location_uri", "")).strip()
          marker_uri = str(iceberg_surface.get("marker_object_uri", "")).strip()
          marker_bucket, marker_key = parse_s3_uri(marker_uri)

          drill_checks: list[dict[str, Any]] = []
          if not blockers:
              try:
                  table_payload = glue.get_table(DatabaseName=database_name, Name=table_name)
                  table_obj = table_payload.get("Table", {}) if isinstance(table_payload, dict) else {}
                  location = str(((table_obj.get("StorageDescriptor", {}) if isinstance(table_obj, dict) else {}) or {}).get("Location", "")).strip()
                  check_pass = bool(location) and location.rstrip("/") == table_location.rstrip("/")
                  drill_checks.append({"check": "glue_table_readback", "pass": check_pass, "expected_location": table_location, "actual_location": location})
                  if not check_pass:
                      blockers.append({"code": "M10-B8", "message": "Rollback target Glue table location mismatch."})
              except (BotoCoreError, ClientError) as exc:
                  read_errors.append({"surface": f"glue.table.{database_name}.{table_name}", "error": type(exc).__name__})
                  blockers.append({"code": "M10-B8", "message": "Rollback target Glue table unreadable."})

          if not blockers:
              try:
                  s3.head_object(Bucket=marker_bucket, Key=marker_key)
                  drill_checks.append({"check": "rollback_marker_exists", "pass": True, "marker_uri": marker_uri})
              except (BotoCoreError, ClientError) as exc:
                  read_errors.append({"surface": marker_uri, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B8", "message": "Rollback target marker object unreadable."})

          recipe_dir = manifest_key.rsplit("/", 1)[0] if "/" in manifest_key else f"evidence/runs/{platform_run_id}/learning/ofs"
          rollback_recipe_key = f"{recipe_dir}/rollback_recipe.json"
          rollback_drill_key = f"{recipe_dir}/rollback_drill_report.json"
          idempotency_key = hashlib.sha256(f"{platform_run_id}:{execution_id}".encode("utf-8")).hexdigest()
          drill_pass = len(blockers) == 0

          rollback_recipe = {
              "captured_at_utc": captured_at,
              "phase": "M10.H",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "rollback_recipe_version": "v0",
              "rollback_objective": "Restore previous OFS table commit surface using deterministic metadata-pointer controls.",
              "rollback_mode": "non_destructive_drill_only_v0",
              "idempotency_key": idempotency_key,
              "targets": {
                  "glue_database": database_name,
                  "glue_table": table_name,
                  "table_location_uri": table_location,
                  "rollback_marker_uri": marker_uri,
                  "manifest_ref": f"s3://{evidence_bucket}/{manifest_key}",
                  "fingerprint_ref": f"s3://{evidence_bucket}/{fingerprint_key}",
                  "time_bound_audit_ref": f"s3://{evidence_bucket}/{audit_key}",
              },
              "preconditions": [
                  {"id": "pc_m10g_pass", "description": "M10.G pass gate is satisfied", "required": True},
                  {"id": "pc_artifacts_readable", "description": "Manifest/fingerprint/time-bound artifacts are readable", "required": True},
                  {"id": "pc_table_target_readable", "description": "Glue rollback target is readable", "required": True},
                  {"id": "pc_marker_target_readable", "description": "Rollback marker object is readable", "required": True},
              ],
              "ordered_steps": [
                  {"step_id": 1, "action": "capture_prestate", "mode": "dry_run", "description": "Read current Glue table metadata and rollback marker."},
                  {"step_id": 2, "action": "validate_targets", "mode": "dry_run", "description": "Confirm rollback target refs are readable and coherent."},
                  {"step_id": 3, "action": "apply_pointer_rewind", "mode": "manual_approval_required", "description": "Apply metadata-pointer rollback to approved prior commit state."},
                  {"step_id": 4, "action": "verify_poststate", "mode": "manual_approval_required", "description": "Re-read Glue table metadata and confirm rollback target activation."},
              ],
              "operator_stop_gates": {
                  "require_manual_approval": True,
                  "stop_on_missing_preconditions": True,
                  "stop_on_surface_mismatch": True,
              },
              "drill_report_ref": f"s3://{evidence_bucket}/{rollback_drill_key}",
              "status": "COMMITTED" if drill_pass else "HOLD_REMEDIATE",
          }

          rollback_drill_report = {
              "captured_at_utc": captured_at,
              "phase": "M10.H",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "drill_mode": "non_destructive",
              "drill_pass": drill_pass,
              "checks": drill_checks,
              "precondition_summary": {
                  "m10g_gate_pass": bool(isinstance(m10g_summary, dict) and m10g_summary.get("overall_pass")),
                  "artifact_refs_readable": len(read_errors) == 0,
                  "table_target_readable": any(x.get("check") == "glue_table_readback" and x.get("pass") for x in drill_checks),
                  "marker_target_readable": any(x.get("check") == "rollback_marker_exists" and x.get("pass") for x in drill_checks),
              },
              "status": "COMMITTED" if drill_pass else "HOLD_REMEDIATE",
          }

          blockers = dedupe_blockers(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M10.I_READY" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": captured_at,
              "phase": "M10.H",
              "phase_id": "P13",
              "execution_id": execution_id,
              "upstream_m10g_execution": upstream_m10g_execution,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "rollback_artifacts": {
                  "rollback_recipe_key": rollback_recipe_key,
                  "rollback_drill_key": rollback_drill_key,
              },
              "rollback_targets": {
                  "database_name": database_name,
                  "table_name": table_name,
                  "table_location_uri": table_location,
                  "rollback_marker_uri": marker_uri,
              },
              "drill_pass": drill_pass,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M10.H",
              "execution_id": execution_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "read_errors": read_errors,
              "upload_errors": upload_errors,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "M10.H",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
              "rollback_recipe_key": rollback_recipe_key,
              "rollback_drill_key": rollback_drill_key,
          }

          run_control_artifacts = {
              "m10h_rollback_recipe_snapshot.json": snapshot,
              "m10h_blocker_register.json": blocker_register,
              "m10h_execution_summary.json": summary,
          }
          write_local(run_dir, run_control_artifacts)

          run_scoped_artifacts = {
              rollback_recipe_key: rollback_recipe,
              rollback_drill_key: rollback_drill_report,
          }
          for key, payload in run_scoped_artifacts.items():
              try:
                  s3_put_json(s3, evidence_bucket, key, payload)
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})

          run_control_prefix = f"evidence/dev_full/run_control/{execution_id}/"
          for name, payload in run_control_artifacts.items():
              key = f"{run_control_prefix}{name}"
              try:
                  s3_put_json(s3, evidence_bucket, key, payload)
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})

          if upload_errors:
              blockers.append({"code": "M10-B12", "message": "Failed to publish/readback one or more M10.H artifacts."})
              blockers = dedupe_blockers(blockers)
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers)
              summary["next_gate"] = "HOLD_REMEDIATE"
              blocker_register["blocker_count"] = len(blockers)
              blocker_register["blockers"] = blockers
              blocker_register["upload_errors"] = upload_errors
              snapshot["overall_pass"] = False
              snapshot["blocker_count"] = len(blockers)
              snapshot["next_gate"] = "HOLD_REMEDIATE"
              write_local(run_dir, run_control_artifacts)

          print(json.dumps({
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{run_control_prefix}",
          }, ensure_ascii=True))

          if not summary["overall_pass"]:
              raise SystemExit(1)
          PY

      - name: Execute M10.I (managed)
        shell: bash
        env:
          M10I_EXECUTION_ID: ${{ steps.run_meta.outputs.m10i_execution_id }}
          M10I_RUN_DIR: ${{ steps.run_meta.outputs.m10i_run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M10C_EXECUTION: ${{ inputs.upstream_m10c_execution }}
          UPSTREAM_M10D_EXECUTION: ${{ steps.run_meta.outputs.m10d_execution_id }}
          UPSTREAM_M10E_EXECUTION: ${{ steps.run_meta.outputs.m10e_execution_id }}
          UPSTREAM_M10F_EXECUTION: ${{ steps.run_meta.outputs.m10f_execution_id }}
          UPSTREAM_M10G_EXECUTION: ${{ steps.run_meta.outputs.m10g_execution_id }}
          UPSTREAM_M10H_EXECUTION: ${{ steps.run_meta.outputs.m10h_execution_id }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations

          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")


          def now_utc() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")


          def parse_handles(path: Path) -> dict[str, Any]:
              out: dict[str, Any] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out


          def is_placeholder(value: Any) -> bool:
              s = str(value).strip()
              s_lower = s.lower()
              if not s:
                  return True
              if s_lower in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "to_pin" in s_lower or "placeholder" in s_lower:
                  return True
              if "<" in s and ">" in s:
                  return True
              return False


          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              body = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload


          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = json.dumps(payload, indent=2, ensure_ascii=True).encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)


          def write_local(run_dir: Path, artifacts: dict[str, dict[str, Any]]) -> None:
              run_dir.mkdir(parents=True, exist_ok=True)
              for name, payload in artifacts.items():
                  (run_dir / name).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")


          def dedupe_blockers(items: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in items:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out


          def execution_from_summary_key(key: str) -> str:
              m = re.search(r"/run_control/([^/]+)/[^/]+_execution_summary\.json$", key)
              if not m:
                  return ""
              return m.group(1).strip()


          env = dict(os.environ)
          execution_id = env.get("M10I_EXECUTION_ID", "").strip()
          run_dir_raw = env.get("M10I_RUN_DIR", "").strip()
          evidence_bucket = env.get("EVIDENCE_BUCKET", "").strip()
          aws_region = env.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          upstream_m10c_execution = env.get("UPSTREAM_M10C_EXECUTION", "").strip()
          upstream_m10d_execution = env.get("UPSTREAM_M10D_EXECUTION", "").strip()
          upstream_m10e_execution = env.get("UPSTREAM_M10E_EXECUTION", "").strip()
          upstream_m10f_execution = env.get("UPSTREAM_M10F_EXECUTION", "").strip()
          upstream_m10g_execution = env.get("UPSTREAM_M10G_EXECUTION", "").strip()
          upstream_m10h_execution = env.get("UPSTREAM_M10H_EXECUTION", "").strip()
          if not execution_id or not run_dir_raw or not evidence_bucket:
              raise SystemExit("M10.I required env is missing.")
          if not all([upstream_m10c_execution, upstream_m10d_execution, upstream_m10e_execution, upstream_m10f_execution, upstream_m10g_execution, upstream_m10h_execution]):
              raise SystemExit("M10.I required upstream execution ids are missing.")

          run_dir = Path(run_dir_raw)
          s3 = boto3.client("s3", region_name=aws_region)
          handles = parse_handles(HANDLES_PATH)
          captured_at = now_utc()
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          expected_bucket = str(handles.get("S3_EVIDENCE_BUCKET", "")).strip()
          run_control_pattern = str(handles.get("S3_RUN_CONTROL_ROOT_PATTERN", "")).strip()
          if is_placeholder(expected_bucket):
              blockers.append({"code": "M10-B10", "message": "S3_EVIDENCE_BUCKET handle unresolved for M10.I."})
          if is_placeholder(run_control_pattern) or "{phase_execution_id}" not in run_control_pattern:
              blockers.append({"code": "M10-B10", "message": "S3_RUN_CONTROL_ROOT_PATTERN handle unresolved for M10.I."})
          if expected_bucket and expected_bucket != evidence_bucket:
              blockers.append({"code": "M10-B10", "message": "EVIDENCE_BUCKET does not match S3_EVIDENCE_BUCKET handle."})

          # Resolve M10.A/B execution lineage via M10.C snapshot.
          m10c_snapshot_key = f"evidence/dev_full/run_control/{upstream_m10c_execution}/m10c_input_binding_snapshot.json"
          m10c_summary_key = f"evidence/dev_full/run_control/{upstream_m10c_execution}/m10c_execution_summary.json"
          m10b_execution = ""
          m10a_execution = ""
          try:
              m10c_snapshot = s3_get_json(s3, evidence_bucket, m10c_snapshot_key)
              m10b_summary_ref = str(((m10c_snapshot.get("upstream_refs", {}) if isinstance(m10c_snapshot, dict) else {}) or {}).get("m10b_summary_ref", "")).strip()
              m10b_execution = execution_from_summary_key(m10b_summary_ref)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": m10c_snapshot_key, "error": type(exc).__name__})
              blockers.append({"code": "M10-B9", "message": "M10.C snapshot unreadable for lineage resolution."})

          if not m10b_execution:
              blockers.append({"code": "M10-B9", "message": "Unable to resolve upstream M10.B execution id from M10.C snapshot."})

          if not blockers:
              m10b_snapshot_key = f"evidence/dev_full/run_control/{m10b_execution}/m10b_databricks_readiness_snapshot.json"
              try:
                  m10b_snapshot = s3_get_json(s3, evidence_bucket, m10b_snapshot_key)
                  m10a_execution = str(m10b_snapshot.get("upstream_m10a_execution", "")).strip()
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": m10b_snapshot_key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B9", "message": "M10.B snapshot unreadable for lineage resolution."})

          if not m10a_execution:
              blockers.append({"code": "M10-B9", "message": "Unable to resolve upstream M10.A execution id from M10.B snapshot."})

          expected_chain: list[tuple[str, str, str]] = []
          if not blockers:
              expected_chain = [
                  ("M10.A", m10a_execution, "M10.B_READY"),
                  ("M10.B", m10b_execution, "M10.C_READY"),
                  ("M10.C", upstream_m10c_execution, "M10.D_READY"),
                  ("M10.D", upstream_m10d_execution, "M10.E_READY"),
                  ("M10.E", upstream_m10e_execution, "M10.F_READY"),
                  ("M10.F", upstream_m10f_execution, "M10.G_READY"),
                  ("M10.G", upstream_m10g_execution, "M10.H_READY"),
                  ("M10.H", upstream_m10h_execution, "M10.I_READY"),
              ]

          rollup_rows: list[dict[str, Any]] = []
          platform_values: set[str] = set()
          scenario_values: set[str] = set()

          for phase_name, phase_execution, expected_next_gate in expected_chain:
              summary_name = f"{phase_name.lower().replace('.', '')}_execution_summary.json"
              key = f"evidence/dev_full/run_control/{phase_execution}/{summary_name}"
              summary_payload: dict[str, Any] | None = None
              try:
                  summary_payload = s3_get_json(s3, evidence_bucket, key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B9", "message": f"{phase_name} summary unreadable for rollup."})

              overall_pass = bool((summary_payload or {}).get("overall_pass"))
              next_gate = str((summary_payload or {}).get("next_gate", "")).strip()
              platform_run_id = str((summary_payload or {}).get("platform_run_id", "")).strip()
              scenario_run_id = str((summary_payload or {}).get("scenario_run_id", "")).strip()
              if platform_run_id:
                  platform_values.add(platform_run_id)
              if scenario_run_id:
                  scenario_values.add(scenario_run_id)
              row_pass = bool(summary_payload) and overall_pass and next_gate == expected_next_gate
              if not row_pass and summary_payload:
                  blockers.append({"code": "M10-B9", "message": f"{phase_name} gate continuity failed (expected {expected_next_gate})."})
              rollup_rows.append({
                  "phase": phase_name,
                  "execution_id": phase_execution,
                  "summary_key": key,
                  "overall_pass": overall_pass,
                  "actual_next_gate": next_gate,
                  "expected_next_gate": expected_next_gate,
                  "row_pass": row_pass,
              })

          if len(platform_values) != 1 or len(scenario_values) != 1:
              blockers.append({"code": "M10-B9", "message": "Run scope continuity failed across M10.A..M10.H summaries."})

          platform_run_id = next(iter(platform_values)) if len(platform_values) == 1 else ""
          scenario_run_id = next(iter(scenario_values)) if len(scenario_values) == 1 else ""

          m10g_summary_key = f"evidence/dev_full/run_control/{upstream_m10g_execution}/m10g_execution_summary.json"
          m10h_summary_key = f"evidence/dev_full/run_control/{upstream_m10h_execution}/m10h_execution_summary.json"
          manifest_key = ""
          fingerprint_key = ""
          time_bound_audit_key = ""
          rollback_recipe_key = ""
          rollback_drill_key = ""
          try:
              m10g_summary = s3_get_json(s3, evidence_bucket, m10g_summary_key)
              manifest_key = str(m10g_summary.get("manifest_key", "")).strip()
              fingerprint_key = str(m10g_summary.get("fingerprint_key", "")).strip()
              time_bound_audit_key = str(m10g_summary.get("time_bound_audit_key", "")).strip()
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": m10g_summary_key, "error": type(exc).__name__})
              blockers.append({"code": "M10-B10", "message": "M10.G summary unreadable for M11 handoff refs."})
          try:
              m10h_summary = s3_get_json(s3, evidence_bucket, m10h_summary_key)
              rollback_recipe_key = str(m10h_summary.get("rollback_recipe_key", "")).strip()
              rollback_drill_key = str(m10h_summary.get("rollback_drill_key", "")).strip()
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": m10h_summary_key, "error": type(exc).__name__})
              blockers.append({"code": "M10-B10", "message": "M10.H summary unreadable for M11 handoff refs."})

          blockers = dedupe_blockers(blockers)
          overall_pass = len(blockers) == 0
          verdict = "ADVANCE_TO_P14" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M11_READY" if overall_pass else "HOLD_REMEDIATE"

          rollup_matrix = {
              "captured_at_utc": captured_at,
              "phase": "M10.I",
              "phase_id": "P13",
              "execution_id": execution_id,
              "source_chain_rows": rollup_rows,
              "platform_run_scope_values": sorted(platform_values),
              "scenario_run_scope_values": sorted(scenario_values),
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
          }

          gate_verdict = {
              "captured_at_utc": captured_at,
              "phase": "M10.I",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "upstream_refs": {
                  "m10a_execution_id": m10a_execution,
                  "m10b_execution_id": m10b_execution,
                  "m10c_execution_id": upstream_m10c_execution,
                  "m10d_execution_id": upstream_m10d_execution,
                  "m10e_execution_id": upstream_m10e_execution,
                  "m10f_execution_id": upstream_m10f_execution,
                  "m10g_execution_id": upstream_m10g_execution,
                  "m10h_execution_id": upstream_m10h_execution,
              },
          }

          m11_handoff_pack = {
              "captured_at_utc": captured_at,
              "phase": "M10.I",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "p13_verdict": verdict,
              "next_gate": next_gate,
              "m11_entry_ready": overall_pass,
              "m11_entry_gate": {
                  "required_verdict": "ADVANCE_TO_P14",
                  "next_gate": "M11_READY",
              },
              "required_refs": {
                  "m10i_gate_verdict_ref": f"evidence/dev_full/run_control/{execution_id}/m10i_p13_gate_verdict.json",
                  "m10g_manifest_ref": f"s3://{evidence_bucket}/{manifest_key}" if manifest_key else "",
                  "m10g_fingerprint_ref": f"s3://{evidence_bucket}/{fingerprint_key}" if fingerprint_key else "",
                  "m10g_time_bound_audit_ref": f"s3://{evidence_bucket}/{time_bound_audit_key}" if time_bound_audit_key else "",
                  "m10h_rollback_recipe_ref": f"s3://{evidence_bucket}/{rollback_recipe_key}" if rollback_recipe_key else "",
                  "m10h_rollback_drill_ref": f"s3://{evidence_bucket}/{rollback_drill_key}" if rollback_drill_key else "",
              },
              "source_chain": [
                  {"phase": row["phase"], "execution_id": row["execution_id"], "row_pass": row["row_pass"]}
                  for row in rollup_rows
              ],
          }

          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M10.I",
              "execution_id": execution_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "read_errors": read_errors,
              "upload_errors": upload_errors,
          }
          summary = {
              "captured_at_utc": captured_at,
              "phase": "M10.I",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "m11_handoff_key": f"evidence/dev_full/run_control/{execution_id}/m11_handoff_pack.json",
          }

          run_control_artifacts = {
              "m10i_p13_rollup_matrix.json": rollup_matrix,
              "m10i_p13_gate_verdict.json": gate_verdict,
              "m11_handoff_pack.json": m11_handoff_pack,
              "m10i_blocker_register.json": blocker_register,
              "m10i_execution_summary.json": summary,
          }
          write_local(run_dir, run_control_artifacts)

          run_control_prefix = f"evidence/dev_full/run_control/{execution_id}/"
          for name, payload in run_control_artifacts.items():
              key = f"{run_control_prefix}{name}"
              try:
                  s3_put_json(s3, evidence_bucket, key, payload)
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})

          if upload_errors:
              blockers.append({"code": "M10-B12", "message": "Failed to publish/readback one or more M10.I artifacts."})
              blockers = dedupe_blockers(blockers)
              summary["overall_pass"] = False
              summary["blocker_count"] = len(blockers)
              summary["verdict"] = "HOLD_REMEDIATE"
              summary["next_gate"] = "HOLD_REMEDIATE"
              blocker_register["blocker_count"] = len(blockers)
              blocker_register["blockers"] = blockers
              blocker_register["upload_errors"] = upload_errors
              rollup_matrix["overall_pass"] = False
              rollup_matrix["blocker_count"] = len(blockers)
              gate_verdict["overall_pass"] = False
              gate_verdict["blocker_count"] = len(blockers)
              gate_verdict["verdict"] = "HOLD_REMEDIATE"
              gate_verdict["next_gate"] = "HOLD_REMEDIATE"
              m11_handoff_pack["p13_verdict"] = "HOLD_REMEDIATE"
              m11_handoff_pack["next_gate"] = "HOLD_REMEDIATE"
              m11_handoff_pack["m11_entry_ready"] = False
              write_local(run_dir, run_control_artifacts)

          print(json.dumps({
              "execution_id": execution_id,
              "overall_pass": summary["overall_pass"],
              "blocker_count": summary["blocker_count"],
              "verdict": summary["verdict"],
              "next_gate": summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{run_control_prefix}",
          }, ensure_ascii=True))

          if not summary["overall_pass"]:
              raise SystemExit(1)
          PY

      - name: Execute M10.J (managed)
        shell: bash
        env:
          M10J_EXECUTION_ID: ${{ steps.run_meta.outputs.m10j_execution_id }}
          M10J_RUN_DIR: ${{ steps.run_meta.outputs.m10j_run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M10I_EXECUTION: ${{ steps.run_meta.outputs.m10i_execution_id }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations

          import json
          import os
          import re
          from datetime import date, datetime, timedelta, timezone
          from decimal import Decimal, InvalidOperation
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now_utc() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, Any]:
              out: dict[str, Any] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      out[key] = raw[1:-1]
                  elif raw.lower() == "true":
                      out[key] = True
                  elif raw.lower() == "false":
                      out[key] = False
                  else:
                      out[key] = raw
              return out

          def is_placeholder(value: Any) -> bool:
              s = str(value).strip()
              s_lower = s.lower()
              if not s:
                  return True
              if s_lower in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "to_pin" in s_lower or "placeholder" in s_lower:
                  return True
              if "<" in s and ">" in s:
                  return True
              return False

          def to_decimal(value: Any) -> Decimal:
              return Decimal(str(value))

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              body = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = json.dumps(payload, indent=2, ensure_ascii=True).encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def write_local(run_dir: Path, artifacts: dict[str, dict[str, Any]]) -> None:
              run_dir.mkdir(parents=True, exist_ok=True)
              for name, payload in artifacts.items():
                  (run_dir / name).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          def dedupe_blockers(items: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in items:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          env = dict(os.environ)
          execution_id = env.get("M10J_EXECUTION_ID", "").strip()
          run_dir_raw = env.get("M10J_RUN_DIR", "").strip()
          evidence_bucket = env.get("EVIDENCE_BUCKET", "").strip()
          upstream_m10i_execution = env.get("UPSTREAM_M10I_EXECUTION", "").strip()
          aws_region = env.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          if not execution_id or not run_dir_raw or not evidence_bucket or not upstream_m10i_execution:
              raise SystemExit("M10.J required env is missing.")

          run_dir = Path(run_dir_raw)
          s3 = boto3.client("s3", region_name=aws_region)
          ce = boto3.client("ce", region_name="us-east-1")
          handles = parse_handles(HANDLES_PATH)
          captured_at = now_utc()
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          required_handle_keys = [
              "S3_EVIDENCE_BUCKET",
              "S3_RUN_CONTROL_ROOT_PATTERN",
              "DEV_FULL_MONTHLY_BUDGET_LIMIT_USD",
              "DEV_FULL_BUDGET_ALERT_1_USD",
              "DEV_FULL_BUDGET_ALERT_2_USD",
              "DEV_FULL_BUDGET_ALERT_3_USD",
              "BUDGET_CURRENCY",
              "COST_CAPTURE_SCOPE",
              "AWS_COST_CAPTURE_ENABLED",
              "DATABRICKS_COST_CAPTURE_ENABLED",
          ]
          for key in required_handle_keys:
              if key not in handles or is_placeholder(handles.get(key)):
                  blockers.append({"code": "M10-B11", "message": f"{key} unresolved for M10.J."})

          expected_bucket = str(handles.get("S3_EVIDENCE_BUCKET", "")).strip()
          if expected_bucket and expected_bucket != evidence_bucket:
              blockers.append({"code": "M10-B11", "message": "EVIDENCE_BUCKET does not match S3_EVIDENCE_BUCKET handle."})

          m10i_summary_key = f"evidence/dev_full/run_control/{upstream_m10i_execution}/m10i_execution_summary.json"
          m10i_verdict_key = f"evidence/dev_full/run_control/{upstream_m10i_execution}/m10i_p13_gate_verdict.json"
          m10i_rollup_key = f"evidence/dev_full/run_control/{upstream_m10i_execution}/m10i_p13_rollup_matrix.json"
          m10i_blocker_key = f"evidence/dev_full/run_control/{upstream_m10i_execution}/m10i_blocker_register.json"
          m11_handoff_key = f"evidence/dev_full/run_control/{upstream_m10i_execution}/m11_handoff_pack.json"

          required_upstream_keys = [m10i_summary_key, m10i_verdict_key, m10i_rollup_key, m10i_blocker_key, m11_handoff_key]
          readable_upstream = 0
          m10i_summary: dict[str, Any] = {}
          m10i_rollup: dict[str, Any] = {}
          m10i_verdict: dict[str, Any] = {}
          m11_handoff: dict[str, Any] = {}
          for key in required_upstream_keys:
              try:
                  payload = s3_get_json(s3, evidence_bucket, key)
                  readable_upstream += 1
                  if key == m10i_summary_key:
                      m10i_summary = payload
                  elif key == m10i_rollup_key:
                      m10i_rollup = payload
                  elif key == m10i_verdict_key:
                      m10i_verdict = payload
                  elif key == m11_handoff_key:
                      m11_handoff = payload
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": key, "error": type(exc).__name__})
                  blockers.append({"code": "M10-B11", "message": f"Required upstream artifact unreadable: {key}."})

          if m10i_summary:
              if not bool(m10i_summary.get("overall_pass")):
                  blockers.append({"code": "M10-B11", "message": "M10.I summary is not pass posture."})
              if str(m10i_summary.get("verdict", "")).strip() != "ADVANCE_TO_P14":
                  blockers.append({"code": "M10-B11", "message": "M10.I verdict is not ADVANCE_TO_P14."})
              if str(m10i_summary.get("next_gate", "")).strip() != "M11_READY":
                  blockers.append({"code": "M10-B11", "message": "M10.I next_gate is not M11_READY."})

          source_chain = (m11_handoff.get("source_chain", []) if isinstance(m11_handoff, dict) else []) or []
          if source_chain:
              all_row_pass = all(bool(row.get("row_pass")) for row in source_chain if isinstance(row, dict))
              if not all_row_pass:
                  blockers.append({"code": "M10-B11", "message": "M10.I source chain contains non-pass rows."})

          platform_run_id = str(m10i_summary.get("platform_run_id", "")).strip()
          scenario_run_id = str(m10i_summary.get("scenario_run_id", "")).strip()
          if not platform_run_id or not scenario_run_id:
              blockers.append({"code": "M10-B11", "message": "M10.I summary missing run-scope values."})

          monthly_limit = Decimal("0")
          alert_1 = Decimal("0")
          alert_2 = Decimal("0")
          alert_3 = Decimal("0")
          budget_currency = str(handles.get("BUDGET_CURRENCY", "USD")).strip() or "USD"
          capture_scope = str(handles.get("COST_CAPTURE_SCOPE", "")).strip()
          aws_capture_enabled = bool(handles.get("AWS_COST_CAPTURE_ENABLED", False))
          databricks_capture_enabled = bool(handles.get("DATABRICKS_COST_CAPTURE_ENABLED", False))
          databricks_defer_reason = str(handles.get("DATABRICKS_COST_CAPTURE_DEFER_REASON", "")).strip()
          databricks_reenable_gate = str(handles.get("DATABRICKS_COST_CAPTURE_REENABLE_GATE", "")).strip()
          try:
              monthly_limit = to_decimal(handles.get("DEV_FULL_MONTHLY_BUDGET_LIMIT_USD"))
              alert_1 = to_decimal(handles.get("DEV_FULL_BUDGET_ALERT_1_USD"))
              alert_2 = to_decimal(handles.get("DEV_FULL_BUDGET_ALERT_2_USD"))
              alert_3 = to_decimal(handles.get("DEV_FULL_BUDGET_ALERT_3_USD"))
          except (InvalidOperation, ValueError, TypeError):
              blockers.append({"code": "M10-B11", "message": "Budget thresholds are not parseable decimals."})

          if not (alert_1 < alert_2 < alert_3 <= monthly_limit):
              blockers.append({"code": "M10-B11", "message": "Budget threshold ordering invalid (require a1 < a2 < a3 <= monthly_limit)."})

          aws_mtd_cost: Decimal | None = None
          aws_cost_unit = budget_currency
          ce_surface_error = ""
          if aws_capture_enabled:
              try:
                  today = date.today()
                  month_start = today.replace(day=1).isoformat()
                  tomorrow = (today + timedelta(days=1)).isoformat()
                  ce_payload = ce.get_cost_and_usage(
                      TimePeriod={"Start": month_start, "End": tomorrow},
                      Granularity="MONTHLY",
                      Metrics=["UnblendedCost"],
                  )
                  groups = (((ce_payload.get("ResultsByTime", []) or [{}])[0]).get("Total", {}) or {}).get("UnblendedCost", {}) or {}
                  aws_mtd_cost = Decimal(str(groups.get("Amount", "0")))
                  aws_cost_unit = str(groups.get("Unit", budget_currency)).strip() or budget_currency
              except (BotoCoreError, ClientError, InvalidOperation, ValueError, TypeError) as exc:
                  ce_surface_error = type(exc).__name__
                  blockers.append({"code": "M10-B11", "message": "AWS cost capture failed for M10.J."})
          else:
              blockers.append({"code": "M10-B11", "message": "AWS_COST_CAPTURE_ENABLED is false; M10.J requires AWS cost capture."})

          utilization_pct: str | None = None
          if aws_mtd_cost is not None and monthly_limit > 0:
              utilization_pct = str((aws_mtd_cost / monthly_limit) * Decimal("100"))

          budget_envelope = {
              "captured_at_utc": captured_at,
              "phase": "M10.J",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "budget_currency": budget_currency,
              "monthly_limit_amount": str(monthly_limit),
              "alert_thresholds": {
                  "alert_1_amount": str(alert_1),
                  "alert_2_amount": str(alert_2),
                  "alert_3_amount": str(alert_3),
              },
              "threshold_order_valid": bool(alert_1 < alert_2 < alert_3 <= monthly_limit),
              "cost_capture_scope": capture_scope,
              "aws_cost_capture_enabled": aws_capture_enabled,
              "databricks_cost_capture_enabled": databricks_capture_enabled,
              "databricks_defer_reason": databricks_defer_reason,
              "databricks_reenable_gate": databricks_reenable_gate,
          }

          cost_receipt = {
              "captured_at_utc": captured_at,
              "phase": "M10.J",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "cost_capture_scope": capture_scope,
              "aws_mtd_cost_amount": str(aws_mtd_cost) if aws_mtd_cost is not None else None,
              "aws_cost_unit": aws_cost_unit,
              "budget_currency": budget_currency,
              "budget_monthly_limit_amount": str(monthly_limit),
              "utilization_pct": utilization_pct,
              "databricks_capture_mode": "enabled" if databricks_capture_enabled else "deferred",
              "databricks_defer_reason": databricks_defer_reason if not databricks_capture_enabled else "",
              "databricks_reenable_gate": databricks_reenable_gate if not databricks_capture_enabled else "",
              "ce_surface_error": ce_surface_error,
          }

          required_output_names = [
              "m10_phase_budget_envelope.json",
              "m10_phase_cost_outcome_receipt.json",
              "m10j_blocker_register.json",
              "m10j_execution_summary.json",
              "m10_execution_summary.json",
          ]

          blockers = dedupe_blockers(blockers)
          overall_pass = len(blockers) == 0
          verdict = "ADVANCE_TO_M11" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M11_READY" if overall_pass else "HOLD_REMEDIATE"

          blocker_register = {
              "captured_at_utc": captured_at,
              "phase": "M10.J",
              "execution_id": execution_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "read_errors": read_errors,
              "upload_errors": upload_errors,
          }

          m10j_summary = {
              "captured_at_utc": captured_at,
              "phase": "M10.J",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "contract_parity": {
                  "required_upstream_artifacts": len(required_upstream_keys),
                  "readable_upstream_artifacts": readable_upstream,
                  "required_outputs": len(required_output_names),
                  "published_outputs": 0,
                  "all_required_available": False,
              },
          }

          m10_summary = {
              "captured_at_utc": captured_at,
              "phase": "M10",
              "phase_id": "P13",
              "execution_id": execution_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "upstream_refs": {
                  "m10i_execution_id": upstream_m10i_execution,
                  "m11_handoff_key": f"evidence/dev_full/run_control/{upstream_m10i_execution}/m11_handoff_pack.json",
              },
          }

          run_control_artifacts = {
              "m10_phase_budget_envelope.json": budget_envelope,
              "m10_phase_cost_outcome_receipt.json": cost_receipt,
              "m10j_blocker_register.json": blocker_register,
              "m10j_execution_summary.json": m10j_summary,
              "m10_execution_summary.json": m10_summary,
          }
          write_local(run_dir, run_control_artifacts)

          run_control_prefix = f"evidence/dev_full/run_control/{execution_id}/"
          published_count = 0
          for name, payload in run_control_artifacts.items():
              key = f"{run_control_prefix}{name}"
              try:
                  s3_put_json(s3, evidence_bucket, key, payload)
                  published_count += 1
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})

          all_required_available = readable_upstream == len(required_upstream_keys) and published_count == len(required_output_names)
          m10j_summary["contract_parity"]["published_outputs"] = published_count
          m10j_summary["contract_parity"]["all_required_available"] = all_required_available
          # Persist contract parity fields after publish-count is known.
          write_local(run_dir, run_control_artifacts)
          for name in ("m10j_execution_summary.json", "m10j_blocker_register.json", "m10_execution_summary.json"):
              key = f"{run_control_prefix}{name}"
              try:
                  s3_put_json(s3, evidence_bucket, key, run_control_artifacts[name])
              except (BotoCoreError, ClientError, ValueError) as exc:
                  upload_errors.append({"surface": key, "error": type(exc).__name__})
          if not all_required_available:
              blockers.append({"code": "M10-B11", "message": "M10.J contract parity check failed."})

          if upload_errors:
              blockers.append({"code": "M10-B12", "message": "Failed to publish/readback one or more M10.J artifacts."})

          blockers = dedupe_blockers(blockers)
          if blockers:
              overall_pass = False
              verdict = "HOLD_REMEDIATE"
              next_gate = "HOLD_REMEDIATE"
              blocker_register["blocker_count"] = len(blockers)
              blocker_register["blockers"] = blockers
              blocker_register["upload_errors"] = upload_errors
              m10j_summary["overall_pass"] = False
              m10j_summary["blocker_count"] = len(blockers)
              m10j_summary["verdict"] = verdict
              m10j_summary["next_gate"] = next_gate
              m10_summary["overall_pass"] = False
              m10_summary["blocker_count"] = len(blockers)
              m10_summary["verdict"] = verdict
              m10_summary["next_gate"] = next_gate
              write_local(run_dir, run_control_artifacts)
              for name, payload in run_control_artifacts.items():
                  key = f"{run_control_prefix}{name}"
                  try:
                      s3_put_json(s3, evidence_bucket, key, payload)
                  except (BotoCoreError, ClientError, ValueError):
                      pass

          print(json.dumps({
              "execution_id": execution_id,
              "overall_pass": m10j_summary["overall_pass"],
              "blocker_count": m10j_summary["blocker_count"],
              "verdict": m10j_summary["verdict"],
              "next_gate": m10j_summary["next_gate"],
              "run_dir": str(run_dir),
              "run_control_prefix": f"s3://{evidence_bucket}/{run_control_prefix}",
          }, ensure_ascii=True))

          if not m10j_summary["overall_pass"]:
              raise SystemExit(1)
          PY

      - name: Upload M10.D managed artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m10-defghij-managed-${{ steps.run_meta.outputs.timestamp }}
          path: |
            ${{ steps.run_meta.outputs.m10d_run_dir }}
            ${{ steps.run_meta.outputs.m10e_run_dir }}
            ${{ steps.run_meta.outputs.m10f_run_dir }}
            ${{ steps.run_meta.outputs.m10g_run_dir }}
            ${{ steps.run_meta.outputs.m10h_run_dir }}
            ${{ steps.run_meta.outputs.m10i_run_dir }}
            ${{ steps.run_meta.outputs.m10j_run_dir }}
          if-no-files-found: warn
