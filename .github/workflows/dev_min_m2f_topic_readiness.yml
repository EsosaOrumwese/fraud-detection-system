name: dev-min-m2f-topic-readiness

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: "AWS region for Terraform and verification commands"
        required: true
        type: string
      aws_role_to_assume:
        description: "OIDC role ARN used by GitHub Actions"
        required: true
        type: string
      tf_state_bucket:
        description: "Terraform backend bucket"
        required: false
        default: "fraud-platform-dev-min-tfstate"
        type: string
      tf_lock_table:
        description: "Terraform backend lock table"
        required: false
        default: "fraud-platform-dev-min-tf-locks"
        type: string
      tf_state_key_confluent:
        description: "Terraform state key for Confluent stack"
        required: false
        default: "dev_min/confluent/terraform.tfstate"
        type: string
      evidence_bucket:
        description: "Evidence bucket used for durable M2.F snapshot upload"
        required: false
        default: "fraud-platform-dev-min-evidence"
        type: string
      m2_execution_id:
        description: "Optional fixed execution id (default: m2_<timestamp>)"
        required: false
        default: ""
        type: string

permissions:
  contents: read
  id-token: write

env:
  TF_VAR_confluent_cloud_api_key: ${{ secrets.TF_VAR_CONFLUENT_CLOUD_API_KEY }}
  TF_VAR_confluent_cloud_api_secret: ${{ secrets.TF_VAR_CONFLUENT_CLOUD_API_SECRET }}
  TF_IN_AUTOMATION: "true"

concurrency:
  group: dev-min-m2f-topic-readiness-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  run_m2f:
    name: Apply Confluent stack and verify M2.F topic readiness
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Ensure Confluent management secrets are present
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${TF_VAR_confluent_cloud_api_key:-}" ]]; then
            echo "Missing TF_VAR_confluent_cloud_api_key mapping from GitHub secret TF_VAR_CONFLUENT_CLOUD_API_KEY."
            exit 1
          fi
          if [[ -z "${TF_VAR_confluent_cloud_api_secret:-}" ]]; then
            echo "Missing TF_VAR_confluent_cloud_api_secret mapping from GitHub secret TF_VAR_CONFLUENT_CLOUD_API_SECRET."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install verifier dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install kafka-python confluent-kafka

      - name: Compute execution metadata
        id: run_meta
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m2_execution_id }}" ]]; then
            M2_EXECUTION_ID="${{ inputs.m2_execution_id }}"
          else
            M2_EXECUTION_ID="m2_${TS}"
          fi
          OUTPUT_PATH="runs/dev_substrate/m2_f/${TS}/topic_readiness_snapshot.json"
          EVIDENCE_S3_URI="s3://${{ inputs.evidence_bucket }}/evidence/dev_min/substrate/${M2_EXECUTION_ID}/topic_readiness_snapshot.json"
          echo "timestamp=${TS}" >> "$GITHUB_OUTPUT"
          echo "m2_execution_id=${M2_EXECUTION_ID}" >> "$GITHUB_OUTPUT"
          echo "output_path=${OUTPUT_PATH}" >> "$GITHUB_OUTPUT"
          echo "evidence_s3_uri=${EVIDENCE_S3_URI}" >> "$GITHUB_OUTPUT"

      - name: Terraform init (Confluent stack)
        shell: bash
        run: |
          set -euo pipefail
          terraform -chdir=infra/terraform/dev_min/confluent init \
            -reconfigure \
            -backend-config="bucket=${{ inputs.tf_state_bucket }}" \
            -backend-config="key=${{ inputs.tf_state_key_confluent }}" \
            -backend-config="region=${{ inputs.aws_region }}" \
            -backend-config="dynamodb_table=${{ inputs.tf_lock_table }}" \
            -backend-config="encrypt=true"

      - name: Adopt pre-existing Kafka topics (best-effort)
        shell: bash
        run: |
          set -euo pipefail
          TF_DIR="infra/terraform/dev_min/confluent"

          tf_output_raw() {
            terraform -chdir="$TF_DIR" output -raw "$1" 2>/dev/null || true
          }

          CLUSTER_ID="$(tf_output_raw confluent_cluster_id)"
          REST_ENDPOINT="$(tf_output_raw kafka_rest_endpoint)"
          IMPORT_API_KEY="$(tf_output_raw runtime_kafka_api_key)"
          IMPORT_API_SECRET="$(tf_output_raw runtime_kafka_api_secret)"

          if [[ -z "$CLUSTER_ID" ]]; then
            echo "No existing cluster found in state; skipping topic import."
            exit 0
          fi

          if [[ -n "$REST_ENDPOINT" ]]; then
            export IMPORT_KAFKA_REST_ENDPOINT="$REST_ENDPOINT"
          fi

          if [[ -z "$IMPORT_API_KEY" || -z "$IMPORT_API_SECRET" ]]; then
            KEY_PATH="$(tf_output_raw ssm_confluent_api_key_path)"
            SECRET_PATH="$(tf_output_raw ssm_confluent_api_secret_path)"
            if [[ -n "$KEY_PATH" && -n "$SECRET_PATH" ]]; then
              IMPORT_API_KEY="$(aws ssm get-parameter --name "$KEY_PATH" --with-decryption --query 'Parameter.Value' --output text 2>/dev/null || true)"
              IMPORT_API_SECRET="$(aws ssm get-parameter --name "$SECRET_PATH" --with-decryption --query 'Parameter.Value' --output text 2>/dev/null || true)"
            fi
          fi

          if [[ -z "$IMPORT_API_KEY" || -z "$IMPORT_API_SECRET" ]]; then
            echo "Cluster exists (${CLUSTER_ID}) but import Kafka credentials are unavailable."
            echo "Required: runtime Kafka key/secret from Terraform outputs or SSM."
            exit 1
          fi

          export IMPORT_KAFKA_API_KEY="$IMPORT_API_KEY"
          export IMPORT_KAFKA_API_SECRET="$IMPORT_API_SECRET"

          TOPICS=(
            "fp.bus.control.v1"
            "fp.bus.traffic.fraud.v1"
            "fp.bus.context.arrival_events.v1"
            "fp.bus.context.arrival_entities.v1"
            "fp.bus.context.flow_anchor.fraud.v1"
            "fp.bus.rtdl.v1"
            "fp.bus.audit.v1"
            "fp.bus.case.triggers.v1"
            "fp.bus.labels.events.v1"
          )

          IMPORT_FAILURE=0
          for topic in "${TOPICS[@]}"; do
            IMPORT_ADDR="module.confluent.confluent_kafka_topic.topics[\"${topic}\"]"
            IMPORT_ID="${CLUSTER_ID}/${topic}"
            if IMPORT_OUTPUT="$(terraform -chdir="$TF_DIR" import "$IMPORT_ADDR" "$IMPORT_ID" 2>&1)"; then
              echo "Imported topic into state: ${topic}"
            elif printf '%s\n' "$IMPORT_OUTPUT" | grep -qi "already managed by Terraform"; then
              echo "Topic already managed in state: ${topic}"
            elif printf '%s\n' "$IMPORT_OUTPUT" | grep -qi "Cannot import non-existent remote object"; then
              echo "Topic does not exist yet (will be created by apply): ${topic}"
            else
              echo "Import failed for topic: ${topic}"
              printf '%s\n' "$IMPORT_OUTPUT"
              IMPORT_FAILURE=1
            fi
          done

          if [[ "$IMPORT_FAILURE" -ne 0 ]]; then
            echo "One or more Kafka topic imports failed unexpectedly."
            exit 1
          fi

      - name: Terraform apply (Confluent stack)
        shell: bash
        run: |
          set -euo pipefail
          terraform -chdir=infra/terraform/dev_min/confluent apply -input=false -auto-approve

      - name: Run canonical M2.F verifier
        shell: bash
        run: |
          set -euo pipefail
          python tools/dev_substrate/verify_m2f_topic_readiness.py \
            --aws-region "${{ inputs.aws_region }}" \
            --output "${{ steps.run_meta.outputs.output_path }}" \
            --evidence-s3-uri "${{ steps.run_meta.outputs.evidence_s3_uri }}"

      - name: Upload M2.F CI evidence artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: m2f-topic-readiness-${{ steps.run_meta.outputs.timestamp }}
          path: ${{ steps.run_meta.outputs.output_path }}
          if-no-files-found: warn
