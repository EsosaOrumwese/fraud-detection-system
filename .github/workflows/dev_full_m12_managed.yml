name: dev-full-m12-managed

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: AWS region
        required: true
        default: eu-west-2
        type: string
      aws_role_to_assume:
        description: OIDC role ARN
        required: true
        type: string
      evidence_bucket:
        description: S3 evidence bucket
        required: true
        default: fraud-platform-dev-full-evidence
        type: string
      m12_subphase:
        description: "M12 subphase target (single managed lane)"
        required: true
        default: A
        type: choice
        options:
          - A
          - B
          - C
          - D
          - E
          - F
          - G
          - H
          - I
          - J
      execution_mode:
        description: "Execution mode"
        required: true
        default: materialization_check
        type: choice
        options:
          - materialization_check
          - m12a_execute
          - m12b_execute
          - m12c_execute
          - m12d_execute
          - m12e_execute
          - m12f_execute
      upstream_m11j_execution:
        description: Upstream M11.J execution id (entry readiness ref)
        required: true
        default: m11j_closure_sync_20260227T104756Z
        type: string
      upstream_m12a_execution:
        description: Upstream M12.A execution id (M12.B entry readiness ref)
        required: true
        default: m12a_handle_closure_20260227T121911Z
        type: string
      upstream_m12b_execution:
        description: Upstream M12.B execution id (M12.C entry readiness ref)
        required: true
        default: m12b_candidate_eligibility_20260227T123135Z
        type: string
      upstream_m12c_execution:
        description: Upstream M12.C execution id (M12.D entry readiness ref)
        required: true
        default: m12c_compatibility_precheck_20260227T130306Z
        type: string
      upstream_m12d_execution:
        description: Upstream M12.D execution id (M12.E entry readiness ref)
        required: true
        default: m12d_promotion_commit_20260227T144832Z
        type: string
      upstream_m12e_execution:
        description: Upstream M12.E execution id (M12.F entry readiness ref)
        required: true
        default: m12e_rollback_drill_20260227T165747Z
        type: string
      m12_execution_id:
        description: Optional fixed execution id override
        required: false
        default: ""
        type: string

permissions:
  contents: read
  id-token: write

concurrency:
  group: dev-full-m12-managed-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  run_m12_managed:
    name: Run M12.${{ inputs.m12_subphase }} managed lane
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate supported mode and subphase
        shell: bash
        run: |
          set -euo pipefail
          case "${{ inputs.m12_subphase }}" in
            A|B|C|D|E|F|G|H|I|J) ;;
            *)
              echo "Unsupported m12_subphase='${{ inputs.m12_subphase }}'"
              exit 1
              ;;
          esac
          if [[ "${{ inputs.execution_mode }}" != "materialization_check" && "${{ inputs.execution_mode }}" != "m12a_execute" && "${{ inputs.execution_mode }}" != "m12b_execute" && "${{ inputs.execution_mode }}" != "m12c_execute" && "${{ inputs.execution_mode }}" != "m12d_execute" && "${{ inputs.execution_mode }}" != "m12e_execute" && "${{ inputs.execution_mode }}" != "m12f_execute" ]]; then
            echo "Unsupported execution_mode='${{ inputs.execution_mode }}'"
            exit 1
          fi
          if [[ "${{ inputs.execution_mode }}" == "m12a_execute" && "${{ inputs.m12_subphase }}" != "A" ]]; then
            echo "execution_mode='m12a_execute' currently supports only m12_subphase='A'."
            exit 1
          fi
          if [[ "${{ inputs.execution_mode }}" == "m12b_execute" && "${{ inputs.m12_subphase }}" != "B" ]]; then
            echo "execution_mode='m12b_execute' currently supports only m12_subphase='B'."
            exit 1
          fi
          if [[ "${{ inputs.execution_mode }}" == "m12c_execute" && "${{ inputs.m12_subphase }}" != "C" ]]; then
            echo "execution_mode='m12c_execute' currently supports only m12_subphase='C'."
            exit 1
          fi
          if [[ "${{ inputs.execution_mode }}" == "m12d_execute" && "${{ inputs.m12_subphase }}" != "D" ]]; then
            echo "execution_mode='m12d_execute' currently supports only m12_subphase='D'."
            exit 1
          fi
          if [[ "${{ inputs.execution_mode }}" == "m12e_execute" && "${{ inputs.m12_subphase }}" != "E" ]]; then
            echo "execution_mode='m12e_execute' currently supports only m12_subphase='E'."
            exit 1
          fi
          if [[ "${{ inputs.execution_mode }}" == "m12f_execute" && "${{ inputs.m12_subphase }}" != "F" ]]; then
            echo "execution_mode='m12f_execute' currently supports only m12_subphase='F'."
            exit 1
          fi

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore

      - name: Compute execution metadata
        id: run_meta
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ -n "${{ inputs.m12_execution_id }}" ]]; then
            EXEC_ID="${{ inputs.m12_execution_id }}"
          else
            case "${{ inputs.m12_subphase }}" in
              A) EXEC_ID="m12a_handle_closure_${TS}" ;;
              B) EXEC_ID="m12b_candidate_eligibility_${TS}" ;;
              C) EXEC_ID="m12c_compatibility_precheck_${TS}" ;;
              D) EXEC_ID="m12d_promotion_commit_${TS}" ;;
              E) EXEC_ID="m12e_rollback_drill_${TS}" ;;
              F) EXEC_ID="m12f_active_resolution_${TS}" ;;
              G) EXEC_ID="m12g_governance_append_${TS}" ;;
              H) EXEC_ID="m12h_p15_gate_rollup_${TS}" ;;
              I) EXEC_ID="m12i_phase_cost_outcome_${TS}" ;;
              J) EXEC_ID="m12j_closure_sync_${TS}" ;;
            esac
          fi
          echo "execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=runs/dev_substrate/dev_full/m12/${EXEC_ID}" >> "$GITHUB_OUTPUT"

      - name: Execute M12-B0 managed lane materialization check
        if: ${{ inputs.execution_mode == 'materialization_check' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          AWS_REGION: ${{ inputs.aws_region }}
          M12_SUBPHASE: ${{ inputs.m12_subphase }}
          EXECUTION_MODE: ${{ inputs.execution_mode }}
          UPSTREAM_M11J_EXEC: ${{ inputs.upstream_m11j_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import json
          import os
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              raw = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(raw)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          bucket = os.environ["EVIDENCE_BUCKET"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          subphase = os.environ["M12_SUBPHASE"].strip()
          mode = os.environ["EXECUTION_MODE"].strip()
          upstream_m11j = os.environ["UPSTREAM_M11J_EXEC"].strip()

          s3 = boto3.client("s3", region_name=region)
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          supported_subphases = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J"]
          if subphase not in supported_subphases:
              blockers.append({"code": "M12-B0", "message": f"Unsupported subphase '{subphase}'."})
          if mode != "materialization_check":
              blockers.append({"code": "M12-B0", "message": f"Unsupported execution mode '{mode}'."})

          routing_map = {
              "A": "m12a_handle_closure_<ts>",
              "B": "m12b_candidate_eligibility_<ts>",
              "C": "m12c_compatibility_precheck_<ts>",
              "D": "m12d_promotion_commit_<ts>",
              "E": "m12e_rollback_drill_<ts>",
              "F": "m12f_active_resolution_<ts>",
              "G": "m12g_governance_append_<ts>",
              "H": "m12h_p15_gate_rollup_<ts>",
              "I": "m12i_phase_cost_outcome_<ts>",
              "J": "m12j_closure_sync_<ts>",
          }

          upstream_summary_key = f"evidence/dev_full/run_control/{upstream_m11j}/m11j_execution_summary.json"
          upstream_summary: dict[str, Any] = {}
          try:
              upstream_summary = s3_get_json(s3, bucket, upstream_summary_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": upstream_summary_key, "error": type(exc).__name__})
              blockers.append({"code": "M12-B0", "message": "Unreadable M11.J upstream summary."})

          if upstream_summary:
              if not bool(upstream_summary.get("overall_pass", False)):
                  blockers.append({"code": "M12-B0", "message": "M11.J upstream summary overall_pass is false."})
              if str(upstream_summary.get("next_gate", "")).strip() != "M12_READY":
                  blockers.append({"code": "M12-B0", "message": "M11.J upstream summary next_gate is not M12_READY."})

          snapshot = {
              "captured_at_utc": now(),
              "phase": "M12.B0",
              "execution_id": exec_id,
              "mode": mode,
              "requested_subphase": subphase,
              "supported_subphases": supported_subphases,
              "deterministic_execution_routing": routing_map,
              "upstream_m11j_execution": upstream_m11j,
              "upstream_m11j_summary_key": upstream_summary_key,
              "entry_ready": bool(upstream_summary.get("overall_pass", False)) and str(upstream_summary.get("next_gate", "")).strip() == "M12_READY",
              "managed_lane_materialized": True,
          }

          dispatchability = {
              "captured_at_utc": now(),
              "phase": "M12.B0",
              "execution_id": exec_id,
              "requested_subphase": subphase,
              "dispatchable": True,
              "single_workflow_lane": "dev-full-m12-managed",
              "authoritative_path": "managed_only",
          }

          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M12.B0",
              "execution_id": exec_id,
              "blocker_count": 0,
              "blockers": [],
              "overall_pass": False,
          }

          execution_summary = {
              "captured_at_utc": now(),
              "phase": "M12.B0",
              "execution_id": exec_id,
              "requested_subphase": subphase,
              "overall_pass": False,
              "blocker_count": 0,
              "verdict": "HOLD_REMEDIATE",
              "next_gate": "HOLD_REMEDIATE",
              "artifact_keys": {},
          }

          artifacts = {
              "m12_managed_lane_materialization_snapshot.json": snapshot,
              "m12_subphase_dispatchability_snapshot.json": dispatchability,
              "m12b0_blocker_register.json": blocker_register,
              "m12b0_execution_summary.json": execution_summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B0", "message": f"Failed to publish {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          verdict = "ADVANCE_TO_M12_A" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M12.A_READY" if overall_pass else "HOLD_REMEDIATE"
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          execution_summary["overall_pass"] = overall_pass
          execution_summary["blocker_count"] = len(blockers)
          execution_summary["verdict"] = verdict
          execution_summary["next_gate"] = next_gate
          execution_summary["artifact_keys"] = {
              "m12_managed_lane_materialization_snapshot": run_control_prefix + "m12_managed_lane_materialization_snapshot.json",
              "m12_subphase_dispatchability_snapshot": run_control_prefix + "m12_subphase_dispatchability_snapshot.json",
              "m12b0_blocker_register": run_control_prefix + "m12b0_blocker_register.json",
              "m12b0_execution_summary": run_control_prefix + "m12b0_execution_summary.json",
          }
          execution_summary["read_errors"] = read_errors
          execution_summary["upload_errors"] = upload_errors

          for fname, payload in {
              "m12b0_blocker_register.json": blocker_register,
              "m12b0_execution_summary.json": execution_summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "requested_subphase": subphase,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "evidence_prefix": f"s3://{bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Execute M12.A handle closure (managed)
        if: ${{ inputs.execution_mode == 'm12a_execute' && inputs.m12_subphase == 'A' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          AWS_REGION: ${{ inputs.aws_region }}
          UPSTREAM_M11J_EXEC: ${{ inputs.upstream_m11j_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              raw = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(raw)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          def parse_handles(path: Path) -> dict[str, str]:
              rx = re.compile(r"^\*\s+`([A-Z0-9_]+)\s*=\s*(.+)`\s*$")
              out: dict[str, str] = {}
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if (raw.startswith('"') and raw.endswith('"')) or (raw.startswith("'") and raw.endswith("'")):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          bucket = os.environ["EVIDENCE_BUCKET"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          upstream_m11j = os.environ["UPSTREAM_M11J_EXEC"].strip()

          s3 = boto3.client("s3", region_name=region)
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          upstream_key = f"evidence/dev_full/run_control/{upstream_m11j}/m11j_execution_summary.json"
          upstream_summary: dict[str, Any] = {}
          try:
              upstream_summary = s3_get_json(s3, bucket, upstream_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": upstream_key, "error": type(exc).__name__})
              blockers.append({"code": "M12-B1", "message": "Unreadable M11.J upstream summary."})

          platform_run_id = ""
          scenario_run_id = ""
          if upstream_summary:
              platform_run_id = str(upstream_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(upstream_summary.get("scenario_run_id", "")).strip()
              if not bool(upstream_summary.get("overall_pass", False)):
                  blockers.append({"code": "M12-B1", "message": "M11.J upstream summary overall_pass is false."})
              try:
                  blocker_count = int(str(upstream_summary.get("blocker_count", "")).strip())
              except Exception:
                  blocker_count = -1
              if blocker_count != 0:
                  blockers.append({"code": "M12-B1", "message": "M11.J upstream blocker_count is not zero."})
              if str(upstream_summary.get("next_gate", "")).strip() != "M12_READY":
                  blockers.append({"code": "M12-B1", "message": "M11.J upstream next_gate is not M12_READY."})
              if not platform_run_id or not scenario_run_id:
                  blockers.append({"code": "M12-B1", "message": "M11.J upstream run scope is incomplete."})

          required_handles = [
              "MPR_PROMOTION_RECEIPT_PATH_PATTERN",
              "MPR_ROLLBACK_DRILL_PATH_PATTERN",
              "FP_BUS_LEARNING_REGISTRY_EVENTS_V1",
              "GOV_APPEND_LOG_PATH_PATTERN",
              "GOV_RUN_CLOSE_MARKER_PATH_PATTERN",
              "MF_CANDIDATE_BUNDLE_PATH_PATTERN",
              "PHASE_BUDGET_ENVELOPE_PATH_PATTERN",
              "PHASE_COST_OUTCOME_RECEIPT_PATH_PATTERN",
          ]
          handles_path = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")
          handles: dict[str, str] = {}
          try:
              handles = parse_handles(handles_path)
          except Exception as exc:
              blockers.append({"code": "M12-B1", "message": f"Unable to parse handles registry: {type(exc).__name__}."})

          resolved_handles: dict[str, str] = {}
          unresolved: list[str] = []
          for key in required_handles:
              value = str(handles.get(key, "")).strip()
              resolved_handles[key] = value
              if (not value) or (value.upper() == "TO_PIN"):
                  unresolved.append(key)
          if unresolved:
              blockers.append({"code": "M12-B1", "message": f"Unresolved required handles: {', '.join(unresolved)}."})

          handle_snapshot = {
              "captured_at_utc": now(),
              "phase": "M12.A",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_m11j_execution": upstream_m11j,
              "upstream_summary_key": upstream_key,
              "required_handles": required_handles,
              "resolved_handles": resolved_handles,
              "unresolved_handles": unresolved,
              "handle_matrix_complete": len(unresolved) == 0,
          }

          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M12.A",
              "phase_id": "P15",
              "execution_id": exec_id,
              "blocker_count": 0,
              "blockers": [],
              "overall_pass": False,
          }

          execution_summary = {
              "captured_at_utc": now(),
              "phase": "M12.A",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": False,
              "blocker_count": 0,
              "verdict": "HOLD_REMEDIATE",
              "next_gate": "HOLD_REMEDIATE",
              "upstream_refs": {"m11j_execution_id": upstream_m11j},
              "artifact_keys": {},
          }

          artifacts = {
              "m12a_handle_closure_snapshot.json": handle_snapshot,
              "m12a_blocker_register.json": blocker_register,
              "m12a_execution_summary.json": execution_summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B1", "message": f"Failed to publish {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          verdict = "ADVANCE_TO_M12_B" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M12.B_READY" if overall_pass else "HOLD_REMEDIATE"
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          execution_summary["overall_pass"] = overall_pass
          execution_summary["blocker_count"] = len(blockers)
          execution_summary["verdict"] = verdict
          execution_summary["next_gate"] = next_gate
          execution_summary["artifact_keys"] = {
              "m12a_handle_closure_snapshot": run_control_prefix + "m12a_handle_closure_snapshot.json",
              "m12a_blocker_register": run_control_prefix + "m12a_blocker_register.json",
              "m12a_execution_summary": run_control_prefix + "m12a_execution_summary.json",
          }
          execution_summary["read_errors"] = read_errors
          execution_summary["upload_errors"] = upload_errors

          for fname, payload in {
              "m12a_blocker_register.json": blocker_register,
              "m12a_execution_summary.json": execution_summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "evidence_prefix": f"s3://{bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Execute M12.B candidate eligibility precheck (managed)
        if: ${{ inputs.execution_mode == 'm12b_execute' && inputs.m12_subphase == 'B' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          AWS_REGION: ${{ inputs.aws_region }}
          UPSTREAM_M12A_EXEC: ${{ inputs.upstream_m12a_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import json
          import os
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              raw = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(raw)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def s3_parse_uri(uri: str) -> tuple[str, str]:
              if not uri.startswith("s3://"):
                  raise ValueError("not_s3_uri")
              tail = uri[5:]
              if "/" not in tail:
                  raise ValueError("missing_key")
              bucket, key = tail.split("/", 1)
              if not bucket or not key:
                  raise ValueError("incomplete_s3_uri")
              return bucket, key

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          default_bucket = os.environ["EVIDENCE_BUCKET"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          upstream_m12a = os.environ["UPSTREAM_M12A_EXEC"].strip()

          s3 = boto3.client("s3", region_name=region)
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          m12a_summary_key = f"evidence/dev_full/run_control/{upstream_m12a}/m12a_execution_summary.json"
          m12a_summary: dict[str, Any] = {}
          try:
              m12a_summary = s3_get_json(s3, default_bucket, m12a_summary_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": m12a_summary_key, "error": type(exc).__name__})
              blockers.append({"code": "M12-B2", "message": "Unreadable M12.A execution summary."})

          platform_run_id = ""
          scenario_run_id = ""
          upstream_m11j = ""
          if m12a_summary:
              platform_run_id = str(m12a_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m12a_summary.get("scenario_run_id", "")).strip()
              upstream_m11j = str((m12a_summary.get("upstream_refs") or {}).get("m11j_execution_id", "")).strip()
              if not bool(m12a_summary.get("overall_pass", False)):
                  blockers.append({"code": "M12-B2", "message": "M12.A overall_pass is false."})
              try:
                  m12a_blocker_count = int(str(m12a_summary.get("blocker_count", "")).strip())
              except Exception:
                  m12a_blocker_count = -1
              if m12a_blocker_count != 0:
                  blockers.append({"code": "M12-B2", "message": "M12.A blocker_count is not zero."})
              if str(m12a_summary.get("next_gate", "")).strip() != "M12.B_READY":
                  blockers.append({"code": "M12-B2", "message": "M12.A next_gate is not M12.B_READY."})
              if not platform_run_id or not scenario_run_id:
                  blockers.append({"code": "M12-B2", "message": "M12.A run scope is incomplete."})
              if not upstream_m11j:
                  blockers.append({"code": "M12-B2", "message": "M12.A upstream m11j_execution_id is missing."})

          m11j_summary_key = f"evidence/dev_full/run_control/{upstream_m11j}/m11j_execution_summary.json" if upstream_m11j else ""
          m11j_summary: dict[str, Any] = {}
          if m11j_summary_key:
              try:
                  m11j_summary = s3_get_json(s3, default_bucket, m11j_summary_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": m11j_summary_key, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B2", "message": "Unreadable M11.J execution summary."})

          upstream_m11i = ""
          if m11j_summary:
              if not bool(m11j_summary.get("overall_pass", False)):
                  blockers.append({"code": "M12-B2", "message": "M11.J overall_pass is false."})
              if str(m11j_summary.get("next_gate", "")).strip() != "M12_READY":
                  blockers.append({"code": "M12-B2", "message": "M11.J next_gate is not M12_READY."})
              upstream_m11i = str((m11j_summary.get("upstream_refs") or {}).get("m11i_execution_id", "")).strip()
              if not upstream_m11i:
                  blockers.append({"code": "M12-B2", "message": "M11.J upstream m11i_execution_id is missing."})

          handoff_key = f"evidence/dev_full/run_control/{upstream_m11i}/m12_handoff_pack.json" if upstream_m11i else ""
          handoff_pack: dict[str, Any] = {}
          if handoff_key:
              try:
                  handoff_pack = s3_get_json(s3, default_bucket, handoff_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": handoff_key, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B2", "message": "Unreadable m12_handoff_pack.json from M11.I."})

          if handoff_pack:
              hp_platform_run = str(handoff_pack.get("platform_run_id", "")).strip()
              hp_scenario_run = str(handoff_pack.get("scenario_run_id", "")).strip()
              if hp_platform_run != platform_run_id or hp_scenario_run != scenario_run_id:
                  blockers.append({"code": "M12-B2", "message": "Run-scope mismatch between M12.A and M11.I handoff pack."})

          required_ref_keys = [
              "mf_candidate_bundle_ref",
              "m11_model_operability_report_ref",
              "m11_eval_report_ref",
              "m11_eval_vs_baseline_report_ref",
              "mf_leakage_provenance_check_ref",
              "m11_reproducibility_check_ref",
              "m11f_mlflow_lineage_snapshot_ref",
          ]
          handoff_required_refs = handoff_pack.get("required_refs") if isinstance(handoff_pack.get("required_refs"), dict) else {}
          missing_required_refs: list[str] = []
          readable_ref_report: dict[str, dict[str, Any]] = {}
          loaded_refs: dict[str, dict[str, Any]] = {}
          for ref_key in required_ref_keys:
              ref_value = str(handoff_required_refs.get(ref_key, "")).strip()
              ref_state = {"ref": ref_value, "readable": False, "error": ""}
              if not ref_value:
                  missing_required_refs.append(ref_key)
                  ref_state["error"] = "missing_ref"
                  readable_ref_report[ref_key] = ref_state
                  continue
              try:
                  ref_bucket, ref_key_path = s3_parse_uri(ref_value)
                  ref_payload = s3_get_json(s3, ref_bucket, ref_key_path)
                  ref_state["readable"] = True
                  loaded_refs[ref_key] = ref_payload
              except Exception as exc:
                  ref_state["error"] = type(exc).__name__
                  read_errors.append({"surface": ref_value, "error": type(exc).__name__})
              readable_ref_report[ref_key] = ref_state

          if missing_required_refs:
              blockers.append({"code": "M12-B2", "message": f"Missing required handoff refs: {', '.join(missing_required_refs)}."})

          unreadable_refs = [k for k, v in readable_ref_report.items() if not bool(v.get("readable", False))]
          if unreadable_refs:
              blockers.append({"code": "M12-B2", "message": f"Unreadable required refs: {', '.join(unreadable_refs)}."})

          candidate_bundle = loaded_refs.get("mf_candidate_bundle_ref", {})
          candidate_checks = {
              "present": bool(candidate_bundle),
              "bundle_status_candidate": False,
              "run_scope_match": False,
              "required_fields_present": False,
              "eval_gate_results_all_true": False,
              "lineage_fields_present": False,
          }
          required_candidate_fields = [
              "bundle_id",
              "bundle_status",
              "model",
              "metrics",
              "lineage",
              "rollback_pointers",
              "platform_run_id",
              "scenario_run_id",
          ]
          missing_candidate_fields: list[str] = []
          if candidate_bundle:
              candidate_checks["bundle_status_candidate"] = str(candidate_bundle.get("bundle_status", "")).strip() == "CANDIDATE"
              candidate_checks["run_scope_match"] = (
                  str(candidate_bundle.get("platform_run_id", "")).strip() == platform_run_id
                  and str(candidate_bundle.get("scenario_run_id", "")).strip() == scenario_run_id
              )
              for key in required_candidate_fields:
                  v = candidate_bundle.get(key)
                  if v is None:
                      missing_candidate_fields.append(key)
                  elif isinstance(v, str) and not v.strip():
                      missing_candidate_fields.append(key)
              candidate_checks["required_fields_present"] = len(missing_candidate_fields) == 0
              eval_gate = ((candidate_bundle.get("metrics") or {}).get("eval_gate_results")) if isinstance(candidate_bundle.get("metrics"), dict) else None
              if isinstance(eval_gate, dict):
                  gate_keys = ["compatibility", "leakage", "performance", "stability"]
                  candidate_checks["eval_gate_results_all_true"] = all(bool(eval_gate.get(k, False)) for k in gate_keys)
              lineage = candidate_bundle.get("lineage") if isinstance(candidate_bundle.get("lineage"), dict) else {}
              lineage_keys = ["m11d_execution_id", "m11e_execution_id", "m11f_execution_id", "mlflow_run_id"]
              candidate_checks["lineage_fields_present"] = all(bool(str(lineage.get(k, "")).strip()) for k in lineage_keys)

          if not candidate_checks["present"]:
              blockers.append({"code": "M12-B2", "message": "Candidate bundle is unreadable."})
          if candidate_bundle and not candidate_checks["bundle_status_candidate"]:
              blockers.append({"code": "M12-B2", "message": "Candidate bundle status is not CANDIDATE."})
          if candidate_bundle and not candidate_checks["run_scope_match"]:
              blockers.append({"code": "M12-B2", "message": "Candidate bundle run scope mismatches M12 run scope."})
          if candidate_bundle and not candidate_checks["required_fields_present"]:
              blockers.append({"code": "M12-B2", "message": f"Candidate bundle missing required fields: {', '.join(missing_candidate_fields)}."})
          if candidate_bundle and not candidate_checks["eval_gate_results_all_true"]:
              blockers.append({"code": "M12-B2", "message": "Candidate bundle eval gate results are not all true."})
          if candidate_bundle and not candidate_checks["lineage_fields_present"]:
              blockers.append({"code": "M12-B2", "message": "Candidate bundle lineage fields are incomplete."})

          eligibility_snapshot = {
              "captured_at_utc": now(),
              "phase": "M12.B",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_refs": {
                  "m12a_execution_id": upstream_m12a,
                  "m12a_execution_summary_key": m12a_summary_key,
                  "m11j_execution_id": upstream_m11j,
                  "m11j_execution_summary_key": m11j_summary_key,
                  "m11i_execution_id": upstream_m11i,
                  "m12_handoff_pack_key": handoff_key,
              },
              "required_handoff_refs": required_ref_keys,
              "readable_ref_report": readable_ref_report,
              "candidate_checks": candidate_checks,
              "missing_candidate_fields": missing_candidate_fields,
              "candidate_bundle_bundle_id": str(candidate_bundle.get("bundle_id", "")).strip() if candidate_bundle else "",
          }

          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M12.B",
              "phase_id": "P15",
              "execution_id": exec_id,
              "blocker_count": 0,
              "blockers": [],
              "overall_pass": False,
          }

          execution_summary = {
              "captured_at_utc": now(),
              "phase": "M12.B",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": False,
              "blocker_count": 0,
              "verdict": "HOLD_REMEDIATE",
              "next_gate": "HOLD_REMEDIATE",
              "upstream_refs": {
                  "m12a_execution_id": upstream_m12a,
                  "m11j_execution_id": upstream_m11j,
                  "m11i_execution_id": upstream_m11i,
              },
              "artifact_keys": {},
          }

          artifacts = {
              "m12b_candidate_eligibility_snapshot.json": eligibility_snapshot,
              "m12b_blocker_register.json": blocker_register,
              "m12b_execution_summary.json": execution_summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, default_bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B2", "message": f"Failed to publish {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          verdict = "ADVANCE_TO_M12_C" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M12.C_READY" if overall_pass else "HOLD_REMEDIATE"
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          execution_summary["overall_pass"] = overall_pass
          execution_summary["blocker_count"] = len(blockers)
          execution_summary["verdict"] = verdict
          execution_summary["next_gate"] = next_gate
          execution_summary["artifact_keys"] = {
              "m12b_candidate_eligibility_snapshot": run_control_prefix + "m12b_candidate_eligibility_snapshot.json",
              "m12b_blocker_register": run_control_prefix + "m12b_blocker_register.json",
              "m12b_execution_summary": run_control_prefix + "m12b_execution_summary.json",
          }
          execution_summary["read_errors"] = read_errors
          execution_summary["upload_errors"] = upload_errors

          for fname, payload in {
              "m12b_blocker_register.json": blocker_register,
              "m12b_execution_summary.json": execution_summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, default_bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "evidence_prefix": f"s3://{default_bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Execute M12.C compatibility prechecks (managed)
        if: ${{ inputs.execution_mode == 'm12c_execute' && inputs.m12_subphase == 'C' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          AWS_REGION: ${{ inputs.aws_region }}
          UPSTREAM_M12B_EXEC: ${{ inputs.upstream_m12b_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              raw = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(raw)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def s3_parse_uri(uri: str) -> tuple[str, str]:
              if not uri.startswith("s3://"):
                  raise ValueError("not_s3_uri")
              tail = uri[5:]
              if "/" not in tail:
                  raise ValueError("missing_key")
              bucket, key = tail.split("/", 1)
              if not bucket or not key:
                  raise ValueError("incomplete_s3_uri")
              return bucket, key

          def parse_handles(path: Path) -> dict[str, str]:
              rx = re.compile(r"^\*\s+`([A-Z0-9_]+)\s*=\s*(.+)`\s*$")
              out: dict[str, str] = {}
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if (raw.startswith('"') and raw.endswith('"')) or (raw.startswith("'") and raw.endswith("'")):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          default_bucket = os.environ["EVIDENCE_BUCKET"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          upstream_m12b = os.environ["UPSTREAM_M12B_EXEC"].strip()

          s3 = boto3.client("s3", region_name=region)
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []

          m12b_summary_key = f"evidence/dev_full/run_control/{upstream_m12b}/m12b_execution_summary.json"
          m12b_snapshot_key = f"evidence/dev_full/run_control/{upstream_m12b}/m12b_candidate_eligibility_snapshot.json"
          m12b_summary: dict[str, Any] = {}
          m12b_snapshot: dict[str, Any] = {}
          try:
              m12b_summary = s3_get_json(s3, default_bucket, m12b_summary_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": m12b_summary_key, "error": type(exc).__name__})
              blockers.append({"code": "M12-B3", "message": "Unreadable M12.B execution summary."})
          try:
              m12b_snapshot = s3_get_json(s3, default_bucket, m12b_snapshot_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": m12b_snapshot_key, "error": type(exc).__name__})
              blockers.append({"code": "M12-B3", "message": "Unreadable M12.B eligibility snapshot."})

          platform_run_id = ""
          scenario_run_id = ""
          upstream_m11i = ""
          if m12b_summary:
              platform_run_id = str(m12b_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m12b_summary.get("scenario_run_id", "")).strip()
              upstream_m11i = str((m12b_summary.get("upstream_refs") or {}).get("m11i_execution_id", "")).strip()
              if not bool(m12b_summary.get("overall_pass", False)):
                  blockers.append({"code": "M12-B3", "message": "M12.B overall_pass is false."})
              try:
                  m12b_blocker_count = int(str(m12b_summary.get("blocker_count", "")).strip())
              except Exception:
                  m12b_blocker_count = -1
              if m12b_blocker_count != 0:
                  blockers.append({"code": "M12-B3", "message": "M12.B blocker_count is not zero."})
              if str(m12b_summary.get("next_gate", "")).strip() != "M12.C_READY":
                  blockers.append({"code": "M12-B3", "message": "M12.B next_gate is not M12.C_READY."})
              if not platform_run_id or not scenario_run_id:
                  blockers.append({"code": "M12-B3", "message": "M12.B run scope is incomplete."})
              if not upstream_m11i:
                  blockers.append({"code": "M12-B3", "message": "M12.B upstream m11i_execution_id is missing."})

          handoff_key = f"evidence/dev_full/run_control/{upstream_m11i}/m12_handoff_pack.json" if upstream_m11i else ""
          handoff_pack: dict[str, Any] = {}
          if handoff_key:
              try:
                  handoff_pack = s3_get_json(s3, default_bucket, handoff_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": handoff_key, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B3", "message": "Unreadable M11.I m12_handoff_pack.json."})

          required_refs = handoff_pack.get("required_refs") if isinstance(handoff_pack.get("required_refs"), dict) else {}
          candidate_ref = str(required_refs.get("mf_candidate_bundle_ref", "")).strip()
          eval_gate_ref = str(required_refs.get("m11_eval_vs_baseline_report_ref", "")).strip()
          if not candidate_ref:
              blockers.append({"code": "M12-B3", "message": "Handoff missing mf_candidate_bundle_ref."})
          if not eval_gate_ref:
              blockers.append({"code": "M12-B3", "message": "Handoff missing m11_eval_vs_baseline_report_ref."})

          candidate_bundle: dict[str, Any] = {}
          if candidate_ref:
              try:
                  cb_bucket, cb_key = s3_parse_uri(candidate_ref)
                  candidate_bundle = s3_get_json(s3, cb_bucket, cb_key)
              except Exception as exc:
                  read_errors.append({"surface": candidate_ref, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B3", "message": "Candidate bundle is unreadable for compatibility checks."})

          feature_contract_checks = {
              "fingerprint_ref_present": False,
              "fingerprint_readable": False,
              "fingerprint_required_fields_present": False,
              "fingerprint_join_scope_matches_run": False,
          }
          fingerprint_required_fields = ["replay_basis", "feature_asof_utc", "label_asof_utc", "feature_def_set"]
          fingerprint_missing_fields: list[str] = []
          dataset_fingerprint_ref = ""
          dataset_fingerprint: dict[str, Any] = {}
          if candidate_bundle:
              dataset_fingerprint_ref = str((candidate_bundle.get("lineage") or {}).get("m10g_fingerprint_ref", "")).strip()
              feature_contract_checks["fingerprint_ref_present"] = bool(dataset_fingerprint_ref)
              if not dataset_fingerprint_ref:
                  blockers.append({"code": "M12-B3", "message": "Candidate bundle is missing lineage.m10g_fingerprint_ref."})
          if dataset_fingerprint_ref:
              try:
                  fp_bucket, fp_key = s3_parse_uri(dataset_fingerprint_ref)
                  dataset_fingerprint = s3_get_json(s3, fp_bucket, fp_key)
                  feature_contract_checks["fingerprint_readable"] = True
              except Exception as exc:
                  read_errors.append({"surface": dataset_fingerprint_ref, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B3", "message": "Dataset fingerprint is unreadable."})
          if dataset_fingerprint:
              req_values = dataset_fingerprint.get("required_field_values") if isinstance(dataset_fingerprint.get("required_field_values"), dict) else {}
              for k in fingerprint_required_fields:
                  v = req_values.get(k)
                  if v is None or (isinstance(v, str) and not v.strip()):
                      fingerprint_missing_fields.append(k)
              feature_contract_checks["fingerprint_required_fields_present"] = len(fingerprint_missing_fields) == 0
              if fingerprint_missing_fields:
                  blockers.append({"code": "M12-B3", "message": f"Dataset fingerprint missing required_field_values: {', '.join(fingerprint_missing_fields)}."})
              join_scope = str(req_values.get("join_scope", "")).strip()
              feature_contract_checks["fingerprint_join_scope_matches_run"] = (
                  f"platform_run_id={platform_run_id}" in join_scope and f"scenario_run_id={scenario_run_id}" in join_scope
              )
              if not feature_contract_checks["fingerprint_join_scope_matches_run"]:
                  blockers.append({"code": "M12-B3", "message": "Dataset fingerprint join_scope does not match M12 run scope."})

          policy_degrade_checks = {
              "m11_eval_gate_readable": False,
              "m11_eval_compatibility_true": False,
              "candidate_eval_compatibility_true": False,
              "required_vs_degrade_mask_compatible": True,
          }
          m11e_snapshot_key = str((candidate_bundle.get("rollback_pointers") or {}).get("m11e_snapshot_key", "")).strip() if candidate_bundle else ""
          m11e_snapshot: dict[str, Any] = {}
          if not m11e_snapshot_key:
              blockers.append({"code": "M12-B3", "message": "Candidate rollback pointers missing m11e_snapshot_key."})
          else:
              try:
                  m11e_snapshot = s3_get_json(s3, default_bucket, m11e_snapshot_key)
                  policy_degrade_checks["m11_eval_gate_readable"] = True
              except Exception as exc:
                  read_errors.append({"surface": m11e_snapshot_key, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B3", "message": "M11.E snapshot unreadable from candidate rollback pointers."})

          if m11e_snapshot:
              gate_results = m11e_snapshot.get("gate_results") if isinstance(m11e_snapshot.get("gate_results"), dict) else {}
              policy_degrade_checks["m11_eval_compatibility_true"] = bool(gate_results.get("compatibility", False))
              if not policy_degrade_checks["m11_eval_compatibility_true"]:
                  blockers.append({"code": "M12-B3", "message": "M11.E compatibility gate is false."})

          if candidate_bundle:
              c_eval = ((candidate_bundle.get("metrics") or {}).get("eval_gate_results")) if isinstance(candidate_bundle.get("metrics"), dict) else {}
              policy_degrade_checks["candidate_eval_compatibility_true"] = bool(c_eval.get("compatibility", False))
              if not policy_degrade_checks["candidate_eval_compatibility_true"]:
                  blockers.append({"code": "M12-B3", "message": "Candidate eval gate compatibility is false."})
              required_caps = candidate_bundle.get("required_capabilities")
              degrade_mask = candidate_bundle.get("degrade_capability_mask")
              if isinstance(required_caps, list) and required_caps:
                  if not isinstance(degrade_mask, list):
                      policy_degrade_checks["required_vs_degrade_mask_compatible"] = False
                      blockers.append({"code": "M12-B3", "message": "Candidate required_capabilities declared but degrade_capability_mask missing."})
                  else:
                      req_set = {str(x).strip() for x in required_caps if str(x).strip()}
                      deg_set = {str(x).strip() for x in degrade_mask if str(x).strip()}
                      if not req_set.issubset(deg_set):
                          policy_degrade_checks["required_vs_degrade_mask_compatible"] = False
                          blockers.append({"code": "M12-B3", "message": "Candidate required_capabilities not satisfiable by degrade_capability_mask."})

          schema_checks = {
              "handles_resolved": False,
              "registry_mode_valid": False,
              "compatibility_mode_valid": False,
              "topic_handle_valid": False,
          }
          handles_path = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")
          handles: dict[str, str] = {}
          required_schema_handles = [
              "SCHEMA_REGISTRY_MODE",
              "GLUE_SCHEMA_REGISTRY_NAME",
              "GLUE_SCHEMA_COMPATIBILITY_MODE",
              "FP_BUS_LEARNING_REGISTRY_EVENTS_V1",
          ]
          resolved_schema_handles: dict[str, str] = {}
          unresolved_schema_handles: list[str] = []
          try:
              handles = parse_handles(handles_path)
          except Exception as exc:
              blockers.append({"code": "M12-B3", "message": f"Unable to parse handles registry: {type(exc).__name__}."})
          for h in required_schema_handles:
              v = str(handles.get(h, "")).strip()
              resolved_schema_handles[h] = v
              if (not v) or (v.upper() == "TO_PIN"):
                  unresolved_schema_handles.append(h)
          schema_checks["handles_resolved"] = len(unresolved_schema_handles) == 0
          if unresolved_schema_handles:
              blockers.append({"code": "M12-B3", "message": f"Unresolved schema/event handles: {', '.join(unresolved_schema_handles)}."})
          registry_mode = resolved_schema_handles.get("SCHEMA_REGISTRY_MODE", "")
          schema_checks["registry_mode_valid"] = registry_mode == "AWS_GLUE_SCHEMA_REGISTRY"
          if not schema_checks["registry_mode_valid"]:
              blockers.append({"code": "M12-B3", "message": f"SCHEMA_REGISTRY_MODE invalid for M12.C: '{registry_mode}'."})
          compat_mode = resolved_schema_handles.get("GLUE_SCHEMA_COMPATIBILITY_MODE", "")
          schema_checks["compatibility_mode_valid"] = compat_mode in {"BACKWARD", "FORWARD", "FULL"}
          if not schema_checks["compatibility_mode_valid"]:
              blockers.append({"code": "M12-B3", "message": f"GLUE_SCHEMA_COMPATIBILITY_MODE invalid: '{compat_mode}'."})
          topic_handle = resolved_schema_handles.get("FP_BUS_LEARNING_REGISTRY_EVENTS_V1", "")
          schema_checks["topic_handle_valid"] = topic_handle.startswith("fp.bus.learning.registry.")
          if not schema_checks["topic_handle_valid"]:
              blockers.append({"code": "M12-B3", "message": f"FP_BUS_LEARNING_REGISTRY_EVENTS_V1 invalid: '{topic_handle}'."})

          compatibility_snapshot = {
              "captured_at_utc": now(),
              "phase": "M12.C",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_refs": {
                  "m12b_execution_id": upstream_m12b,
                  "m12b_execution_summary_key": m12b_summary_key,
                  "m12b_snapshot_key": m12b_snapshot_key,
                  "m11i_execution_id": upstream_m11i,
                  "m12_handoff_pack_key": handoff_key,
                  "dataset_fingerprint_ref": dataset_fingerprint_ref,
                  "m11e_snapshot_key": m11e_snapshot_key,
              },
              "feature_input_contract_checks": feature_contract_checks,
              "fingerprint_required_fields_checked": fingerprint_required_fields,
              "fingerprint_missing_fields": fingerprint_missing_fields,
              "policy_degrade_checks": policy_degrade_checks,
              "schema_event_envelope_checks": schema_checks,
              "resolved_schema_handles": resolved_schema_handles,
          }

          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M12.C",
              "phase_id": "P15",
              "execution_id": exec_id,
              "blocker_count": 0,
              "blockers": [],
              "overall_pass": False,
          }

          execution_summary = {
              "captured_at_utc": now(),
              "phase": "M12.C",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": False,
              "blocker_count": 0,
              "verdict": "HOLD_REMEDIATE",
              "next_gate": "HOLD_REMEDIATE",
              "upstream_refs": {
                  "m12b_execution_id": upstream_m12b,
                  "m11i_execution_id": upstream_m11i,
              },
              "artifact_keys": {},
          }

          artifacts = {
              "m12c_compatibility_precheck_snapshot.json": compatibility_snapshot,
              "m12c_blocker_register.json": blocker_register,
              "m12c_execution_summary.json": execution_summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, default_bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B3", "message": f"Failed to publish {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          verdict = "ADVANCE_TO_M12_D" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M12.D_READY" if overall_pass else "HOLD_REMEDIATE"
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          execution_summary["overall_pass"] = overall_pass
          execution_summary["blocker_count"] = len(blockers)
          execution_summary["verdict"] = verdict
          execution_summary["next_gate"] = next_gate
          execution_summary["artifact_keys"] = {
              "m12c_compatibility_precheck_snapshot": run_control_prefix + "m12c_compatibility_precheck_snapshot.json",
              "m12c_blocker_register": run_control_prefix + "m12c_blocker_register.json",
              "m12c_execution_summary": run_control_prefix + "m12c_execution_summary.json",
          }
          execution_summary["read_errors"] = read_errors
          execution_summary["upload_errors"] = upload_errors

          for fname, payload in {
              "m12c_blocker_register.json": blocker_register,
              "m12c_execution_summary.json": execution_summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, default_bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "evidence_prefix": f"s3://{default_bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Generate M12.D strict broker transport proof (MSK ACK + consumer readback)
        if: ${{ inputs.execution_mode == 'm12d_execute' && inputs.m12_subphase == 'D' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          AWS_REGION: ${{ inputs.aws_region }}
          UPSTREAM_M12C_EXEC: ${{ inputs.upstream_m12c_execution }}
          M12D_PROBE_IMAGE_URI: "230372904534.dkr.ecr.eu-west-2.amazonaws.com/fraud-platform-dev-full@sha256:49eb6cb0c5e33061fae4d1aaceeac2e44600adb5c4250436be9ac8395ed29cb2"
        run: |
          set -euo pipefail
          mkdir -p "${RUN_DIR}"
          python -m pip install --quiet aws-msk-iam-sasl-signer-python
          python - <<'PY'
          from __future__ import annotations
          import hashlib
          import json
          import os
          import random
          import re
          import string
          import subprocess
          import tempfile
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from aws_msk_iam_sasl_signer import MSKAuthTokenProvider
          from botocore.exceptions import BotoCoreError, ClientError

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              raw = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(raw)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_parse_uri(uri: str) -> tuple[str, str]:
              if not uri.startswith("s3://"):
                  raise ValueError("not_s3_uri")
              tail = uri[5:]
              if "/" not in tail:
                  raise ValueError("missing_key")
              b, k = tail.split("/", 1)
              if not b or not k:
                  raise ValueError("incomplete_s3_uri")
              return b, k

          def parse_handles(path: Path) -> dict[str, str]:
              rx = re.compile(r"^\*\s+`([A-Z0-9_]+)\s*=\s*(.+)`\s*$")
              out: dict[str, str] = {}
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if (raw.startswith('"') and raw.endswith('"')) or (raw.startswith("'") and raw.endswith("'")):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          def sh(cmd: list[str], *, check: bool = True) -> subprocess.CompletedProcess[str]:
              return subprocess.run(cmd, check=check, text=True, capture_output=True)

          def rand_suffix(n: int = 8) -> str:
              alpha = string.ascii_lowercase + string.digits
              return "".join(random.choice(alpha) for _ in range(n))

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          default_bucket = os.environ["EVIDENCE_BUCKET"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          upstream_m12c = os.environ["UPSTREAM_M12C_EXEC"].strip()

          run_dir.mkdir(parents=True, exist_ok=True)
          proof_path = run_dir / "m12d_broker_transport_proof.json"
          event_path = run_dir / "m12d_transport_event.json"
          context_path = run_dir / "m12d_transport_context.json"
          probe_log_path = run_dir / "m12d_broker_transport_probe.log"

          s3 = boto3.client("s3", region_name=region)
          ssm = boto3.client("ssm", region_name=region)

          proof: dict[str, Any] = {
              "captured_at_utc": now(),
              "phase": "M12.D",
              "phase_id": "P15",
              "execution_id": exec_id,
              "overall_pass": False,
              "reason": "UNINITIALIZED",
              "registry_event_id": "",
              "event_sha256": "",
              "produced": {},
              "consumed": {},
              "runtime": {},
              "errors": [],
          }

          context: dict[str, Any] = {}
          try:
              m12c_summary_key = f"evidence/dev_full/run_control/{upstream_m12c}/m12c_execution_summary.json"
              m12c_snapshot_key = f"evidence/dev_full/run_control/{upstream_m12c}/m12c_compatibility_precheck_snapshot.json"
              m12c_summary = s3_get_json(s3, default_bucket, m12c_summary_key)
              m12c_snapshot = s3_get_json(s3, default_bucket, m12c_snapshot_key)
              platform_run_id = str(m12c_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m12c_summary.get("scenario_run_id", "")).strip()
              upstream_m12b = str((m12c_snapshot.get("upstream_refs") or {}).get("m12b_execution_id", "")).strip()
              upstream_m11i = str((m12c_snapshot.get("upstream_refs") or {}).get("m11i_execution_id", "")).strip()
              if not platform_run_id or not scenario_run_id:
                  raise RuntimeError("M12D_TRANSPORT_CONTEXT_MISSING_RUN_SCOPE")

              m12b_snapshot_key = f"evidence/dev_full/run_control/{upstream_m12b}/m12b_candidate_eligibility_snapshot.json"
              m12b_snapshot = s3_get_json(s3, default_bucket, m12b_snapshot_key)
              m12_handoff_pack_key = f"evidence/dev_full/run_control/{upstream_m11i}/m12_handoff_pack.json"
              m12_handoff_pack = s3_get_json(s3, default_bucket, m12_handoff_pack_key)

              candidate_ref = ""
              required_refs = m12_handoff_pack.get("required_refs") if isinstance(m12_handoff_pack.get("required_refs"), dict) else {}
              if required_refs:
                  candidate_ref = str(required_refs.get("mf_candidate_bundle_ref", "")).strip()
              if not candidate_ref and isinstance(m12b_snapshot.get("readable_ref_report"), dict):
                  candidate_ref = str((m12b_snapshot.get("readable_ref_report") or {}).get("mf_candidate_bundle_ref", {}).get("ref", "")).strip()
              if not candidate_ref:
                  raise RuntimeError("M12D_TRANSPORT_CONTEXT_CANDIDATE_REF_MISSING")

              candidate_bucket, candidate_key = s3_parse_uri(candidate_ref)
              candidate_bundle = s3_get_json(s3, candidate_bucket, candidate_key)
              bundle_id = str(candidate_bundle.get("bundle_id", "")).strip()
              if not bundle_id:
                  raise RuntimeError("M12D_TRANSPORT_CONTEXT_BUNDLE_ID_MISSING")

              handles = parse_handles(Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md"))
              topic = str(handles.get("FP_BUS_LEARNING_REGISTRY_EVENTS_V1", "")).strip()
              msk_cluster_name = str(handles.get("MSK_CLUSTER_NAME", "")).strip()
              msk_cluster_arn = str(handles.get("MSK_CLUSTER_ARN", "")).strip()
              bootstrap = str(handles.get("MSK_BOOTSTRAP_BROKERS_SASL_IAM", "")).strip()
              bootstrap_ssm_path = str(handles.get("SSM_MSK_BOOTSTRAP_BROKERS_PATH", "")).strip()
              eks_cluster_name = str(handles.get("EKS_CLUSTER_NAME", "")).strip()
              eks_namespace = str(handles.get("EKS_NAMESPACE_LEARNING", "")).strip() or "fraud-platform-learning"
              if not topic or not msk_cluster_name or not eks_cluster_name:
                  raise RuntimeError("M12D_TRANSPORT_CONTEXT_REQUIRED_HANDLE_MISSING")
              if msk_cluster_arn.upper() == "TO_PIN":
                  msk_cluster_arn = ""
              if bootstrap.upper() == "TO_PIN":
                  bootstrap = ""
              if bootstrap_ssm_path.upper() == "TO_PIN":
                  bootstrap_ssm_path = ""
              if not bootstrap and bootstrap_ssm_path:
                  bootstrap = str(
                      ssm.get_parameter(Name=bootstrap_ssm_path, WithDecryption=True).get("Parameter", {}).get("Value", "")
                  ).strip()
              if not bootstrap:
                  raise RuntimeError("M12D_TRANSPORT_BOOTSTRAP_UNRESOLVED")
              if ":" not in bootstrap:
                  raise RuntimeError("M12D_TRANSPORT_BOOTSTRAP_INVALID")

              registry_event_seed = f"{platform_run_id}|{scenario_run_id}|{bundle_id}|{exec_id}|BUNDLE_PROMOTED_ACTIVE"
              registry_event_id = hashlib.sha256(registry_event_seed.encode("utf-8")).hexdigest()
              candidate_bundle_s3_uri = f"s3://{candidate_bucket}/{candidate_key}"

              lifecycle_event = {
                  "schema_version": "learning.registry_lifecycle.v0",
                  "registry_event_id": registry_event_id,
                  "event_type": "BUNDLE_PROMOTED_ACTIVE",
                  "scope_key": {
                      "environment": "dev_full",
                      "mode": "managed",
                      "bundle_slot": "active",
                      "tenant_id": f"{platform_run_id}:{scenario_run_id}",
                  },
                  "bundle_ref": {
                      "bundle_id": bundle_id,
                      "bundle_version": str(candidate_bundle.get("execution_id", "")).strip() or "v0",
                      "registry_ref": candidate_bundle_s3_uri,
                  },
                  "actor": {
                      "actor_id": "SYSTEM::m12d_managed_lane",
                      "source_type": "SYSTEM",
                  },
                  "ts_utc": now(),
                  "evidence_refs": [
                      {"ref_type": "m12c_execution_summary", "ref_id": m12c_summary_key},
                      {"ref_type": "m12c_snapshot", "ref_id": m12c_snapshot_key},
                      {"ref_type": "m12b_snapshot", "ref_id": m12b_snapshot_key},
                      {"ref_type": "m12_handoff_pack", "ref_id": m12_handoff_pack_key},
                      {"ref_type": "candidate_bundle", "ref_id": candidate_bundle_s3_uri},
                  ],
              }

              event_sha256 = hashlib.sha256(json.dumps(lifecycle_event, sort_keys=True, ensure_ascii=True).encode("utf-8")).hexdigest()
              event_path.write_text(json.dumps(lifecycle_event, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

              token, expiry_ms = MSKAuthTokenProvider.generate_auth_token(region)
              if not token:
                  raise RuntimeError("M12D_TRANSPORT_TOKEN_GENERATION_FAILED")

              context = {
                  "captured_at_utc": now(),
                  "execution_id": exec_id,
                  "platform_run_id": platform_run_id,
                  "scenario_run_id": scenario_run_id,
                  "topic": topic,
                  "msk_cluster_name": msk_cluster_name,
                  "msk_cluster_arn": msk_cluster_arn,
                  "bootstrap_brokers_sasl_iam": bootstrap,
                  "eks_cluster_name": eks_cluster_name,
                  "eks_namespace": eks_namespace,
                  "registry_event_id": registry_event_id,
                  "event_sha256": event_sha256,
                  "token_expiry_ms": int(expiry_ms),
              }
              context_path.write_text(json.dumps(context, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

              sh(["aws", "eks", "update-kubeconfig", "--region", region, "--name", eks_cluster_name], check=True)
              sh(["kubectl", "get", "namespace", eks_namespace], check=False)
              sh(["kubectl", "create", "namespace", eks_namespace], check=False)

              suffix = rand_suffix()
              cm_name = f"m12d-transport-{suffix}"
              secret_name = f"m12d-transport-token-{suffix}"
              job_name = f"m12d-transport-{suffix}"

              import textwrap
              probe_script = textwrap.dedent(
                  """
                  import hashlib
                  import json
                  import os
                  import time

                  from confluent_kafka import Consumer, Producer, TopicPartition


                  def oauth_cb(_config):
                      return os.environ["MSK_OAUTH_TOKEN"], int(time.time()) + 600


                  def main():
                      event = json.loads(open("/work/event.json", "r", encoding="utf-8").read())
                      topic = os.environ["KAFKA_TOPIC"]
                      bootstrap = os.environ["MSK_BOOTSTRAP_BROKERS"]
                      exec_id = os.environ["M12D_EXEC_ID"]
                      key_text = str(event.get("registry_event_id", "")).strip()
                      payload_bytes = json.dumps(event, sort_keys=True, ensure_ascii=True).encode("utf-8")
                      payload_hash = hashlib.sha256(payload_bytes).hexdigest()

                      producer = Producer(
                          {
                              "bootstrap.servers": bootstrap,
                              "security.protocol": "SASL_SSL",
                              "sasl.mechanism": "OAUTHBEARER",
                              "oauth_cb": oauth_cb,
                              "enable.idempotence": True,
                              "request.timeout.ms": 30000,
                              "message.timeout.ms": 30000,
                              "retries": 3,
                              "acks": "all",
                          }
                      )
                      delivery = {"msg": None, "err": None}

                      def on_delivery(err, msg):
                          delivery["msg"] = msg
                          delivery["err"] = err

                      producer.produce(topic=topic, key=key_text.encode("utf-8"), value=payload_bytes, on_delivery=on_delivery)
                      deadline = time.time() + 45
                      while time.time() < deadline and delivery["msg"] is None and delivery["err"] is None:
                          producer.poll(0.2)
                      producer.flush(5.0)
                      if delivery["err"] is not None:
                          raise RuntimeError(f"produce_failed:{delivery['err']}")
                      if delivery["msg"] is None:
                          raise RuntimeError("produce_timeout_no_delivery")

                      md = delivery["msg"]
                      partition = int(md.partition())
                      offset = int(md.offset())

                      group_id = f"m12d-proof-{exec_id}"
                      consumer = Consumer(
                          {
                              "bootstrap.servers": bootstrap,
                              "security.protocol": "SASL_SSL",
                              "sasl.mechanism": "OAUTHBEARER",
                              "oauth_cb": oauth_cb,
                              "group.id": group_id,
                              "enable.auto.commit": False,
                              "auto.offset.reset": "latest",
                              "session.timeout.ms": 10000,
                          }
                      )
                      tp = TopicPartition(topic, partition, offset)
                      consumer.assign([tp])
                      found = None
                      deadline = time.time() + 60
                      while time.time() < deadline:
                          msg = consumer.poll(1.0)
                          if msg is None:
                              continue
                          if msg.error():
                              continue
                          if int(msg.offset()) != offset:
                              continue
                          rec_payload = msg.value() or b""
                          rec_hash = hashlib.sha256(rec_payload).hexdigest()
                          rec_key = msg.key() or b""
                          found = {
                              "topic": msg.topic(),
                              "partition": int(msg.partition()),
                              "offset": int(msg.offset()),
                              "key": rec_key.decode("utf-8", errors="replace"),
                              "payload_hash": rec_hash,
                              "payload_hash_match": rec_hash == payload_hash,
                          }
                          break
                      consumer.close()

                      out = {
                          "overall_pass": bool(found and found.get("payload_hash_match", False)),
                          "reason": "BROKER_ACK_AND_READBACK_PASS"
                          if (found and found.get("payload_hash_match", False))
                          else "BROKER_READBACK_NOT_CONFIRMED",
                          "registry_event_id": key_text,
                          "event_sha256": payload_hash,
                          "produced": {
                              "topic": topic,
                              "partition": partition,
                              "offset": offset,
                          },
                          "consumed": found or {},
                      }
                      print(json.dumps(out, ensure_ascii=True))


                  if __name__ == "__main__":
                      try:
                          main()
                      except Exception as exc:
                          print(
                              json.dumps(
                                  {
                                      "overall_pass": False,
                                      "reason": f"EXCEPTION:{type(exc).__name__}",
                                      "error": str(exc),
                                      "produced": {},
                                      "consumed": {},
                                  },
                                  ensure_ascii=True,
                              )
                          )
                  """
              ).lstrip()
              with tempfile.TemporaryDirectory() as td:
                  td_path = Path(td)
                  probe_py = td_path / "probe.py"
                  event_json = td_path / "event.json"
                  probe_py.write_text(probe_script, encoding="utf-8")
                  event_json.write_text(event_path.read_text(encoding="utf-8"), encoding="utf-8")

                  cm_yaml = sh(["kubectl", "-n", eks_namespace, "create", "configmap", cm_name, "--from-file", f"probe.py={probe_py}", "--from-file", f"event.json={event_json}", "--dry-run=client", "-o", "yaml"], check=True).stdout
                  subprocess.run(["kubectl", "apply", "-f", "-"], input=cm_yaml, text=True, check=True, capture_output=True)

              secret_yaml = sh(["kubectl", "-n", eks_namespace, "create", "secret", "generic", secret_name, "--from-literal", f"MSK_OAUTH_TOKEN={token}", "--dry-run=client", "-o", "yaml"], check=True).stdout
              subprocess.run(["kubectl", "apply", "-f", "-"], input=secret_yaml, text=True, check=True, capture_output=True)

              probe_image = os.environ.get("M12D_PROBE_IMAGE_URI", "").strip() or "python:3.12-slim"
              job_yaml = (
                  "apiVersion: batch/v1\n"
                  "kind: Job\n"
                  "metadata:\n"
                  f"  name: {job_name}\n"
                  f"  namespace: {eks_namespace}\n"
                  "spec:\n"
                  "  ttlSecondsAfterFinished: 600\n"
                  "  backoffLimit: 0\n"
                  "  template:\n"
                  "    spec:\n"
                  "      restartPolicy: Never\n"
                  "      containers:\n"
                  "        - name: probe\n"
                  f"          image: {probe_image}\n"
                  "          command: [\"/bin/bash\", \"-lc\"]\n"
                  "          args:\n"
                  "            - |\n"
                  "              set -euo pipefail\n"
                  "              python /work/probe.py\n"
                  "          env:\n"
                  "            - name: KAFKA_TOPIC\n"
                  f"              value: \"{topic}\"\n"
                  "            - name: MSK_BOOTSTRAP_BROKERS\n"
                  f"              value: \"{bootstrap}\"\n"
                  "            - name: M12D_EXEC_ID\n"
                  f"              value: \"{exec_id}\"\n"
                  "          envFrom:\n"
                  "            - secretRef:\n"
                  f"                name: {secret_name}\n"
                  "          volumeMounts:\n"
                  "            - name: work\n"
                  "              mountPath: /work\n"
                  "      volumes:\n"
                  "        - name: work\n"
                  "          configMap:\n"
                  f"            name: {cm_name}\n"
              )
              subprocess.run(["kubectl", "apply", "-f", "-"], input=job_yaml, text=True, check=True, capture_output=True)
              wait_proc = subprocess.run(["kubectl", "-n", eks_namespace, "wait", "--for=condition=complete", f"job/{job_name}", "--timeout=240s"], text=True, capture_output=True)
              logs_proc = subprocess.run(["kubectl", "-n", eks_namespace, "logs", f"job/{job_name}"], text=True, capture_output=True)
              probe_log = (logs_proc.stdout or "") + ("\n" + logs_proc.stderr if logs_proc.stderr else "")
              probe_log_path.write_text(probe_log, encoding="utf-8")

              parsed: dict[str, Any] = {}
              for line in reversed(probe_log.splitlines()):
                  raw = line.strip()
                  if not (raw.startswith("{") and raw.endswith("}")):
                      continue
                  try:
                      payload = json.loads(raw)
                  except Exception:
                      continue
                  if isinstance(payload, dict) and "overall_pass" in payload:
                      parsed = payload
                      break

              if not parsed:
                  parsed = {
                      "overall_pass": False,
                      "reason": "PROBE_JSON_NOT_FOUND",
                      "produced": {},
                      "consumed": {},
                  }

              proof.update(
                  {
                      "captured_at_utc": now(),
                      "registry_event_id": str(parsed.get("registry_event_id", context.get("registry_event_id", ""))).strip(),
                      "event_sha256": str(parsed.get("event_sha256", context.get("event_sha256", ""))).strip(),
                      "overall_pass": bool(parsed.get("overall_pass", False)),
                      "reason": str(parsed.get("reason", "UNKNOWN")).strip(),
                      "produced": parsed.get("produced", {}) if isinstance(parsed.get("produced"), dict) else {},
                      "consumed": parsed.get("consumed", {}) if isinstance(parsed.get("consumed"), dict) else {},
                      "runtime": {
                          "eks_cluster_name": eks_cluster_name,
                          "eks_namespace": eks_namespace,
                          "job_name": job_name,
                          "msk_cluster_name": msk_cluster_name,
                          "msk_cluster_arn": msk_cluster_arn,
                          "bootstrap_brokers": bootstrap,
                          "wait_exit_code": int(wait_proc.returncode),
                      },
                  }
              )
              if wait_proc.returncode != 0:
                  proof["overall_pass"] = False
                  proof["errors"].append({"surface": "kubectl.wait", "error": (wait_proc.stderr or wait_proc.stdout or "wait_failed").strip()})
              if proof.get("registry_event_id") != context.get("registry_event_id"):
                  proof["overall_pass"] = False
                  proof["errors"].append({"surface": "registry_event_id", "error": "mismatch_with_context"})
              if proof.get("event_sha256") != context.get("event_sha256"):
                  proof["overall_pass"] = False
                  proof["errors"].append({"surface": "event_sha256", "error": "mismatch_with_context"})

              for cmd in [
                  ["kubectl", "-n", eks_namespace, "delete", "job", job_name, "--ignore-not-found=true"],
                  ["kubectl", "-n", eks_namespace, "delete", "configmap", cm_name, "--ignore-not-found=true"],
                  ["kubectl", "-n", eks_namespace, "delete", "secret", secret_name, "--ignore-not-found=true"],
              ]:
                  subprocess.run(cmd, text=True, capture_output=True)

          except Exception as exc:
              proof["overall_pass"] = False
              proof["reason"] = f"TRANSPORT_PROOF_EXCEPTION:{type(exc).__name__}"
              proof["errors"].append({"surface": "transport_proof", "error": str(exc)})
              if not event_path.exists():
                  event_path.write_text(json.dumps({}, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              if not context_path.exists():
                  context_path.write_text(json.dumps(context, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          proof_path.write_text(json.dumps(proof, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
          print(json.dumps({
              "execution_id": exec_id,
              "transport_proof_path": str(proof_path),
              "transport_event_path": str(event_path),
              "overall_pass": bool(proof.get("overall_pass", False)),
              "reason": str(proof.get("reason", "")),
          }, ensure_ascii=True))
          PY

      - name: Execute M12.D promotion event commit (managed)
        if: ${{ inputs.execution_mode == 'm12d_execute' && inputs.m12_subphase == 'D' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          AWS_REGION: ${{ inputs.aws_region }}
          UPSTREAM_M12C_EXEC: ${{ inputs.upstream_m12c_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import hashlib
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              raw = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(raw)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def s3_parse_uri(uri: str) -> tuple[str, str]:
              if not uri.startswith("s3://"):
                  raise ValueError("not_s3_uri")
              tail = uri[5:]
              if "/" not in tail:
                  raise ValueError("missing_key")
              b, k = tail.split("/", 1)
              if not b or not k:
                  raise ValueError("incomplete_s3_uri")
              return b, k

          def parse_handles(path: Path) -> dict[str, str]:
              rx = re.compile(r"^\*\s+`([A-Z0-9_]+)\s*=\s*(.+)`\s*$")
              out: dict[str, str] = {}
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if (raw.startswith('"') and raw.endswith('"')) or (raw.startswith("'") and raw.endswith("'")):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          default_bucket = os.environ["EVIDENCE_BUCKET"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          upstream_m12c = os.environ["UPSTREAM_M12C_EXEC"].strip()

          s3 = boto3.client("s3", region_name=region)
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []
          notes: list[str] = []
          transport_event: dict[str, Any] = {}
          transport_proof: dict[str, Any] = {}
          transport_event_path = run_dir / "m12d_transport_event.json"
          transport_proof_path = run_dir / "m12d_broker_transport_proof.json"
          if transport_event_path.exists():
              try:
                  maybe_event = json.loads(transport_event_path.read_text(encoding="utf-8"))
                  if isinstance(maybe_event, dict):
                      transport_event = maybe_event
                  else:
                      blockers.append({"code": "M12-B4", "message": "Transport event payload is not a JSON object."})
              except Exception as exc:
                  read_errors.append({"surface": str(transport_event_path), "error": type(exc).__name__})
                  blockers.append({"code": "M12-B4", "message": "Unable to read transport event payload."})
          else:
              blockers.append({"code": "M12-B4", "message": "Strict transport event payload missing from M12.D lane."})
          if transport_proof_path.exists():
              try:
                  maybe_proof = json.loads(transport_proof_path.read_text(encoding="utf-8"))
                  if isinstance(maybe_proof, dict):
                      transport_proof = maybe_proof
                  else:
                      blockers.append({"code": "M12-B4", "message": "Broker transport proof payload is not a JSON object."})
              except Exception as exc:
                  read_errors.append({"surface": str(transport_proof_path), "error": type(exc).__name__})
                  blockers.append({"code": "M12-B4", "message": "Unable to read broker transport proof payload."})
          else:
              blockers.append({"code": "M12-B4", "message": "Strict broker transport proof missing from M12.D lane."})

          m12c_summary_key = f"evidence/dev_full/run_control/{upstream_m12c}/m12c_execution_summary.json"
          m12c_snapshot_key = f"evidence/dev_full/run_control/{upstream_m12c}/m12c_compatibility_precheck_snapshot.json"
          m12c_summary: dict[str, Any] = {}
          m12c_snapshot: dict[str, Any] = {}
          try:
              m12c_summary = s3_get_json(s3, default_bucket, m12c_summary_key)
              m12c_snapshot = s3_get_json(s3, default_bucket, m12c_snapshot_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": "m12c_summary_snapshot", "error": type(exc).__name__})
              blockers.append({"code": "M12-B4", "message": "Unreadable M12.C summary/snapshot."})

          platform_run_id = str(m12c_summary.get("platform_run_id", "")).strip()
          scenario_run_id = str(m12c_summary.get("scenario_run_id", "")).strip()
          if m12c_summary:
              if not bool(m12c_summary.get("overall_pass", False)):
                  blockers.append({"code": "M12-B4", "message": "M12.C overall_pass is false."})
              try:
                  m12c_blocker_count = int(str(m12c_summary.get("blocker_count", "")).strip())
              except Exception:
                  m12c_blocker_count = -1
              if m12c_blocker_count != 0:
                  blockers.append({"code": "M12-B4", "message": "M12.C blocker_count is not zero."})
              if str(m12c_summary.get("next_gate", "")).strip() != "M12.D_READY":
                  blockers.append({"code": "M12-B4", "message": "M12.C next_gate is not M12.D_READY."})
              if not platform_run_id or not scenario_run_id:
                  blockers.append({"code": "M12-B4", "message": "M12.C run scope is incomplete."})

          upstream_m12b = str((m12c_snapshot.get("upstream_refs") or {}).get("m12b_execution_id", "")).strip()
          m12b_snapshot_key = f"evidence/dev_full/run_control/{upstream_m12b}/m12b_candidate_eligibility_snapshot.json" if upstream_m12b else ""
          m12b_snapshot: dict[str, Any] = {}
          if m12b_snapshot_key:
              try:
                  m12b_snapshot = s3_get_json(s3, default_bucket, m12b_snapshot_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": m12b_snapshot_key, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B4", "message": "Unreadable M12.B candidate eligibility snapshot."})
          else:
              blockers.append({"code": "M12-B4", "message": "M12.C snapshot missing upstream M12.B execution id."})

          upstream_m11i = str((m12c_snapshot.get("upstream_refs") or {}).get("m11i_execution_id", "")).strip()
          m12_handoff_pack_key = f"evidence/dev_full/run_control/{upstream_m11i}/m12_handoff_pack.json" if upstream_m11i else ""
          m12_handoff_pack: dict[str, Any] = {}
          if m12_handoff_pack_key:
              try:
                  m12_handoff_pack = s3_get_json(s3, default_bucket, m12_handoff_pack_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": m12_handoff_pack_key, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B4", "message": "Unreadable M12 handoff pack from M11.I."})
          else:
              blockers.append({"code": "M12-B4", "message": "M12.C snapshot missing upstream M11.I execution id."})

          candidate_ref = ""
          required_refs = m12_handoff_pack.get("required_refs") if isinstance(m12_handoff_pack.get("required_refs"), dict) else {}
          if required_refs:
              candidate_ref = str(required_refs.get("mf_candidate_bundle_ref", "")).strip()
          if not candidate_ref and isinstance(m12b_snapshot.get("readable_ref_report"), dict):
              candidate_ref = str((m12b_snapshot.get("readable_ref_report") or {}).get("mf_candidate_bundle_ref", {}).get("ref", "")).strip()

          candidate_bundle: dict[str, Any] = {}
          candidate_bucket = default_bucket
          candidate_key = ""
          if not candidate_ref:
              blockers.append({"code": "M12-B4", "message": "Unable to resolve candidate bundle ref from upstream evidence."})
          else:
              try:
                  candidate_bucket, candidate_key = s3_parse_uri(candidate_ref)
                  candidate_bundle = s3_get_json(s3, candidate_bucket, candidate_key)
              except Exception as exc:
                  read_errors.append({"surface": candidate_ref, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B4", "message": "Candidate bundle is unreadable."})

          bundle_id = str(candidate_bundle.get("bundle_id", "")).strip()
          candidate_status = str(candidate_bundle.get("bundle_status", "")).strip()
          candidate_run_scope_ok = (
              str(candidate_bundle.get("platform_run_id", "")).strip() == platform_run_id
              and str(candidate_bundle.get("scenario_run_id", "")).strip() == scenario_run_id
          )
          if candidate_bundle:
              if candidate_status != "CANDIDATE":
                  blockers.append({"code": "M12-B4", "message": "Candidate bundle status is not CANDIDATE at M12.D entry."})
              if not bundle_id:
                  blockers.append({"code": "M12-B4", "message": "Candidate bundle missing bundle_id."})
              if not candidate_run_scope_ok:
                  blockers.append({"code": "M12-B4", "message": "Candidate bundle run scope mismatches M12.C scope."})

          handles_path = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")
          handles: dict[str, str] = {}
          try:
              handles = parse_handles(handles_path)
          except Exception as exc:
              blockers.append({"code": "M12-B4", "message": f"Unable to parse handles registry: {type(exc).__name__}."})

          required_handles = [
              "MPR_PROMOTION_RECEIPT_PATH_PATTERN",
              "FP_BUS_LEARNING_REGISTRY_EVENTS_V1",
              "SCHEMA_REGISTRY_MODE",
              "GLUE_SCHEMA_REGISTRY_NAME",
              "GLUE_SCHEMA_COMPATIBILITY_MODE",
          ]
          resolved_handles: dict[str, str] = {}
          unresolved_handles: list[str] = []
          for key in required_handles:
              value = str(handles.get(key, "")).strip()
              resolved_handles[key] = value
              if (not value) or (value.upper() == "TO_PIN"):
                  unresolved_handles.append(key)
          if unresolved_handles:
              blockers.append({"code": "M12-B4", "message": f"Unresolved required handles: {', '.join(unresolved_handles)}."})

          mpr_pattern = resolved_handles.get("MPR_PROMOTION_RECEIPT_PATH_PATTERN", "")
          topic_handle = resolved_handles.get("FP_BUS_LEARNING_REGISTRY_EVENTS_V1", "")
          promotion_key = ""
          if mpr_pattern and platform_run_id:
              promotion_key = mpr_pattern.replace("{platform_run_id}", platform_run_id)
          if (not promotion_key) or ("{" in promotion_key) or ("}" in promotion_key):
              blockers.append({"code": "M12-B4", "message": "MPR_PROMOTION_RECEIPT_PATH_PATTERN did not fully resolve."})
          if not topic_handle.startswith("fp.bus.learning.registry."):
              blockers.append({"code": "M12-B4", "message": f"FP_BUS_LEARNING_REGISTRY_EVENTS_V1 invalid: '{topic_handle}'."})

          registry_event_seed = f"{platform_run_id}|{scenario_run_id}|{bundle_id}|{exec_id}|BUNDLE_PROMOTED_ACTIVE"
          registry_event_id = hashlib.sha256(registry_event_seed.encode("utf-8")).hexdigest()
          candidate_bundle_s3_uri = f"s3://{candidate_bucket}/{candidate_key}" if candidate_key else ""

          if transport_event:
              lifecycle_event = dict(transport_event)
              registry_event_id = str(lifecycle_event.get("registry_event_id", "")).strip() or registry_event_id
          else:
              lifecycle_event = {
                  "schema_version": "learning.registry_lifecycle.v0",
                  "registry_event_id": registry_event_id,
                  "event_type": "BUNDLE_PROMOTED_ACTIVE",
                  "scope_key": {
                      "environment": "dev_full",
                      "mode": "managed",
                      "bundle_slot": "active",
                      "tenant_id": f"{platform_run_id}:{scenario_run_id}",
                  },
                  "bundle_ref": {
                      "bundle_id": bundle_id,
                      "bundle_version": str(candidate_bundle.get("execution_id", "")).strip() or "v0",
                      "registry_ref": candidate_bundle_s3_uri or f"s3://{default_bucket}/{promotion_key}",
                  },
                  "actor": {
                      "actor_id": "SYSTEM::m12d_managed_lane",
                      "source_type": "SYSTEM",
                  },
                  "ts_utc": now(),
                  "evidence_refs": [
                      {"ref_type": "m12c_execution_summary", "ref_id": m12c_summary_key},
                      {"ref_type": "m12c_snapshot", "ref_id": m12c_snapshot_key},
                      {"ref_type": "m12b_snapshot", "ref_id": m12b_snapshot_key},
                      {"ref_type": "m12_handoff_pack", "ref_id": m12_handoff_pack_key},
                      {"ref_type": "candidate_bundle", "ref_id": candidate_bundle_s3_uri},
                  ],
              }
          lifecycle_event_sha256 = hashlib.sha256(json.dumps(lifecycle_event, sort_keys=True, ensure_ascii=True).encode("utf-8")).hexdigest()

          lifecycle_required = [
              "schema_version",
              "registry_event_id",
              "event_type",
              "scope_key",
              "bundle_ref",
              "actor",
              "ts_utc",
          ]
          missing_lifecycle_fields = [k for k in lifecycle_required if not lifecycle_event.get(k)]
          lifecycle_scope = lifecycle_event.get("scope_key") if isinstance(lifecycle_event.get("scope_key"), dict) else {}
          lifecycle_bundle_ref = lifecycle_event.get("bundle_ref") if isinstance(lifecycle_event.get("bundle_ref"), dict) else {}
          lifecycle_actor = lifecycle_event.get("actor") if isinstance(lifecycle_event.get("actor"), dict) else {}
          if missing_lifecycle_fields:
              blockers.append({"code": "M12-B4", "message": f"Lifecycle payload missing required fields: {', '.join(missing_lifecycle_fields)}."})
          if str(lifecycle_event.get("schema_version", "")).strip() != "learning.registry_lifecycle.v0":
              blockers.append({"code": "M12-B4", "message": "Lifecycle payload schema_version mismatch."})
          if str(lifecycle_event.get("event_type", "")).strip() != "BUNDLE_PROMOTED_ACTIVE":
              blockers.append({"code": "M12-B4", "message": "Lifecycle payload event_type mismatch for promotion."})
          if not all(bool(str(lifecycle_scope.get(k, "")).strip()) for k in ["environment", "mode", "bundle_slot", "tenant_id"]):
              blockers.append({"code": "M12-B4", "message": "Lifecycle payload scope_key is incomplete."})
          if not all(bool(str(lifecycle_bundle_ref.get(k, "")).strip()) for k in ["bundle_id", "bundle_version", "registry_ref"]):
              blockers.append({"code": "M12-B4", "message": "Lifecycle payload bundle_ref is incomplete."})
          if not all(bool(str(lifecycle_actor.get(k, "")).strip()) for k in ["actor_id", "source_type"]):
              blockers.append({"code": "M12-B4", "message": "Lifecycle payload actor is incomplete."})

          transport_overall_pass = bool(transport_proof.get("overall_pass", False))
          transport_reason = str(transport_proof.get("reason", "")).strip()
          transport_registry_event_id = str(transport_proof.get("registry_event_id", "")).strip()
          transport_event_sha256 = str(transport_proof.get("event_sha256", "")).strip()
          transport_produced = transport_proof.get("produced") if isinstance(transport_proof.get("produced"), dict) else {}
          transport_consumed = transport_proof.get("consumed") if isinstance(transport_proof.get("consumed"), dict) else {}
          if not transport_overall_pass:
              blockers.append({"code": "M12-B4", "message": f"Broker transport proof failed: {transport_reason or 'unknown_reason'}."})
          if transport_registry_event_id != registry_event_id:
              blockers.append({"code": "M12-B4", "message": "Broker transport proof registry_event_id mismatch."})
          if transport_event_sha256 != lifecycle_event_sha256:
              blockers.append({"code": "M12-B4", "message": "Broker transport proof payload hash mismatch."})
          produced_topic = str(transport_produced.get("topic", "")).strip()
          produced_partition_raw = transport_produced.get("partition")
          produced_offset_raw = transport_produced.get("offset")
          try:
              produced_partition = int(produced_partition_raw)
          except Exception:
              produced_partition = -1
          try:
              produced_offset = int(produced_offset_raw)
          except Exception:
              produced_offset = -1
          if produced_topic != topic_handle:
              blockers.append({"code": "M12-B4", "message": "Broker transport proof topic mismatch."})
          if produced_partition < 0 or produced_offset < 0:
              blockers.append({"code": "M12-B4", "message": "Broker transport proof missing valid partition/offset."})
          consumed_match = bool(transport_consumed.get("payload_hash_match", False))
          consumed_offset_raw = transport_consumed.get("offset")
          try:
              consumed_offset = int(consumed_offset_raw)
          except Exception:
              consumed_offset = -1
          if not consumed_match:
              blockers.append({"code": "M12-B4", "message": "Consumer readback payload hash does not match produced event."})
          if consumed_offset != produced_offset:
              blockers.append({"code": "M12-B4", "message": "Consumer readback offset mismatch against produced ACK offset."})

          promotion_receipt = {
              "captured_at_utc": now(),
              "phase": "M12.D",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "status": "PROMOTION_COMMITTED",
              "promotion": {
                  "candidate_bundle_ref": candidate_bundle_s3_uri,
                  "bundle_id": bundle_id,
                  "bundle_status_before": candidate_status,
                  "bundle_status_after": "PROMOTED_ACTIVE",
              },
              "registry_event": lifecycle_event,
              "publication": {
                  "topic": topic_handle,
                  "publication_mode": "MSK_BROKER_ACK_AND_CONSUMER_READBACK",
                  "event_sha256": lifecycle_event_sha256,
                  "status": "BROKER_ACK_CONFIRMED",
                  "broker_ack": {
                      "topic": produced_topic,
                      "partition": produced_partition,
                      "offset": produced_offset,
                  },
                  "consumer_readback": {
                      "payload_hash_match": consumed_match,
                      "offset": consumed_offset,
                  },
                  "note": "Strict transport gate: broker ACK + consumer readback required for PASS.",
              },
              "provenance": {
                  "upstream_m12c_execution_id": upstream_m12c,
                  "upstream_m12b_execution_id": upstream_m12b,
                  "upstream_m11i_execution_id": upstream_m11i,
              },
          }

          publication_receipt = {
              "captured_at_utc": now(),
              "phase": "M12.D",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "topic": topic_handle,
              "registry_event_id": registry_event_id,
              "publication_mode": "MSK_BROKER_ACK_AND_CONSUMER_READBACK",
              "publication_status": "BROKER_ACK_CONFIRMED",
              "broker_ack": {
                  "topic": produced_topic,
                  "partition": produced_partition,
                  "offset": produced_offset,
              },
              "transport_readback": {
                  "source": "kafka_consumer_group_readback",
                  "receipt_key": promotion_key,
                  "consumer_group": f"m12d-proof-{exec_id}",
                  "read_offset": consumed_offset,
                  "payload_hash_match": consumed_match,
                  "broker_offset_claimed": True,
              },
          }

          if not blockers and promotion_key:
              try:
                  s3_put_json(s3, default_bucket, promotion_key, promotion_receipt)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": promotion_key, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B4", "message": "Failed to write run-scoped promotion receipt."})

          promotion_receipt_readback_ok = False
          promotion_receipt_readback_error = ""
          if not blockers and promotion_key:
              try:
                  readback_payload = s3_get_json(s3, default_bucket, promotion_key)
                  promotion_receipt_readback_ok = (
                      str(readback_payload.get("phase", "")).strip() == "M12.D"
                      and str(readback_payload.get("phase_id", "")).strip() == "P15"
                      and str(readback_payload.get("platform_run_id", "")).strip() == platform_run_id
                      and str(readback_payload.get("scenario_run_id", "")).strip() == scenario_run_id
                      and str((((readback_payload.get("registry_event") or {}).get("registry_event_id")) or "")).strip() == registry_event_id
                  )
                  if not promotion_receipt_readback_ok:
                      blockers.append({"code": "M12-B4", "message": "Promotion receipt readback content mismatch."})
              except Exception as exc:
                  promotion_receipt_readback_error = type(exc).__name__
                  read_errors.append({"surface": promotion_key, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B4", "message": "Promotion receipt readback failed."})

          promotion_snapshot = {
              "captured_at_utc": now(),
              "phase": "M12.D",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_refs": {
                  "m12c_execution_id": upstream_m12c,
                  "m12c_execution_summary_key": m12c_summary_key,
                  "m12c_snapshot_key": m12c_snapshot_key,
                  "m12b_execution_id": upstream_m12b,
                  "m12b_snapshot_key": m12b_snapshot_key,
                  "m11i_execution_id": upstream_m11i,
                  "m12_handoff_pack_key": m12_handoff_pack_key,
              },
              "required_handles": required_handles,
              "resolved_handles": resolved_handles,
              "unresolved_handles": unresolved_handles,
              "promotion_receipt_key": promotion_key,
              "promotion_receipt_s3_uri": f"s3://{default_bucket}/{promotion_key}" if promotion_key else "",
              "promotion_receipt_readback_ok": promotion_receipt_readback_ok,
              "promotion_receipt_readback_error": promotion_receipt_readback_error,
              "candidate_bundle_ref": candidate_bundle_s3_uri,
              "candidate_checks": {
                  "bundle_status_candidate": candidate_status == "CANDIDATE",
                  "bundle_id_present": bool(bundle_id),
                  "run_scope_match": candidate_run_scope_ok,
              },
              "lifecycle_event_checks": {
                  "schema_version_ok": str(lifecycle_event.get("schema_version", "")).strip() == "learning.registry_lifecycle.v0",
                  "event_type_ok": str(lifecycle_event.get("event_type", "")).strip() == "BUNDLE_PROMOTED_ACTIVE",
                  "scope_complete": all(bool(str(lifecycle_scope.get(k, "")).strip()) for k in ["environment", "mode", "bundle_slot", "tenant_id"]),
                  "bundle_ref_complete": all(bool(str(lifecycle_bundle_ref.get(k, "")).strip()) for k in ["bundle_id", "bundle_version", "registry_ref"]),
                  "actor_complete": all(bool(str(lifecycle_actor.get(k, "")).strip()) for k in ["actor_id", "source_type"]),
              },
              "publication_checks": {
                  "topic_handle": topic_handle,
                  "topic_handle_valid": topic_handle.startswith("fp.bus.learning.registry."),
                  "publication_mode": publication_receipt["publication_mode"],
                  "publication_status": publication_receipt["publication_status"],
                  "broker_offset_claimed": True,
                  "produced_partition": produced_partition,
                  "produced_offset": produced_offset,
                  "consumed_offset": consumed_offset,
                  "payload_hash_match": consumed_match,
              },
              "notes": notes,
          }

          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M12.D",
              "phase_id": "P15",
              "execution_id": exec_id,
              "blocker_count": 0,
              "blockers": [],
              "overall_pass": False,
          }

          execution_summary = {
              "captured_at_utc": now(),
              "phase": "M12.D",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": False,
              "blocker_count": 0,
              "verdict": "HOLD_REMEDIATE",
              "next_gate": "HOLD_REMEDIATE",
              "upstream_refs": {
                  "m12c_execution_id": upstream_m12c,
                  "m12b_execution_id": upstream_m12b,
                  "m11i_execution_id": upstream_m11i,
              },
              "artifact_keys": {},
          }

          artifacts = {
              "m12d_registry_lifecycle_event.json": lifecycle_event,
              "m12d_broker_transport_proof.json": transport_proof,
              "m12d_learning_registry_publication_receipt.json": publication_receipt,
              "m12d_promotion_commit_snapshot.json": promotion_snapshot,
              "m12d_blocker_register.json": blocker_register,
              "m12d_execution_summary.json": execution_summary,
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, default_bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B4", "message": f"Failed to publish {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          verdict = "ADVANCE_TO_M12_E" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M12.E_READY" if overall_pass else "HOLD_REMEDIATE"
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          execution_summary["overall_pass"] = overall_pass
          execution_summary["blocker_count"] = len(blockers)
          execution_summary["verdict"] = verdict
          execution_summary["next_gate"] = next_gate
          execution_summary["artifact_keys"] = {
              "m12d_registry_lifecycle_event": run_control_prefix + "m12d_registry_lifecycle_event.json",
              "m12d_broker_transport_proof": run_control_prefix + "m12d_broker_transport_proof.json",
              "m12d_learning_registry_publication_receipt": run_control_prefix + "m12d_learning_registry_publication_receipt.json",
              "m12d_promotion_commit_snapshot": run_control_prefix + "m12d_promotion_commit_snapshot.json",
              "m12d_blocker_register": run_control_prefix + "m12d_blocker_register.json",
              "m12d_execution_summary": run_control_prefix + "m12d_execution_summary.json",
              "mpr_promotion_receipt_key": promotion_key,
          }
          execution_summary["read_errors"] = read_errors
          execution_summary["upload_errors"] = upload_errors

          for fname, payload in {
              "m12d_promotion_commit_snapshot.json": promotion_snapshot,
              "m12d_blocker_register.json": blocker_register,
              "m12d_execution_summary.json": execution_summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, default_bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "upstream_m12c_execution": upstream_m12c,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "promotion_receipt_s3_uri": f"s3://{default_bucket}/{promotion_key}" if promotion_key else "",
                      "evidence_prefix": f"s3://{default_bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Execute M12.E rollback drill execution (managed)
        if: ${{ inputs.execution_mode == 'm12e_execute' && inputs.m12_subphase == 'E' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          AWS_REGION: ${{ inputs.aws_region }}
          UPSTREAM_M12D_EXEC: ${{ inputs.upstream_m12d_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import hashlib
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, str]:
              rx = re.compile(r"^\*\s+`([A-Z0-9_]+)\s*=\s*(.+)`\s*$")
              out: dict[str, str] = {}
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if (raw.startswith('"') and raw.endswith('"')) or (raw.startswith("'") and raw.endswith("'")):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          def is_placeholder(value: str) -> bool:
              v = (value or "").strip()
              lo = v.lower()
              if not v:
                  return True
              if lo in {"to_pin", "todo", "tbd", "none", "null", "unset"}:
                  return True
              if "<" in v and ">" in v:
                  return True
              return False

          def s3_get_bytes(s3: Any, bucket: str, key: str) -> bytes:
              return s3.get_object(Bucket=bucket, Key=key)["Body"].read()

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              payload = json.loads(s3_get_bytes(s3, bucket, key).decode("utf-8"))
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def s3_parse_uri(uri: str) -> tuple[str, str]:
              if not uri.startswith("s3://"):
                  raise ValueError("not_s3_uri")
              tail = uri[5:]
              if "/" not in tail:
                  raise ValueError("missing_key")
              b, k = tail.split("/", 1)
              if not b or not k:
                  raise ValueError("incomplete_s3_uri")
              return b, k

          def sha256_bytes(data: bytes) -> str:
              return hashlib.sha256(data).hexdigest()

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          bucket = os.environ["EVIDENCE_BUCKET"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          upstream_m12d = os.environ["UPSTREAM_M12D_EXEC"].strip()
          if not upstream_m12d:
              raise SystemExit("UPSTREAM_M12D_EXEC is required for M12.E.")

          s3 = boto3.client("s3", region_name=region)

          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []
          notes: list[str] = []
          handles = parse_handles(Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md"))

          required_handles = [
              "MPR_PROMOTION_RECEIPT_PATH_PATTERN",
              "MPR_ROLLBACK_DRILL_PATH_PATTERN",
              "MF_CANDIDATE_BUNDLE_PATH_PATTERN",
          ]
          missing = [k for k in required_handles if k not in handles]
          placeholders = [k for k in required_handles if k in handles and is_placeholder(str(handles.get(k, "")))]
          if missing or placeholders:
              blockers.append({"code": "M12-B5", "message": "Required M12.E handles unresolved: " + ",".join(sorted(missing + placeholders))})

          m12d_summary_key = f"evidence/dev_full/run_control/{upstream_m12d}/m12d_execution_summary.json"
          m12d_snapshot_key = f"evidence/dev_full/run_control/{upstream_m12d}/m12d_promotion_commit_snapshot.json"
          m12d_receipt_key = f"evidence/dev_full/run_control/{upstream_m12d}/m12d_learning_registry_publication_receipt.json"
          m12d_transport_key = f"evidence/dev_full/run_control/{upstream_m12d}/m12d_broker_transport_proof.json"
          m12d_summary: dict[str, Any] | None = None
          m12d_snapshot: dict[str, Any] | None = None
          m12d_receipt: dict[str, Any] | None = None
          m12d_transport: dict[str, Any] | None = None
          try:
              m12d_summary = s3_get_json(s3, bucket, m12d_summary_key)
              m12d_snapshot = s3_get_json(s3, bucket, m12d_snapshot_key)
              m12d_receipt = s3_get_json(s3, bucket, m12d_receipt_key)
              m12d_transport = s3_get_json(s3, bucket, m12d_transport_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": "m12d_upstream_bundle", "error": type(exc).__name__})
              blockers.append({"code": "M12-B5", "message": "M12.D upstream evidence unreadable."})

          platform_run_id = ""
          scenario_run_id = ""
          promotion_receipt_key = ""
          candidate_bundle_ref = ""
          candidate_bundle_key = ""
          candidate_bundle_sha256_a = ""
          candidate_bundle_sha256_b = ""
          candidate_bundle: dict[str, Any] | None = None
          mpr_rollback_key = ""
          ofs_rollback_recipe_key = ""
          ofs_rollback_drill_key = ""
          ofs_rollback_recipe_readable = False
          ofs_rollback_drill_pass = False
          transport_strict_pass = False
          prior_mpr_report_sha256 = ""
          prior_mpr_report_phase = ""
          prior_mpr_report_execution_id = ""
          rollback_report_readback_ok = False

          if m12d_summary and m12d_snapshot and m12d_receipt and m12d_transport:
              if not bool(m12d_summary.get("overall_pass")):
                  blockers.append({"code": "M12-B5", "message": "M12.D summary is not pass posture."})
              if str(m12d_summary.get("next_gate", "")).strip() != "M12.E_READY":
                  blockers.append({"code": "M12-B5", "message": "M12.D next_gate is not M12.E_READY."})
              if int(m12d_summary.get("blocker_count", 0)) != 0:
                  blockers.append({"code": "M12-B5", "message": "M12.D blocker_count is not zero."})

              platform_run_id = str(m12d_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m12d_summary.get("scenario_run_id", "")).strip()
              if not platform_run_id or not scenario_run_id:
                  blockers.append({"code": "M12-B5", "message": "M12.D run scope is incomplete."})

              artifacts = m12d_summary.get("artifact_keys") if isinstance(m12d_summary.get("artifact_keys"), dict) else {}
              promotion_receipt_key = str(artifacts.get("mpr_promotion_receipt_key", "")).strip()
              if not promotion_receipt_key and platform_run_id:
                  pattern = str(handles.get("MPR_PROMOTION_RECEIPT_PATH_PATTERN", "")).strip()
                  if pattern:
                      promotion_receipt_key = pattern.replace("{platform_run_id}", platform_run_id)
              if not promotion_receipt_key:
                  blockers.append({"code": "M12-B5", "message": "M12.D promotion receipt key missing."})
              else:
                  try:
                      s3_get_json(s3, bucket, promotion_receipt_key)
                  except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                      read_errors.append({"surface": promotion_receipt_key, "error": type(exc).__name__})
                      blockers.append({"code": "M12-B5", "message": "Promotion receipt unreadable at run-scoped MPR path."})

              candidate_bundle_ref = str(m12d_snapshot.get("candidate_bundle_ref", "")).strip()
              if not candidate_bundle_ref and platform_run_id:
                  mf_pattern = str(handles.get("MF_CANDIDATE_BUNDLE_PATH_PATTERN", "")).strip()
                  if mf_pattern:
                      candidate_bundle_key = mf_pattern.replace("{platform_run_id}", platform_run_id)
                      candidate_bundle_ref = f"s3://{bucket}/{candidate_bundle_key}"
              if candidate_bundle_ref:
                  try:
                      candidate_bucket, candidate_bundle_key = s3_parse_uri(candidate_bundle_ref)
                      raw_a = s3_get_bytes(s3, candidate_bucket, candidate_bundle_key)
                      raw_b = s3_get_bytes(s3, candidate_bucket, candidate_bundle_key)
                      candidate_bundle_sha256_a = sha256_bytes(raw_a)
                      candidate_bundle_sha256_b = sha256_bytes(raw_b)
                      candidate_bundle = json.loads(raw_a.decode("utf-8"))
                      if not isinstance(candidate_bundle, dict):
                          raise ValueError("candidate_bundle_not_object")
                  except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                      read_errors.append({"surface": candidate_bundle_ref, "error": type(exc).__name__})
                      blockers.append({"code": "M12-B5", "message": "Candidate bundle artifact unreadable."})
              else:
                  blockers.append({"code": "M12-B5", "message": "Candidate bundle ref unresolved for rollback drill."})

              transport_strict_pass = bool(m12d_transport.get("overall_pass")) and str(m12d_transport.get("reason", "")).strip() == "BROKER_ACK_AND_READBACK_PASS"
              if not transport_strict_pass:
                  blockers.append({"code": "M12-B5", "message": "M12.D strict transport proof is not pass posture."})

          run_scope_match = False
          candidate_hash_stable = bool(candidate_bundle_sha256_a and candidate_bundle_sha256_b and candidate_bundle_sha256_a == candidate_bundle_sha256_b)
          candidate_status_ok = False
          if candidate_bundle:
              run_scope_match = (
                  str(candidate_bundle.get("platform_run_id", "")).strip() == platform_run_id
                  and str(candidate_bundle.get("scenario_run_id", "")).strip() == scenario_run_id
              )
              candidate_status_ok = str(candidate_bundle.get("bundle_status", "")).strip() in {"CANDIDATE", "PROMOTED"}
              if not run_scope_match:
                  blockers.append({"code": "M12-B5", "message": "Candidate bundle run-scope mismatch."})
              if not candidate_status_ok:
                  blockers.append({"code": "M12-B5", "message": "Candidate bundle status is not rollback-eligible."})
          if not candidate_hash_stable:
              blockers.append({"code": "M12-B5", "message": "Candidate bundle hash stability check failed."})

          if platform_run_id:
              ofs_rollback_recipe_key = f"evidence/runs/{platform_run_id}/learning/ofs/rollback_recipe.json"
              ofs_rollback_drill_key = f"evidence/runs/{platform_run_id}/learning/ofs/rollback_drill_report.json"
              try:
                  s3_get_json(s3, bucket, ofs_rollback_recipe_key)
                  ofs_rollback_recipe_readable = True
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": ofs_rollback_recipe_key, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B5", "message": "OFS rollback recipe unreadable."})
              try:
                  ofs_drill = s3_get_json(s3, bucket, ofs_rollback_drill_key)
                  ofs_rollback_drill_pass = bool(ofs_drill.get("drill_pass") or ofs_drill.get("overall_pass"))
                  if not ofs_rollback_drill_pass:
                      blockers.append({"code": "M12-B5", "message": "OFS rollback drill report is not pass posture."})
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": ofs_rollback_drill_key, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B5", "message": "OFS rollback drill report unreadable."})

          mpr_pattern = str(handles.get("MPR_ROLLBACK_DRILL_PATH_PATTERN", "")).strip()
          if mpr_pattern and platform_run_id:
              mpr_rollback_key = mpr_pattern.replace("{platform_run_id}", platform_run_id)
          if not mpr_rollback_key or "{" in mpr_rollback_key or "}" in mpr_rollback_key:
              blockers.append({"code": "M12-B5", "message": "MPR_ROLLBACK_DRILL_PATH_PATTERN did not fully resolve."})

          if mpr_rollback_key:
              try:
                  prior_bytes = s3_get_bytes(s3, bucket, mpr_rollback_key)
                  prior_mpr_report_sha256 = sha256_bytes(prior_bytes)
                  prior_payload = json.loads(prior_bytes.decode("utf-8"))
                  if isinstance(prior_payload, dict):
                      prior_mpr_report_phase = str(prior_payload.get("phase", "")).strip()
                      prior_mpr_report_execution_id = str(prior_payload.get("execution_id", "")).strip()
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError):
                  pass

          bounded_restore_checks = [
              {"name": "upstream_m12d_pass", "pass": bool(m12d_summary and bool(m12d_summary.get("overall_pass"))), "detail": f"upstream={upstream_m12d}"},
              {"name": "transport_strict_pass", "pass": transport_strict_pass, "detail": f"proof={m12d_transport_key}"},
              {"name": "promotion_receipt_readable", "pass": bool(promotion_receipt_key), "detail": promotion_receipt_key},
              {"name": "candidate_bundle_readable", "pass": candidate_bundle is not None, "detail": candidate_bundle_ref},
              {"name": "candidate_bundle_hash_stable", "pass": candidate_hash_stable, "detail": f"sha256_a={candidate_bundle_sha256_a},sha256_b={candidate_bundle_sha256_b}"},
              {"name": "run_scope_match", "pass": run_scope_match, "detail": f"platform_run_id={platform_run_id},scenario_run_id={scenario_run_id}"},
              {"name": "ofs_rollback_recipe_readable", "pass": ofs_rollback_recipe_readable, "detail": ofs_rollback_recipe_key},
              {"name": "ofs_rollback_drill_pass", "pass": ofs_rollback_drill_pass, "detail": ofs_rollback_drill_key},
          ]
          bounded_restore_pass = all(bool(c.get("pass")) for c in bounded_restore_checks)
          if not bounded_restore_pass:
              blockers.append({"code": "M12-B5", "message": "Bounded restore objective checks failed."})

          mpr_rollback_payload = {
              "captured_at_utc": now(),
              "phase": "M12.E",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "status": "ROLLBACK_DRILL_PASS" if bounded_restore_pass else "ROLLBACK_DRILL_FAIL",
              "bounded_restore_objective": {
                  "objective_name": "PROMOTION_ROLLBACK_BOUNDED_RESTORE",
                  "target_restore_window_minutes": 15,
                  "checks": bounded_restore_checks,
                  "overall_pass": bounded_restore_pass,
              },
              "source_refs": {
                  "m12d_execution_id": upstream_m12d,
                  "m12d_summary_key": m12d_summary_key,
                  "m12d_snapshot_key": m12d_snapshot_key,
                  "m12d_publication_receipt_key": m12d_receipt_key,
                  "m12d_transport_proof_key": m12d_transport_key,
                  "promotion_receipt_key": promotion_receipt_key,
                  "candidate_bundle_ref": candidate_bundle_ref,
                  "ofs_rollback_recipe_key": ofs_rollback_recipe_key,
                  "ofs_rollback_drill_key": ofs_rollback_drill_key,
                  "prior_mpr_rollback_report_sha256": prior_mpr_report_sha256,
                  "prior_mpr_rollback_report_phase": prior_mpr_report_phase,
                  "prior_mpr_rollback_report_execution_id": prior_mpr_report_execution_id,
              },
          }

          if not blockers and mpr_rollback_key:
              try:
                  s3_put_json(s3, bucket, mpr_rollback_key, mpr_rollback_payload)
                  rb = s3_get_bytes(s3, bucket, mpr_rollback_key)
                  rollback_report_readback_ok = sha256_bytes(rb) == sha256_bytes((json.dumps(mpr_rollback_payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8"))
                  if not rollback_report_readback_ok:
                      blockers.append({"code": "M12-B5", "message": "Rollback drill report readback mismatch after write."})
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": mpr_rollback_key, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B5", "message": "Failed to publish run-scoped MPR rollback drill report."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M12.F_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M12_F" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": now(),
              "phase": "M12.E",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_refs": {
                  "m12d_execution_id": upstream_m12d,
                  "m12d_summary_key": m12d_summary_key,
                  "m12d_snapshot_key": m12d_snapshot_key,
                  "m12d_publication_receipt_key": m12d_receipt_key,
                  "m12d_transport_proof_key": m12d_transport_key,
              },
              "required_handles": required_handles,
              "resolved_handles": {k: str(handles.get(k, "")) for k in required_handles},
              "target_refs": {
                  "promotion_receipt_key": promotion_receipt_key,
                  "mpr_rollback_drill_key": mpr_rollback_key,
                  "candidate_bundle_ref": candidate_bundle_ref,
                  "ofs_rollback_recipe_key": ofs_rollback_recipe_key,
                  "ofs_rollback_drill_key": ofs_rollback_drill_key,
              },
              "bounded_restore_objective": {
                  "checks": bounded_restore_checks,
                  "overall_pass": bounded_restore_pass,
                  "report_readback_ok": rollback_report_readback_ok,
              },
              "prior_report": {
                  "sha256": prior_mpr_report_sha256,
                  "phase": prior_mpr_report_phase,
                  "execution_id": prior_mpr_report_execution_id,
              },
              "candidate_hashes": {
                  "sha256_a": candidate_bundle_sha256_a,
                  "sha256_b": candidate_bundle_sha256_b,
              },
              "notes": notes,
              "read_errors": read_errors,
              "upload_errors": upload_errors,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M12.E",
              "phase_id": "P15",
              "execution_id": exec_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "overall_pass": overall_pass,
          }
          summary = {
              "captured_at_utc": now(),
              "phase": "M12.E",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "upstream_refs": {
                  "m12d_execution_id": upstream_m12d,
              },
              "artifact_keys": {
                  "m12e_rollback_drill_snapshot": f"evidence/dev_full/run_control/{exec_id}/m12e_rollback_drill_snapshot.json",
                  "m12e_blocker_register": f"evidence/dev_full/run_control/{exec_id}/m12e_blocker_register.json",
                  "m12e_execution_summary": f"evidence/dev_full/run_control/{exec_id}/m12e_execution_summary.json",
                  "mpr_rollback_drill_key": mpr_rollback_key,
              },
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          artifacts = {
              "m12e_rollback_drill_snapshot.json": snapshot,
              "m12e_blocker_register.json": blocker_register,
              "m12e_execution_summary.json": summary,
          }
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B5", "message": f"Failed to upload {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M12.F_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M12_F" if overall_pass else "HOLD_REMEDIATE"
          snapshot["upload_errors"] = upload_errors
          snapshot["overall_pass"] = overall_pass
          snapshot["blocker_count"] = len(blockers)
          snapshot["next_gate"] = next_gate
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          summary["overall_pass"] = overall_pass
          summary["blocker_count"] = len(blockers)
          summary["next_gate"] = next_gate
          summary["verdict"] = verdict

          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "upstream_m12d_execution": upstream_m12d,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "mpr_rollback_drill_s3_uri": f"s3://{bucket}/{mpr_rollback_key}" if mpr_rollback_key else "",
                      "evidence_prefix": f"s3://{bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Execute M12.F ACTIVE resolution checks (managed)
        if: ${{ inputs.execution_mode == 'm12f_execute' && inputs.m12_subphase == 'F' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          AWS_REGION: ${{ inputs.aws_region }}
          UPSTREAM_M12E_EXEC: ${{ inputs.upstream_m12e_execution }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import hashlib
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, str]:
              rx = re.compile(r"^\*\s+`([A-Z0-9_]+)\s*=\s*(.+)`\s*$")
              out: dict[str, str] = {}
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if (raw.startswith('"') and raw.endswith('"')) or (raw.startswith("'") and raw.endswith("'")):
                      out[key] = raw[1:-1]
                  else:
                      out[key] = raw
              return out

          def is_placeholder(value: str) -> bool:
              v = (value or "").strip()
              lo = v.lower()
              if not v:
                  return True
              if lo in {"to_pin", "todo", "tbd", "none", "null", "unset"}:
                  return True
              if "<" in v and ">" in v:
                  return True
              return False

          def s3_get_bytes(s3: Any, bucket: str, key: str) -> bytes:
              return s3.get_object(Bucket=bucket, Key=key)["Body"].read()

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              payload = json.loads(s3_get_bytes(s3, bucket, key).decode("utf-8"))
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def s3_parse_uri(uri: str) -> tuple[str, str]:
              if not uri.startswith("s3://"):
                  raise ValueError("not_s3_uri")
              tail = uri[5:]
              if "/" not in tail:
                  raise ValueError("missing_key")
              b, k = tail.split("/", 1)
              if not b or not k:
                  raise ValueError("incomplete_s3_uri")
              return b, k

          def sha256_bytes(data: bytes) -> str:
              return hashlib.sha256(data).hexdigest()

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          bucket = os.environ["EVIDENCE_BUCKET"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          upstream_m12e = os.environ["UPSTREAM_M12E_EXEC"].strip()
          if not upstream_m12e:
              raise SystemExit("UPSTREAM_M12E_EXEC is required for M12.F.")

          s3 = boto3.client("s3", region_name=region)
          handles = parse_handles(Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md"))

          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []
          notes: list[str] = []

          required_handles = [
              "MF_CANDIDATE_BUNDLE_PATH_PATTERN",
              "MPR_PROMOTION_RECEIPT_PATH_PATTERN",
              "MPR_ROLLBACK_DRILL_PATH_PATTERN",
              "SM_ENDPOINT_NAME",
              "SM_SERVING_MODE",
              "FP_BUS_LEARNING_REGISTRY_EVENTS_V1",
              "GLUE_SCHEMA_REGISTRY_NAME",
              "GLUE_SCHEMA_COMPATIBILITY_MODE",
          ]
          missing = [k for k in required_handles if k not in handles]
          placeholders = [k for k in required_handles if k in handles and is_placeholder(str(handles.get(k, "")))]
          if missing or placeholders:
              blockers.append({"code": "M12-B6", "message": "Required M12.F handles unresolved: " + ",".join(sorted(missing + placeholders))})

          m12e_summary_key = f"evidence/dev_full/run_control/{upstream_m12e}/m12e_execution_summary.json"
          m12e_snapshot_key = f"evidence/dev_full/run_control/{upstream_m12e}/m12e_rollback_drill_snapshot.json"
          m12e_summary: dict[str, Any] | None = None
          m12e_snapshot: dict[str, Any] | None = None
          try:
              m12e_summary = s3_get_json(s3, bucket, m12e_summary_key)
              m12e_snapshot = s3_get_json(s3, bucket, m12e_snapshot_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": "m12e_summary_snapshot", "error": type(exc).__name__})
              blockers.append({"code": "M12-B6", "message": "M12.E upstream evidence unreadable."})

          platform_run_id = ""
          scenario_run_id = ""
          m12d_execution_id = ""
          promotion_receipt_key = ""
          rollback_drill_key = ""
          candidate_bundle_ref = ""
          candidate_bundle_key = ""
          m12d_snapshot: dict[str, Any] | None = None
          candidate_bundle: dict[str, Any] | None = None
          promotion_receipt: dict[str, Any] | None = None
          rollback_report: dict[str, Any] | None = None
          candidate_sha_a = ""
          candidate_sha_b = ""
          model_artifact_readable = False

          if m12e_summary and m12e_snapshot:
              if not bool(m12e_summary.get("overall_pass")):
                  blockers.append({"code": "M12-B6", "message": "M12.E summary is not pass posture."})
              if str(m12e_summary.get("next_gate", "")).strip() != "M12.F_READY":
                  blockers.append({"code": "M12-B6", "message": "M12.E next_gate is not M12.F_READY."})
              if int(m12e_summary.get("blocker_count", 0)) != 0:
                  blockers.append({"code": "M12-B6", "message": "M12.E blocker_count is not zero."})

              platform_run_id = str(m12e_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m12e_summary.get("scenario_run_id", "")).strip()
              if not platform_run_id or not scenario_run_id:
                  blockers.append({"code": "M12-B6", "message": "M12.E run scope is incomplete."})

              upstream_refs = m12e_snapshot.get("upstream_refs") if isinstance(m12e_snapshot.get("upstream_refs"), dict) else {}
              target_refs = m12e_snapshot.get("target_refs") if isinstance(m12e_snapshot.get("target_refs"), dict) else {}
              m12d_execution_id = str(upstream_refs.get("m12d_execution_id", "")).strip()
              m12d_snapshot_key = str(upstream_refs.get("m12d_snapshot_key", "")).strip()
              promotion_receipt_key = str(target_refs.get("promotion_receipt_key", "")).strip()
              rollback_drill_key = str(target_refs.get("mpr_rollback_drill_key", "")).strip()
              candidate_bundle_ref = str(target_refs.get("candidate_bundle_ref", "")).strip()
              if not promotion_receipt_key and platform_run_id:
                  promotion_receipt_key = str(handles.get("MPR_PROMOTION_RECEIPT_PATH_PATTERN", "")).replace("{platform_run_id}", platform_run_id)
              if not rollback_drill_key and platform_run_id:
                  rollback_drill_key = str(handles.get("MPR_ROLLBACK_DRILL_PATH_PATTERN", "")).replace("{platform_run_id}", platform_run_id)
              if not candidate_bundle_ref and platform_run_id:
                  candidate_bundle_key = str(handles.get("MF_CANDIDATE_BUNDLE_PATH_PATTERN", "")).replace("{platform_run_id}", platform_run_id)
                  candidate_bundle_ref = f"s3://{bucket}/{candidate_bundle_key}"

              if not m12d_snapshot_key:
                  blockers.append({"code": "M12-B6", "message": "M12.D snapshot key missing from M12.E evidence."})
              else:
                  try:
                      m12d_snapshot = s3_get_json(s3, bucket, m12d_snapshot_key)
                  except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                      read_errors.append({"surface": m12d_snapshot_key, "error": type(exc).__name__})
                      blockers.append({"code": "M12-B6", "message": "M12.D promotion snapshot unreadable."})

              for key_name, key_value, msg in [
                  ("promotion_receipt_key", promotion_receipt_key, "Promotion receipt key unresolved."),
                  ("rollback_drill_key", rollback_drill_key, "Rollback drill key unresolved."),
              ]:
                  if not key_value:
                      blockers.append({"code": "M12-B6", "message": msg})

              if promotion_receipt_key:
                  try:
                      promotion_receipt = s3_get_json(s3, bucket, promotion_receipt_key)
                  except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                      read_errors.append({"surface": promotion_receipt_key, "error": type(exc).__name__})
                      blockers.append({"code": "M12-B6", "message": "Promotion receipt unreadable."})
              if rollback_drill_key:
                  try:
                      rollback_report = s3_get_json(s3, bucket, rollback_drill_key)
                  except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                      read_errors.append({"surface": rollback_drill_key, "error": type(exc).__name__})
                      blockers.append({"code": "M12-B6", "message": "Rollback drill report unreadable."})

              if candidate_bundle_ref:
                  try:
                      b, k = s3_parse_uri(candidate_bundle_ref)
                      candidate_bundle_key = k
                      raw_a = s3_get_bytes(s3, b, k)
                      raw_b = s3_get_bytes(s3, b, k)
                      candidate_sha_a = sha256_bytes(raw_a)
                      candidate_sha_b = sha256_bytes(raw_b)
                      payload = json.loads(raw_a.decode("utf-8"))
                      if not isinstance(payload, dict):
                          raise ValueError("candidate_bundle_not_object")
                      candidate_bundle = payload
                  except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                      read_errors.append({"surface": candidate_bundle_ref, "error": type(exc).__name__})
                      blockers.append({"code": "M12-B6", "message": "Candidate bundle unreadable."})
              else:
                  blockers.append({"code": "M12-B6", "message": "Candidate bundle ref unresolved."})

          promoted_ref_from_receipt = ""
          promoted_ref_from_m12d = ""
          promoted_ref_from_m12e = candidate_bundle_ref
          tenant_scope = ""
          bundle_slot = ""
          registry_event_type = ""
          promotion_status_ok = False
          rollback_status_ok = False
          run_scope_match = False
          candidate_hash_stable = bool(candidate_sha_a and candidate_sha_b and candidate_sha_a == candidate_sha_b)
          serving_endpoint_match = False
          serving_mode_match = False
          package_status_ok = False
          schema_mode_valid = False
          schema_registry_present = False
          topic_handle_valid = False
          topic_match = False
          promoted_status_ok = False

          if promotion_receipt:
              promotion_status_ok = str(promotion_receipt.get("status", "")).strip() == "PROMOTION_COMMITTED"
              promotion = promotion_receipt.get("promotion") if isinstance(promotion_receipt.get("promotion"), dict) else {}
              promoted_ref_from_receipt = str(promotion.get("candidate_bundle_ref", "")).strip()

          if rollback_report:
              rollback_status_ok = str(rollback_report.get("status", "")).strip() == "ROLLBACK_DRILL_PASS"
              bo = rollback_report.get("bounded_restore_objective") if isinstance(rollback_report.get("bounded_restore_objective"), dict) else {}
              rollback_status_ok = rollback_status_ok and bool(bo.get("overall_pass"))

          if m12d_snapshot:
              promotion = m12d_snapshot.get("promotion") if isinstance(m12d_snapshot.get("promotion"), dict) else {}
              promoted_ref_from_m12d = str(promotion.get("candidate_bundle_ref", "")).strip()
              promoted_status_ok = str(promotion.get("bundle_status_after", "")).strip() == "PROMOTED_ACTIVE"
              reg = m12d_snapshot.get("registry_event") if isinstance(m12d_snapshot.get("registry_event"), dict) else {}
              scope = reg.get("scope_key") if isinstance(reg.get("scope_key"), dict) else {}
              tenant_scope = str(scope.get("tenant_id", "")).strip()
              bundle_slot = str(scope.get("bundle_slot", "")).strip()
              registry_event_type = str(reg.get("event_type", "")).strip()
              publication = m12d_snapshot.get("publication") if isinstance(m12d_snapshot.get("publication"), dict) else {}
              topic_match = str(publication.get("topic", "")).strip() == str(handles.get("FP_BUS_LEARNING_REGISTRY_EVENTS_V1", "")).strip()

          if candidate_bundle:
              run_scope_match = (
                  str(candidate_bundle.get("platform_run_id", "")).strip() == platform_run_id
                  and str(candidate_bundle.get("scenario_run_id", "")).strip() == scenario_run_id
              )
              serving = candidate_bundle.get("serving") if isinstance(candidate_bundle.get("serving"), dict) else {}
              serving_endpoint_match = str(serving.get("endpoint_name", "")).strip() == str(handles.get("SM_ENDPOINT_NAME", "")).strip()
              serving_mode_match = str(serving.get("serving_mode", "")).strip() == str(handles.get("SM_SERVING_MODE", "")).strip()
              package = candidate_bundle.get("package") if isinstance(candidate_bundle.get("package"), dict) else {}
              package_status_ok = str(package.get("model_package_group_status", "")).strip() in {"Completed", "InService"}
              model = candidate_bundle.get("model") if isinstance(candidate_bundle.get("model"), dict) else {}
              model_artifact_uri = str(model.get("model_artifact_uri", "")).strip()
              if model_artifact_uri.startswith("s3://"):
                  try:
                      mb, mk = s3_parse_uri(model_artifact_uri)
                      s3.head_object(Bucket=mb, Key=mk)
                      model_artifact_readable = True
                  except Exception:
                      model_artifact_readable = False

          schema_mode_valid = str(handles.get("GLUE_SCHEMA_COMPATIBILITY_MODE", "")).strip() in {"BACKWARD", "FORWARD", "FULL", "NONE"}
          schema_registry_present = bool(str(handles.get("GLUE_SCHEMA_REGISTRY_NAME", "")).strip())
          topic_handle_valid = str(handles.get("FP_BUS_LEARNING_REGISTRY_EVENTS_V1", "")).strip().startswith("fp.bus.learning.")

          ref_set = {r for r in [promoted_ref_from_receipt, promoted_ref_from_m12d, promoted_ref_from_m12e] if r}
          one_active_scope = (
              len(ref_set) == 1
              and tenant_scope == f"{platform_run_id}:{scenario_run_id}"
              and bundle_slot == "active"
              and registry_event_type == "BUNDLE_PROMOTED_ACTIVE"
              and promotion_status_ok
              and rollback_status_ok
          )

          active_resolution_checks = [
              {"name": "one_active_per_scope", "pass": one_active_scope, "detail": f"refs={sorted(ref_set)},tenant_scope={tenant_scope},bundle_slot={bundle_slot}"},
              {"name": "promoted_status_ok", "pass": promoted_status_ok, "detail": "bundle_status_after must be PROMOTED_ACTIVE"},
              {"name": "run_scope_match", "pass": run_scope_match, "detail": f"platform_run_id={platform_run_id},scenario_run_id={scenario_run_id}"},
              {"name": "candidate_hash_stable", "pass": candidate_hash_stable, "detail": f"sha256_a={candidate_sha_a},sha256_b={candidate_sha_b}"},
          ]
          runtime_compat_checks = [
              {"name": "serving_endpoint_match", "pass": serving_endpoint_match, "detail": f"expected={handles.get('SM_ENDPOINT_NAME','')}"},
              {"name": "serving_mode_match", "pass": serving_mode_match, "detail": f"expected={handles.get('SM_SERVING_MODE','')}"},
              {"name": "package_status_ok", "pass": package_status_ok, "detail": "model_package_group_status in {Completed,InService}"},
              {"name": "model_artifact_readable", "pass": model_artifact_readable, "detail": "s3_head_object(model_artifact_uri)"},
              {"name": "schema_registry_present", "pass": schema_registry_present, "detail": str(handles.get("GLUE_SCHEMA_REGISTRY_NAME", ""))},
              {"name": "schema_mode_valid", "pass": schema_mode_valid, "detail": str(handles.get("GLUE_SCHEMA_COMPATIBILITY_MODE", ""))},
              {"name": "topic_handle_valid", "pass": topic_handle_valid, "detail": str(handles.get("FP_BUS_LEARNING_REGISTRY_EVENTS_V1", ""))},
              {"name": "topic_match_promotion_snapshot", "pass": topic_match, "detail": str(handles.get("FP_BUS_LEARNING_REGISTRY_EVENTS_V1", ""))},
          ]

          active_resolution_pass = all(bool(c.get("pass")) for c in active_resolution_checks)
          runtime_compat_pass = all(bool(c.get("pass")) for c in runtime_compat_checks)
          if not active_resolution_pass:
              blockers.append({"code": "M12-B6", "message": "One-active-per-scope checks failed."})
          if not runtime_compat_pass:
              blockers.append({"code": "M12-B6", "message": "Runtime compatibility checks failed."})

          post_promotion_observation = {
              "captured_at_utc": now(),
              "phase": "M12.F",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "active_resolution_checks": active_resolution_checks,
              "runtime_compatibility_checks": runtime_compat_checks,
              "source_refs": {
                  "m12e_execution_id": upstream_m12e,
                  "m12e_summary_key": m12e_summary_key,
                  "m12e_snapshot_key": m12e_snapshot_key,
                  "promotion_receipt_key": promotion_receipt_key,
                  "rollback_drill_key": rollback_drill_key,
                  "candidate_bundle_ref": candidate_bundle_ref,
              },
              "overall_pass": bool(active_resolution_pass and runtime_compat_pass),
          }

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M12.G_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M12_G" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": now(),
              "phase": "M12.F",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_refs": {
                  "m12e_execution_id": upstream_m12e,
                  "m12e_summary_key": m12e_summary_key,
                  "m12e_snapshot_key": m12e_snapshot_key,
                  "m12d_execution_id": m12d_execution_id,
              },
              "required_handles": required_handles,
              "resolved_handles": {k: str(handles.get(k, "")) for k in required_handles},
              "active_resolution": {
                  "overall_pass": active_resolution_pass,
                  "checks": active_resolution_checks,
              },
              "runtime_compatibility": {
                  "overall_pass": runtime_compat_pass,
                  "checks": runtime_compat_checks,
              },
              "refs": {
                  "promotion_receipt_key": promotion_receipt_key,
                  "rollback_drill_key": rollback_drill_key,
                  "candidate_bundle_ref": candidate_bundle_ref,
              },
              "candidate_hashes": {
                  "sha256_a": candidate_sha_a,
                  "sha256_b": candidate_sha_b,
              },
              "notes": notes,
              "read_errors": read_errors,
              "upload_errors": upload_errors,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M12.F",
              "phase_id": "P15",
              "execution_id": exec_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "overall_pass": overall_pass,
          }
          summary = {
              "captured_at_utc": now(),
              "phase": "M12.F",
              "phase_id": "P15",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "upstream_refs": {
                  "m12e_execution_id": upstream_m12e,
                  "m12d_execution_id": m12d_execution_id,
              },
              "artifact_keys": {
                  "m12f_active_resolution_snapshot": f"evidence/dev_full/run_control/{exec_id}/m12f_active_resolution_snapshot.json",
                  "m12_post_promotion_observation_snapshot": f"evidence/dev_full/run_control/{exec_id}/m12_post_promotion_observation_snapshot.json",
                  "m12f_blocker_register": f"evidence/dev_full/run_control/{exec_id}/m12f_blocker_register.json",
                  "m12f_execution_summary": f"evidence/dev_full/run_control/{exec_id}/m12f_execution_summary.json",
              },
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          artifacts = {
              "m12f_active_resolution_snapshot.json": snapshot,
              "m12_post_promotion_observation_snapshot.json": post_promotion_observation,
              "m12f_blocker_register.json": blocker_register,
              "m12f_execution_summary.json": summary,
          }
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M12-B6", "message": f"Failed to upload {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M12.G_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M12_G" if overall_pass else "HOLD_REMEDIATE"
          snapshot["upload_errors"] = upload_errors
          snapshot["overall_pass"] = overall_pass
          snapshot["blocker_count"] = len(blockers)
          snapshot["next_gate"] = next_gate
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          summary["overall_pass"] = overall_pass
          summary["blocker_count"] = len(blockers)
          summary["next_gate"] = next_gate
          summary["verdict"] = verdict
          post_promotion_observation["overall_pass"] = bool(active_resolution_pass and runtime_compat_pass and overall_pass)

          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "upstream_m12e_execution": upstream_m12e,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "evidence_prefix": f"s3://{bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Upload local run artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dev-full-m12-${{ inputs.m12_subphase }}-${{ steps.run_meta.outputs.execution_id }}
          path: ${{ steps.run_meta.outputs.run_dir }}
          if-no-files-found: warn
