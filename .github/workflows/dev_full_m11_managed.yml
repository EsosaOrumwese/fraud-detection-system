name: dev-full-m11-managed

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: AWS region
        required: true
        default: eu-west-2
        type: string
      aws_role_to_assume:
        description: OIDC role ARN
        required: true
        type: string
      evidence_bucket:
        description: S3 evidence bucket
        required: true
        default: fraud-platform-dev-full-evidence
        type: string
      m11_subphase:
        description: "M11 subphase to execute (single-runner lane)"
        required: true
        default: D
        type: choice
        options:
          - D
          - E
          - F
          - G
          - H
          - I
          - J
      upstream_m11c_execution:
        description: Upstream M11.C execution id
        required: true
        default: m11c_input_immutability_20260226T192723Z
        type: string
      upstream_m11d_execution:
        description: Upstream M11.D execution id (required for M11.E)
        required: false
        default: m11d_train_eval_execution_20260227T052312Z
        type: string
      upstream_m11e_execution:
        description: Upstream M11.E execution id (required for M11.F)
        required: false
        default: m11e_eval_gate_20260227T061316Z
        type: string
      upstream_m11f_execution:
        description: Upstream M11.F execution id (required for M11.G)
        required: false
        default: m11f_mlflow_lineage_20260227T075634Z
        type: string
      upstream_m11g_execution:
        description: Upstream M11.G execution id (required for M11.H)
        required: false
        default: m11g_candidate_bundle_20260227T081200Z
        type: string
      upstream_m11h_execution:
        description: Upstream M11.H execution id (required for M11.I)
        required: false
        default: m11h_safe_disable_rollback_20260227T085223Z
        type: string
      m11d_execution_id:
        description: Optional fixed execution id
        required: false
        default: ""
        type: string
      m11e_execution_id:
        description: Optional fixed execution id for M11.E
        required: false
        default: ""
        type: string
      m11f_execution_id:
        description: Optional fixed execution id for M11.F
        required: false
        default: ""
        type: string
      m11g_execution_id:
        description: Optional fixed execution id for M11.G
        required: false
        default: ""
        type: string
      m11h_execution_id:
        description: Optional fixed execution id for M11.H
        required: false
        default: ""
        type: string
      m11i_execution_id:
        description: Optional fixed execution id for M11.I
        required: false
        default: ""
        type: string
      poll_timeout_minutes:
        description: Timeout minutes
        required: true
        default: "45"
        type: string
      poll_interval_seconds:
        description: Poll interval seconds
        required: true
        default: "20"
        type: string
      m11d_training_instance_type:
        description: SageMaker training instance type
        required: true
        default: ml.m5.large
        type: string
      m11d_transform_instance_type:
        description: SageMaker transform instance type
        required: true
        default: ml.c4.xlarge
        type: string
      m11d_require_managed_transform:
        description: Fail closed unless managed batch transform completes
        required: true
        default: "true"
        type: choice
        options:
          - "true"
          - "false"

permissions:
  contents: read
  id-token: write

concurrency:
  group: dev-full-m11-managed-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  run_m11_managed:
    name: Run M11.${{ inputs.m11_subphase }} managed lane
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate subphase support
        shell: bash
        run: |
          set -euo pipefail
          if [[ "${{ inputs.m11_subphase }}" != "D" && "${{ inputs.m11_subphase }}" != "E" && "${{ inputs.m11_subphase }}" != "F" && "${{ inputs.m11_subphase }}" != "G" && "${{ inputs.m11_subphase }}" != "H" && "${{ inputs.m11_subphase }}" != "I" ]]; then
            echo "Unsupported m11_subphase='${{ inputs.m11_subphase }}'."
            echo "Supported subphases for this workflow: D, E, F, G, H, I."
            exit 1
          fi

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore "sagemaker<3" "xgboost==1.7.6"

      - name: Compute execution metadata
        id: run_meta
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ "${{ inputs.m11_subphase }}" == "D" ]]; then
            if [[ -n "${{ inputs.m11d_execution_id }}" ]]; then
              EXEC_ID="${{ inputs.m11d_execution_id }}"
            else
              EXEC_ID="m11d_train_eval_execution_${TS}"
            fi
          elif [[ "${{ inputs.m11_subphase }}" == "E" ]]; then
            if [[ -n "${{ inputs.m11e_execution_id }}" ]]; then
              EXEC_ID="${{ inputs.m11e_execution_id }}"
            else
              EXEC_ID="m11e_eval_gate_${TS}"
            fi
          elif [[ "${{ inputs.m11_subphase }}" == "F" ]]; then
            if [[ -n "${{ inputs.m11f_execution_id }}" ]]; then
              EXEC_ID="${{ inputs.m11f_execution_id }}"
            else
              EXEC_ID="m11f_mlflow_lineage_${TS}"
            fi
          elif [[ "${{ inputs.m11_subphase }}" == "G" ]]; then
            if [[ -n "${{ inputs.m11g_execution_id }}" ]]; then
              EXEC_ID="${{ inputs.m11g_execution_id }}"
            else
              EXEC_ID="m11g_candidate_bundle_${TS}"
            fi
          elif [[ "${{ inputs.m11_subphase }}" == "H" ]]; then
            if [[ -n "${{ inputs.m11h_execution_id }}" ]]; then
              EXEC_ID="${{ inputs.m11h_execution_id }}"
            else
              EXEC_ID="m11h_safe_disable_rollback_${TS}"
            fi
          elif [[ "${{ inputs.m11_subphase }}" == "I" ]]; then
            if [[ -n "${{ inputs.m11i_execution_id }}" ]]; then
              EXEC_ID="${{ inputs.m11i_execution_id }}"
            else
              EXEC_ID="m11i_p14_gate_rollup_${TS}"
            fi
          else
            echo "Unsupported m11_subphase='${{ inputs.m11_subphase }}' for execution metadata."
            exit 1
          fi
          echo "execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=runs/dev_substrate/dev_full/m11/${EXEC_ID}" >> "$GITHUB_OUTPUT"

      - name: Execute M11.D (managed)
        if: ${{ inputs.m11_subphase == 'D' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M11C_EXEC: ${{ inputs.upstream_m11c_execution }}
          AWS_REGION: ${{ inputs.aws_region }}
          POLL_TIMEOUT_MINUTES: ${{ inputs.poll_timeout_minutes }}
          POLL_INTERVAL_SECONDS: ${{ inputs.poll_interval_seconds }}
          TRAINING_INSTANCE_TYPE: ${{ inputs.m11d_training_instance_type }}
          TRANSFORM_INSTANCE_TYPE: ${{ inputs.m11d_transform_instance_type }}
          REQUIRE_MANAGED_TRANSFORM: ${{ inputs.m11d_require_managed_transform }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import io, json, os, tarfile, time, hashlib, re, tempfile
          from datetime import datetime, timezone
          from pathlib import Path
          import boto3
          from botocore.exceptions import ClientError, BotoCoreError
          from sagemaker import image_uris

          def now():
              return datetime.now(timezone.utc).isoformat().replace('+00:00','Z')
          def s3json(s3,b,k):
              return json.loads(s3.get_object(Bucket=b,Key=k)['Body'].read().decode('utf-8'))
          def putj(s3,b,k,p):
              s3.put_object(Bucket=b,Key=k,Body=json.dumps(p,indent=2,ensure_ascii=True).encode('utf-8'),ContentType='application/json')
          def putt(s3,b,k,t,ct='text/csv'):
              s3.put_object(Bucket=b,Key=k,Body=t.encode('utf-8'),ContentType=ct)
          def wait_train(sm,name,to,poll):
              st=time.time()
              while True:
                  d=sm.describe_training_job(TrainingJobName=name)
                  s=d.get('TrainingJobStatus','')
                  if s in ('Completed','Failed','Stopped'): return d
                  if time.time()-st>to: raise TimeoutError('training timeout')
                  time.sleep(poll)
          def wait_xf(sm,name,to,poll):
              st=time.time()
              while True:
                  d=sm.describe_transform_job(TransformJobName=name)
                  s=d.get('TransformJobStatus','')
                  if s in ('Completed','Failed','Stopped'): return d
                  if time.time()-st>to: raise TimeoutError('transform timeout')
                  time.sleep(poll)
          def job(s):
              s=re.sub(r'[^A-Za-z0-9-]+','-',s).strip('-')
              return (s[:63] or 'm11d-job').rstrip('-')
          def mk(i,seed):
              f1=((i+seed%17)%100)/100.0
              f2=((i*7+seed%31)%100)/100.0
              f3=((i*13+seed%47)%100)/100.0
              y=1 if (1.3*f1+0.9*f2+0.6*f3) >= (0.95+(seed%9)*0.01) else 0
              return y,round(f1,6),round(f2,6),round(f3,6)
          def metric_pack(labels,preds):
              if not labels or len(labels)!=len(preds):
                  return {'accuracy':0.0,'precision':0.0,'recall':0.0}
              tp=fp=tn=fn=0
              for y,p in zip(labels,preds):
                  yh=1 if p>=0.5 else 0
                  if y==1 and yh==1: tp+=1
                  elif y==0 and yh==1: fp+=1
                  elif y==0 and yh==0: tn+=1
                  else: fn+=1
              n=len(labels)
              return {
                  'accuracy':round((tp+tn)/n,6),
                  'precision':round(tp/max(tp+fp,1),6),
                  'recall':round(tp/max(tp+fn,1),6),
              }

          exec_id=os.environ['EXEC_ID']
          run_dir=Path(os.environ['RUN_DIR'])
          bkt=os.environ['EVIDENCE_BUCKET']
          up=os.environ['UPSTREAM_M11C_EXEC']
          region=os.environ['AWS_REGION']
          timeout=max(int(os.environ.get('POLL_TIMEOUT_MINUTES','45')),1)*60
          poll=max(int(os.environ.get('POLL_INTERVAL_SECONDS','20')),5)
          training_instance_type=str(os.environ.get('TRAINING_INSTANCE_TYPE','ml.m5.large')).strip() or 'ml.m5.large'
          transform_instance_type=str(os.environ.get('TRANSFORM_INSTANCE_TYPE','ml.c4.xlarge')).strip() or 'ml.c4.xlarge'
          require_managed_transform=str(os.environ.get('REQUIRE_MANAGED_TRANSFORM','true')).strip().lower() == 'true'

          s3=boto3.client('s3',region_name=region)
          ssm=boto3.client('ssm',region_name=region)
          sm=boto3.client('sagemaker',region_name=region)
          blockers=[]
          errs=[]
          advisories=[]

          m11c_key=f'evidence/dev_full/run_control/{up}/m11c_execution_summary.json'
          m11c=s3json(s3,bkt,m11c_key)
          if not m11c.get('overall_pass'): blockers.append({'code':'M11-B4','message':'M11.C not pass','surface':m11c_key})
          if m11c.get('next_gate')!='M11.D_READY': blockers.append({'code':'M11-B4','message':'M11.C next_gate mismatch','surface':m11c_key})
          pr=str(m11c.get('platform_run_id','')).strip(); sr=str(m11c.get('scenario_run_id','')).strip()
          if not pr or not sr: blockers.append({'code':'M11-B4','message':'run scope missing','surface':m11c_key})

          m11b_exec=m11c.get('upstream_refs',{}).get('m11b_execution_id','')
          m11a_exec=m11c.get('upstream_refs',{}).get('m11a_execution_id','')
          m11b_snap=s3json(s3,bkt,f'evidence/dev_full/run_control/{m11b_exec}/m11b_sagemaker_readiness_snapshot.json') if m11b_exec else {}
          m11a_snap=s3json(s3,bkt,f'evidence/dev_full/run_control/{m11a_exec}/m11a_handle_closure_snapshot.json') if m11a_exec else {}

          role=m11b_snap.get('ssm_checks',{}).get('ssm_sagemaker_role_arn_value','')
          role_path=''
          train_prefix='fraud-platform-dev-full-mtrain'
          batch_prefix='fraud-platform-dev-full-mbatch'
          eval_pat=f'evidence/runs/{pr}/learning/mf/eval_report.json'
          budget_pat=f'evidence/dev_full/run_control/{exec_id}/phase_budget_envelope.json'
          for row in m11b_snap.get('handle_matrix',[]):
              if row.get('handle')=='SSM_SAGEMAKER_MODEL_EXEC_ROLE_ARN_PATH': role_path=row.get('value','')
          for row in m11a_snap.get('handle_matrix',[]):
              h=row.get('handle'); v=row.get('value','')
              if h=='SM_TRAINING_JOB_NAME_PREFIX': train_prefix=v
              if h=='SM_BATCH_TRANSFORM_JOB_NAME_PREFIX': batch_prefix=v
              if h=='MF_EVAL_REPORT_PATH_PATTERN': eval_pat=v.replace('{platform_run_id}',pr)
              if h=='PHASE_BUDGET_ENVELOPE_PATH_PATTERN': budget_pat=v.replace('{phase_execution_id}',exec_id)

          if not role and role_path:
              try: role=ssm.get_parameter(Name=role_path)['Parameter']['Value']
              except Exception as e: blockers.append({'code':'M11-B4','message':'role resolve failed','surface':role_path}); errs.append(str(e))
          if not role: blockers.append({'code':'M11-B4','message':'role unresolved','surface':'ROLE_SAGEMAKER_EXECUTION'})

          m11c_snap=s3json(s3,bkt,f'evidence/dev_full/run_control/{up}/m11c_input_immutability_snapshot.json')
          fp_ref=m11c_snap.get('resolved_refs',{}).get('m10g_fingerprint_ref','')
          if not fp_ref: blockers.append({'code':'M11-B4','message':'fingerprint ref missing','surface':'m11c_snapshot'})
          if fp_ref.startswith('s3://'):
              x=fp_ref[5:].split('/',1); fb, fk=x[0],x[1]
          else:
              fb, fk=bkt, fp_ref.lstrip('/')
          fps=s3json(s3,fb,fk).get('fingerprint_sha256','') if fp_ref else ''
          if not fps: blockers.append({'code':'M11-B4','message':'fingerprint digest missing','surface':fp_ref or 'm10g_fingerprint'})

          envelope={
              'captured_at_utc':now(),'phase':'M11.D','phase_id':'P14','execution_id':exec_id,
              'platform_run_id':pr,'scenario_run_id':sr,
              'runtime_budget':{'target_minutes':45,'hard_alert_minutes':60,'configured_timeout_minutes':timeout//60,'poll_interval_seconds':poll},
              'status':'EMITTED_PRE_LAUNCH'
          }
          putj(s3,bkt,budget_pat.lstrip('/'),envelope)

          tdesc={}; xdesc={}; metrics={'accuracy':0.0,'precision':0.0,'recall':0.0}; model=''; tjob=''; xjob=''; mdata=''; outk=''
          eval_mode='managed_batch_transform'
          inp={}
          if not blockers:
              seed=int(fps[:8],16)
              rows=[mk(i,seed) for i in range(240)]
              tr,va,te=rows[:160],rows[160:200],rows[200:240]
              labels=[r[0] for r in te]
              pfx=f'evidence/runs/{pr}/learning/mf/input/{exec_id}'
              ktr=f'{pfx}/train/train.csv'; kva=f'{pfx}/validation/validation.csv'; kte=f'{pfx}/test/test_features.csv'; kl=f'{pfx}/test/test_labels.json'
              putt(s3,bkt,ktr,'\n'.join(f"{y},{a},{b},{c}" for y,a,b,c in tr)+'\n')
              putt(s3,bkt,kva,'\n'.join(f"{y},{a},{b},{c}" for y,a,b,c in va)+'\n')
              putt(s3,bkt,kte,'\n'.join(f"{a},{b},{c}" for _,a,b,c in te)+'\n')
              putj(s3,bkt,kl,{'labels':labels,'platform_run_id':pr,'scenario_run_id':sr,'execution_id':exec_id})
              inp={'train_key':ktr,'validation_key':kva,'test_features_key':kte,'test_labels_key':kl}

              image=image_uris.retrieve(
                  framework='xgboost',
                  region=region,
                  version='1.7-1',
                  image_scope='training',
                  instance_type=training_instance_type,
              )
              suf=hashlib.sha256(f'{exec_id}|{fps}'.encode()).hexdigest()[:10]
              tjob=job(f'{train_prefix}-{suf}'); xjob=job(f'{batch_prefix}-{suf}'); model=job(f'{train_prefix}-model-{suf}')
              tr_uri=f"s3://{bkt}/{ktr.rsplit('/',1)[0]}/"
              va_uri=f"s3://{bkt}/{kva.rsplit('/',1)[0]}/"
              # Use object-specific S3 URI so transform does not ingest sibling JSON labels.
              te_uri=f"s3://{bkt}/{kte}"
              out_train=f's3://{bkt}/evidence/runs/{pr}/learning/mf/train_output/{exec_id}/'
              out_xf=f's3://{bkt}/evidence/runs/{pr}/learning/mf/transform_output/{exec_id}/'
              try:
                  sm.create_training_job(
                    TrainingJobName=tjob,
                    AlgorithmSpecification={'TrainingImage':image,'TrainingInputMode':'File'},
                    RoleArn=role,
                    InputDataConfig=[
                      {'ChannelName':'train','DataSource':{'S3DataSource':{'S3DataType':'S3Prefix','S3Uri':tr_uri,'S3DataDistributionType':'FullyReplicated'}},'ContentType':'text/csv'},
                      {'ChannelName':'validation','DataSource':{'S3DataSource':{'S3DataType':'S3Prefix','S3Uri':va_uri,'S3DataDistributionType':'FullyReplicated'}},'ContentType':'text/csv'}],
                    OutputDataConfig={'S3OutputPath':out_train},
                    ResourceConfig={'InstanceType':training_instance_type,'InstanceCount':1,'VolumeSizeInGB':30},
                    StoppingCondition={'MaxRuntimeInSeconds':timeout},
                    HyperParameters={'objective':'binary:logistic','eval_metric':'auc','num_round':'25','max_depth':'4','eta':'0.2','subsample':'0.8','verbosity':'0'} )
                  tdesc=wait_train(sm,tjob,timeout,poll)
                  if tdesc.get('TrainingJobStatus')!='Completed': raise RuntimeError(f"training status={tdesc.get('TrainingJobStatus')}")
                  mdata=tdesc.get('ModelArtifacts',{}).get('S3ModelArtifacts','')
                  if not mdata: raise RuntimeError('missing model artifact')
                  preds=[]
                  labels=json.loads(s3.get_object(Bucket=bkt,Key=kl)['Body'].read().decode('utf-8')).get('labels',[])
                  try:
                      sm.create_model(ModelName=model,ExecutionRoleArn=role,PrimaryContainer={'Image':image,'ModelDataUrl':mdata})
                      sm.create_transform_job(
                        TransformJobName=xjob,ModelName=model,
                        TransformInput={'DataSource':{'S3DataSource':{'S3DataType':'S3Prefix','S3Uri':te_uri}},'ContentType':'text/csv','SplitType':'Line'},
                        TransformOutput={'S3OutputPath':out_xf,'AssembleWith':'Line','Accept':'text/csv'},
                        TransformResources={'InstanceType':transform_instance_type,'InstanceCount':1})
                      xdesc=wait_xf(sm,xjob,timeout,poll)
                      if xdesc.get('TransformJobStatus')!='Completed': raise RuntimeError(f"transform status={xdesc.get('TransformJobStatus')}")
                      out_uri=xdesc.get('TransformOutput',{}).get('S3OutputPath','')
                      xb,xp=out_uri[5:].split('/',1)
                      objs=s3.list_objects_v2(Bucket=xb,Prefix=xp).get('Contents',[])
                      outs=[o.get('Key','') for o in objs if o.get('Key','').endswith('.out')]
                      if not outs: raise RuntimeError('no transform output')
                      outk=outs[0]
                      pred_raw=s3.get_object(Bucket=xb,Key=outk)['Body'].read().decode('utf-8').splitlines()
                      for line in pred_raw:
                          tok=line.strip().split(',')[0].strip()
                          if tok:
                              try: preds.append(float(tok))
                              except Exception: pass
                  except Exception as transform_exc:
                      msg=str(transform_exc)
                      if 'ResourceLimitExceeded' in msg and 'transform job usage' in msg:
                          if require_managed_transform:
                              raise RuntimeError(
                                  f"managed transform required but unavailable ({transform_instance_type}): {msg}"
                              )
                          eval_mode='fallback_local_model_eval'
                          xdesc={'TransformJobStatus':'SKIPPED_RESOURCE_LIMIT','FailureReason':msg}
                          advisories.append({
                              'code':'M11D-AD1',
                              'message':'Batch transform quota unavailable; using local model-artifact evaluation fallback.',
                              'surface':'sagemaker_transform_quota',
                          })
                          import xgboost as xgb
                          mb,mk=mdata[5:].split('/',1)
                          blob=s3.get_object(Bucket=mb,Key=mk)['Body'].read()
                          with tarfile.open(fileobj=io.BytesIO(blob), mode='r:gz') as tf:
                              members=[m for m in tf.getmembers() if m.isfile()]
                              if not members:
                                  raise RuntimeError('no model file in model artifact tar')
                              chosen=next((m for m in members if 'xgboost' in m.name.lower() or 'model' in m.name.lower()), members[0])
                              with tf.extractfile(chosen) as fh:
                                  model_bytes=fh.read()
                          with tempfile.NamedTemporaryFile(suffix='.model') as model_file:
                              model_file.write(model_bytes)
                              model_file.flush()
                              booster=xgb.Booster()
                              booster.load_model(model_file.name)
                          feat_lines=s3.get_object(Bucket=bkt,Key=kte)['Body'].read().decode('utf-8').splitlines()
                          rows_x=[]
                          for line in feat_lines:
                              raw=line.strip()
                              if not raw:
                                  continue
                              rows_x.append([float(v) for v in raw.split(',')])
                          if not rows_x:
                              raise RuntimeError('empty fallback feature rows')
                          preds=booster.predict(xgb.DMatrix(rows_x)).tolist()
                      else:
                          raise
                  metrics=metric_pack(labels,preds)
              except Exception as e:
                  blockers.append({'code':'M11-B4','message':f'managed train/eval failed: {type(e).__name__}','surface':'sagemaker'})
                  errs.append(str(e))

          if not blockers:
              putj(s3,bkt,eval_pat.lstrip('/'),{
                'captured_at_utc':now(),'phase':'M11.D','phase_id':'P14','execution_id':exec_id,
                'platform_run_id':pr,'scenario_run_id':sr,
                'training_job_name':tjob,'transform_job_name':xjob,'eval_mode':eval_mode,'metrics':metrics,'status':'COMMITTED',
                'upstream_m11c_execution':up})

          tel=None; xel=None; total=0.0
          if tdesc.get('TrainingStartTime') and tdesc.get('TrainingEndTime'):
              tel=max((tdesc['TrainingEndTime']-tdesc['TrainingStartTime']).total_seconds(),0.0); total+=tel
          if xdesc.get('TransformStartTime') and xdesc.get('TransformEndTime'):
              xel=max((xdesc['TransformEndTime']-xdesc['TransformStartTime']).total_seconds(),0.0); total+=xel

          ok=len(blockers)==0
          next_gate='M11.E_READY' if ok else 'HOLD_REMEDIATE'
          verdict='ADVANCE_TO_M11_E' if ok else 'HOLD_REMEDIATE'

          snap={'captured_at_utc':now(),'phase':'M11.D','phase_id':'P14','execution_id':exec_id,'platform_run_id':pr,'scenario_run_id':sr,
                'upstream_refs':{'m11c_execution_id':up,'m11b_execution_id':m11b_exec,'m11a_execution_id':m11a_exec},
                'input_refs':{'m10g_fingerprint_ref':fp_ref,'deterministic_input_keys':inp},
                'budget_envelope_key':budget_pat.lstrip('/'),'eval_report_key':eval_pat.lstrip('/'),
                'jobs':{'training':{'job_name':tjob,'status':tdesc.get('TrainingJobStatus',''),'failure_reason':tdesc.get('FailureReason',''),'model_artifact_uri':mdata,'elapsed_seconds':tel},
                        'transform':{'job_name':xjob,'status':xdesc.get('TransformJobStatus',''),'failure_reason':xdesc.get('FailureReason',''),'model_name':model,'output_key':outk,'elapsed_seconds':xel,'eval_mode':eval_mode}},
                'metrics':metrics,'runtime':{
                    'elapsed_seconds':round(total,3),
                    'budget_target_minutes':45,
                    'budget_alert_minutes':60,
                    'poll_timeout_minutes':timeout//60,
                    'poll_interval_seconds':poll,
                    'training_instance_type':training_instance_type,
                    'transform_instance_type':transform_instance_type,
                    'require_managed_transform':require_managed_transform,
                },
                'read_errors':errs,'advisories':advisories, 'overall_pass':ok,'blocker_count':len(blockers),'next_gate':next_gate}
          br={'captured_at_utc':now(),'phase':'M11.D','phase_id':'P14','execution_id':exec_id,'blocker_count':len(blockers),'blockers':blockers,'overall_pass':ok}
          smy={'captured_at_utc':now(),'phase':'M11.D','phase_id':'P14','execution_id':exec_id,'platform_run_id':pr,'scenario_run_id':sr,
               'overall_pass':ok,'blocker_count':len(blockers),'verdict':verdict,'next_gate':next_gate,
               'upstream_refs':{'m11c_execution_id':up,'m11b_execution_id':m11b_exec,'m11a_execution_id':m11a_exec},
               'artifact_keys':{'m11_phase_budget_envelope':budget_pat.lstrip('/'),'m11d_train_eval_execution_snapshot':f'evidence/dev_full/run_control/{exec_id}/m11d_train_eval_execution_snapshot.json','m11d_blocker_register':f'evidence/dev_full/run_control/{exec_id}/m11d_blocker_register.json','m11d_execution_summary':f'evidence/dev_full/run_control/{exec_id}/m11d_execution_summary.json','m11_eval_report':eval_pat.lstrip('/')}}

          run_dir.mkdir(parents=True,exist_ok=True)
          arts={'m11_phase_budget_envelope.json':envelope,'m11d_train_eval_execution_snapshot.json':snap,'m11d_blocker_register.json':br,'m11d_execution_summary.json':smy}
          for fn,p in arts.items():
              (run_dir/fn).write_text(json.dumps(p,indent=2,ensure_ascii=True)+'\n',encoding='utf-8')
          pref=f'evidence/dev_full/run_control/{exec_id}/'
          for fn,p in arts.items():
              putj(s3,bkt,pref+fn,p)

          print(json.dumps({'execution_id':exec_id,'overall_pass':ok,'blocker_count':len(blockers),'next_gate':next_gate,'verdict':verdict,'evidence_prefix':f's3://{bkt}/{pref}'},indent=2,ensure_ascii=True))
          if not ok: raise SystemExit(1)
          PY

      - name: Execute M11.E (managed)
        if: ${{ inputs.m11_subphase == 'E' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M11D_EXEC: ${{ inputs.upstream_m11d_execution }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import json, os, re
          from decimal import Decimal, InvalidOperation
          from pathlib import Path
          from datetime import datetime, timezone
          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, object]:
              out: dict[str, object] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      value: object = raw[1:-1]
                  elif raw.lower() == "true":
                      value = True
                  elif raw.lower() == "false":
                      value = False
                  else:
                      try:
                          value = int(raw) if "." not in raw else float(raw)
                      except ValueError:
                          value = raw
                  out[key] = value
              return out

          def is_placeholder(value: object) -> bool:
              s = str(value).strip()
              sl = s.lower()
              if not s:
                  return True
              if sl in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "placeholder" in sl or "to_pin" in sl:
                  return True
              if "<" in s and ">" in s:
                  return True
              return False

          def as_decimal(value: object, key: str) -> Decimal:
              try:
                  return Decimal(str(value).strip())
              except (InvalidOperation, ValueError) as exc:
                  raise ValueError(f"invalid_decimal:{key}:{value}") from exc

          def s3_get_json(s3: object, bucket: str, key: str) -> dict[str, object]:
              body = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: object, bucket: str, key: str, payload: dict[str, object]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          bucket = os.environ["EVIDENCE_BUCKET"].strip()
          upstream = os.environ["UPSTREAM_M11D_EXEC"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"

          if not upstream:
              raise SystemExit("UPSTREAM_M11D_EXEC is required for M11.E.")

          s3 = boto3.client("s3", region_name=region)
          handles = parse_handles(HANDLES_PATH)
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []
          notes: list[str] = []

          required_handle_keys = [
              "MF_EVAL_ACCURACY_MIN",
              "MF_EVAL_PRECISION_MIN",
              "MF_EVAL_RECALL_MIN",
              "MF_EVAL_BASELINE_DELTA_ACCURACY_MIN",
              "MF_EVAL_BASELINE_DELTA_PRECISION_MIN",
              "MF_EVAL_BASELINE_DELTA_RECALL_MIN",
              "MF_EVAL_LEAKAGE_HARD_FAIL",
              "MF_EVAL_STABILITY_MAX_DELTA_PCT",
              "MF_EVAL_REPORT_PATH_PATTERN",
              "MF_LEAKAGE_PROVENANCE_CHECK_PATH_PATTERN",
              "LEARNING_LEAKAGE_GUARDRAIL_REPORT_PATH_PATTERN",
          ]
          missing_handles = [k for k in required_handle_keys if k not in handles]
          placeholder_handles = [k for k in required_handle_keys if k in handles and is_placeholder(handles.get(k))]
          if missing_handles or placeholder_handles:
              blockers.append({
                  "code": "M11-B5.1",
                  "message": "Required M11.E handles unresolved: "
                  + ",".join(sorted(missing_handles + placeholder_handles)),
              })

          m11d_summary_key = f"evidence/dev_full/run_control/{upstream}/m11d_execution_summary.json"
          m11d_snapshot_key = f"evidence/dev_full/run_control/{upstream}/m11d_train_eval_execution_snapshot.json"
          m11d_summary: dict[str, object] | None = None
          m11d_snapshot: dict[str, object] | None = None
          try:
              m11d_summary = s3_get_json(s3, bucket, m11d_summary_key)
              m11d_snapshot = s3_get_json(s3, bucket, m11d_snapshot_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": "m11d_summary_or_snapshot", "error": type(exc).__name__})
              blockers.append({"code": "M11-B5.2", "message": "Authoritative M11.D evidence unreadable."})

          platform_run_id = ""
          scenario_run_id = ""
          eval_report_key = ""
          leakage_guardrail_key = ""
          leakage_prov_key = ""
          eval_report: dict[str, object] | None = None
          leakage_guardrail: dict[str, object] | None = None

          if m11d_summary and m11d_snapshot:
              if not bool(m11d_summary.get("overall_pass")):
                  blockers.append({"code": "M11-B5.2", "message": "M11.D summary not pass posture."})
              if str(m11d_summary.get("next_gate", "")).strip() != "M11.E_READY":
                  blockers.append({"code": "M11-B5.2", "message": "M11.D next_gate is not M11.E_READY."})
              if str(m11d_snapshot.get("next_gate", "")).strip() != "M11.E_READY":
                  blockers.append({"code": "M11-B5.2", "message": "M11.D snapshot next_gate is not M11.E_READY."})
              platform_run_id = str(m11d_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m11d_summary.get("scenario_run_id", "")).strip()
              if not platform_run_id or not scenario_run_id:
                  blockers.append({"code": "M11-B5.2", "message": "Run scope unresolved from M11.D summary."})

          if platform_run_id:
              eval_report_pat = str(handles.get("MF_EVAL_REPORT_PATH_PATTERN", "")).strip()
              leakage_guardrail_pat = str(handles.get("LEARNING_LEAKAGE_GUARDRAIL_REPORT_PATH_PATTERN", "")).strip()
              leakage_prov_pat = str(handles.get("MF_LEAKAGE_PROVENANCE_CHECK_PATH_PATTERN", "")).strip()
              eval_report_key = eval_report_pat.replace("{platform_run_id}", platform_run_id)
              leakage_guardrail_key = leakage_guardrail_pat.replace("{platform_run_id}", platform_run_id)
              leakage_prov_key = leakage_prov_pat.replace("{platform_run_id}", platform_run_id)
              try:
                  eval_report = s3_get_json(s3, bucket, eval_report_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": eval_report_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B5.2", "message": "MF eval report unreadable."})
              try:
                  leakage_guardrail = s3_get_json(s3, bucket, leakage_guardrail_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": leakage_guardrail_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B5.3", "message": "Learning leakage guardrail report unreadable."})

          policy = {}
          for k in required_handle_keys:
              if k in handles:
                  policy[k] = handles[k]

          metric_min = {}
          delta_min = {}
          try:
              metric_min = {
                  "accuracy": as_decimal(handles.get("MF_EVAL_ACCURACY_MIN", "0"), "MF_EVAL_ACCURACY_MIN"),
                  "precision": as_decimal(handles.get("MF_EVAL_PRECISION_MIN", "0"), "MF_EVAL_PRECISION_MIN"),
                  "recall": as_decimal(handles.get("MF_EVAL_RECALL_MIN", "0"), "MF_EVAL_RECALL_MIN"),
              }
              delta_min = {
                  "accuracy": as_decimal(handles.get("MF_EVAL_BASELINE_DELTA_ACCURACY_MIN", "0"), "MF_EVAL_BASELINE_DELTA_ACCURACY_MIN"),
                  "precision": as_decimal(handles.get("MF_EVAL_BASELINE_DELTA_PRECISION_MIN", "0"), "MF_EVAL_BASELINE_DELTA_PRECISION_MIN"),
                  "recall": as_decimal(handles.get("MF_EVAL_BASELINE_DELTA_RECALL_MIN", "0"), "MF_EVAL_BASELINE_DELTA_RECALL_MIN"),
              }
              stability_max_delta_pct = as_decimal(
                  handles.get("MF_EVAL_STABILITY_MAX_DELTA_PCT", "0"),
                  "MF_EVAL_STABILITY_MAX_DELTA_PCT",
              )
          except ValueError as exc:
              blockers.append({"code": "M11-B5.1", "message": str(exc)})
              metric_min = {"accuracy": Decimal("0"), "precision": Decimal("0"), "recall": Decimal("0")}
              delta_min = {"accuracy": Decimal("0"), "precision": Decimal("0"), "recall": Decimal("0")}
              stability_max_delta_pct = Decimal("0")

          leakage_hard_fail = str(handles.get("MF_EVAL_LEAKAGE_HARD_FAIL", "true")).strip().lower() == "true"

          candidate_metrics_dec = {"accuracy": Decimal("0"), "precision": Decimal("0"), "recall": Decimal("0")}
          performance_gate_rows: list[dict[str, object]] = []
          leakage_gate_pass = True
          stability_gate_pass = True
          stability_delta_pct = Decimal("0")

          if eval_report:
              metrics_obj = eval_report.get("metrics", {})
              if not isinstance(metrics_obj, dict):
                  blockers.append({"code": "M11-B5.2", "message": "Eval report metrics missing/non-object."})
              else:
                  for metric_name in ("accuracy", "precision", "recall"):
                      if metric_name not in metrics_obj:
                          blockers.append({"code": "M11-B5.2", "message": f"Eval report metric missing: {metric_name}."})
                          continue
                      try:
                          candidate_metrics_dec[metric_name] = as_decimal(metrics_obj.get(metric_name), metric_name)
                      except ValueError as exc:
                          blockers.append({"code": "M11-B5.2", "message": str(exc)})

          if leakage_guardrail:
              leakage_guardrail_pass = bool(leakage_guardrail.get("overall_pass", False))
              if leakage_hard_fail and not leakage_guardrail_pass:
                  leakage_gate_pass = False
                  blockers.append({"code": "M11-B5.3", "message": "Leakage guardrail failed under hard-fail policy."})
          else:
              if leakage_hard_fail:
                  leakage_gate_pass = False
                  blockers.append({"code": "M11-B5.3", "message": "Leakage hard-fail policy enabled but guardrail evidence missing."})
              else:
                  notes.append("Leakage hard-fail disabled; missing guardrail evidence tolerated.")

          for metric_name in ("accuracy", "precision", "recall"):
              candidate = candidate_metrics_dec.get(metric_name, Decimal("0"))
              floor = metric_min.get(metric_name, Decimal("0"))
              delta = candidate - floor
              floor_ok = candidate >= floor
              delta_ok = delta >= delta_min.get(metric_name, Decimal("0"))
              row = {
                  "metric": metric_name,
                  "candidate": float(candidate),
                  "baseline_policy_floor": float(floor),
                  "delta_vs_floor": float(delta),
                  "required_delta_floor": float(delta_min.get(metric_name, Decimal("0"))),
                  "floor_ok": floor_ok,
                  "delta_ok": delta_ok,
                  "pass": bool(floor_ok and delta_ok),
              }
              performance_gate_rows.append(row)
              if not row["pass"]:
                  blockers.append({"code": "M11-B5.4", "message": f"Performance gate failed for {metric_name}."})

          if eval_report:
              raw_stability = eval_report.get("stability_delta_pct")
              if raw_stability is None:
                  stability_delta_pct = Decimal("0")
                  notes.append("stability_delta_pct absent in eval report; using deterministic single-run delta=0 for M11.E.")
              else:
                  try:
                      stability_delta_pct = as_decimal(raw_stability, "stability_delta_pct")
                  except ValueError:
                      stability_delta_pct = Decimal("9999")
              stability_gate_pass = stability_delta_pct <= stability_max_delta_pct
              if not stability_gate_pass:
                  blockers.append({"code": "M11-B5.5", "message": "Stability tolerance exceeded."})

          leakage_provenance_check = {
              "captured_at_utc": now(),
              "phase": "M11.E",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_m11d_execution": upstream,
              "source_refs": {
                  "m11d_summary": f"s3://{bucket}/{m11d_summary_key}",
                  "m11d_snapshot": f"s3://{bucket}/{m11d_snapshot_key}",
                  "mf_eval_report": f"s3://{bucket}/{eval_report_key}" if eval_report_key else None,
                  "learning_leakage_guardrail_report": f"s3://{bucket}/{leakage_guardrail_key}" if leakage_guardrail_key else None,
              },
              "checks": {
                  "run_scope_present": bool(platform_run_id and scenario_run_id),
                  "eval_report_readable": bool(eval_report is not None),
                  "leakage_guardrail_readable": bool(leakage_guardrail is not None),
                  "leakage_hard_fail_enabled": leakage_hard_fail,
                  "leakage_gate_pass": leakage_gate_pass,
              },
              "overall_pass": len(blockers) == 0,
          }

          baseline_report = {
              "captured_at_utc": now(),
              "phase": "M11.E",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "baseline_mode": "policy_floor",
              "policy": {
                  "metric_minima": {k: float(v) for k, v in metric_min.items()},
                  "baseline_delta_floors": {k: float(v) for k, v in delta_min.items()},
                  "leakage_hard_fail": leakage_hard_fail,
                  "stability_max_delta_pct": float(stability_max_delta_pct),
              },
              "candidate_metrics": {k: float(v) for k, v in candidate_metrics_dec.items()},
              "metric_comparisons": performance_gate_rows,
              "stability": {
                  "observed_delta_pct": float(stability_delta_pct),
                  "max_allowed_delta_pct": float(stability_max_delta_pct),
                  "pass": stability_gate_pass,
              },
              "leakage": {
                  "hard_fail_enabled": leakage_hard_fail,
                  "pass": leakage_gate_pass,
              },
              "overall_pass": len(blockers) == 0,
          }

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.F_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_F" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": now(),
              "phase": "M11.E",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_refs": {"m11d_execution_id": upstream},
              "resolved_policy_handles": policy,
              "source_refs": {
                  "m11d_summary_key": m11d_summary_key,
                  "m11d_snapshot_key": m11d_snapshot_key,
                  "mf_eval_report_key": eval_report_key,
                  "learning_leakage_guardrail_key": leakage_guardrail_key,
                  "mf_leakage_provenance_check_key": leakage_prov_key,
              },
              "gate_results": {
                  "compatibility": bool(eval_report is not None),
                  "leakage": leakage_gate_pass,
                  "performance": all(bool(r.get("pass")) for r in performance_gate_rows),
                  "stability": stability_gate_pass,
              },
              "read_errors": read_errors,
              "upload_errors": upload_errors,
              "notes": notes,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M11.E",
              "phase_id": "P14",
              "execution_id": exec_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "overall_pass": overall_pass,
          }
          summary = {
              "captured_at_utc": now(),
              "phase": "M11.E",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "upstream_refs": {"m11d_execution_id": upstream},
              "artifact_keys": {
                  "m11_eval_vs_baseline_report": f"evidence/dev_full/run_control/{exec_id}/m11_eval_vs_baseline_report.json",
                  "m11e_eval_gate_snapshot": f"evidence/dev_full/run_control/{exec_id}/m11e_eval_gate_snapshot.json",
                  "m11e_blocker_register": f"evidence/dev_full/run_control/{exec_id}/m11e_blocker_register.json",
                  "m11e_execution_summary": f"evidence/dev_full/run_control/{exec_id}/m11e_execution_summary.json",
                  "mf_leakage_provenance_check": leakage_prov_key,
              },
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          artifacts = {
              "m11_eval_vs_baseline_report.json": baseline_report,
              "m11_leakage_provenance_check.json": leakage_provenance_check,
              "m11e_eval_gate_snapshot.json": snapshot,
              "m11e_blocker_register.json": blocker_register,
              "m11e_execution_summary.json": summary,
          }
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B5.6", "message": f"Failed to upload {fname}."})

          if leakage_prov_key:
              try:
                  s3_put_json(s3, bucket, leakage_prov_key.lstrip("/"), leakage_provenance_check)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": leakage_prov_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B5.6", "message": "Failed to publish leakage provenance check."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.F_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_F" if overall_pass else "HOLD_REMEDIATE"
          snapshot["upload_errors"] = upload_errors
          snapshot["overall_pass"] = overall_pass
          snapshot["blocker_count"] = len(blockers)
          snapshot["next_gate"] = next_gate
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          summary["overall_pass"] = overall_pass
          summary["blocker_count"] = len(blockers)
          summary["verdict"] = verdict
          summary["next_gate"] = next_gate

          # Rewrite local and durable summaries with final status after upload attempts.
          for fname, payload in {
              "m11e_eval_gate_snapshot.json": snapshot,
              "m11e_blocker_register.json": blocker_register,
              "m11e_execution_summary.json": summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "upstream_m11d_execution": upstream,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "evidence_prefix": f"s3://{bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Execute M11.F (managed)
        if: ${{ inputs.m11_subphase == 'F' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M11E_EXEC: ${{ inputs.upstream_m11e_execution }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import json
          import os
          import re
          import time
          import urllib.error
          import urllib.parse
          import urllib.request
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, object]:
              out: dict[str, object] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      value: object = raw[1:-1]
                  elif raw.lower() == "true":
                      value = True
                  elif raw.lower() == "false":
                      value = False
                  else:
                      try:
                          value = int(raw) if "." not in raw else float(raw)
                      except ValueError:
                          value = raw
                  out[key] = value
              return out

          def is_placeholder(value: object) -> bool:
              s = str(value).strip()
              sl = s.lower()
              if not s:
                  return True
              if sl in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "placeholder" in sl or "to_pin" in sl:
                  return True
              if "<" in s and ">" in s:
                  return True
              return False

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              raw = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(raw)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          def dbx_json(base_url: str, token: str, method: str, path: str, payload: dict[str, Any] | None = None) -> dict[str, Any]:
              url = f"{base_url.rstrip('/')}{path}"
              data = None
              headers = {
                  "Authorization": f"Bearer {token}",
                  "Content-Type": "application/json",
              }
              if payload is not None:
                  data = json.dumps(payload).encode("utf-8")
              req = urllib.request.Request(url=url, data=data, headers=headers, method=method.upper())
              with urllib.request.urlopen(req, timeout=60) as resp:
                  body = resp.read().decode("utf-8")
                  return json.loads(body) if body.strip() else {}

          def dbx_mlflow_json(base_url: str, token: str, method: str, path: str, payload: dict[str, Any] | None = None) -> dict[str, Any]:
              candidates = [path]
              if path.startswith("/api/2.0/mlflow/"):
                  candidates.append(path.replace("/api/2.0/mlflow/", "/api/2.0/preview/mlflow/", 1))
              last_404: Exception | None = None
              for p in candidates:
                  try:
                      return dbx_json(base_url=base_url, token=token, method=method, path=p, payload=payload)
                  except urllib.error.HTTPError as exc:
                      if exc.code == 404:
                          last_404 = exc
                          continue
                      raise
              if last_404 is not None:
                  raise last_404
              raise RuntimeError("mlflow_endpoint_resolution_failed")

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          bucket = os.environ["EVIDENCE_BUCKET"].strip()
          upstream = os.environ["UPSTREAM_M11E_EXEC"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"

          if not upstream:
              raise SystemExit("UPSTREAM_M11E_EXEC is required for M11.F.")

          s3 = boto3.client("s3", region_name=region)
          ssm = boto3.client("ssm", region_name=region)
          handles = parse_handles(HANDLES_PATH)
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []
          notes: list[str] = []

          required_handles = [
              "MLFLOW_HOSTING_MODE",
              "MLFLOW_EXPERIMENT_PATH",
              "MLFLOW_MODEL_NAME",
              "SSM_MLFLOW_TRACKING_URI_PATH",
              "SSM_DATABRICKS_WORKSPACE_URL_PATH",
              "SSM_DATABRICKS_TOKEN_PATH",
          ]
          missing = [k for k in required_handles if k not in handles]
          placeholders = [k for k in required_handles if k in handles and is_placeholder(handles.get(k))]
          if missing or placeholders:
              blockers.append({"code": "M11-B6.1", "message": "Required lineage handles unresolved: " + ",".join(sorted(missing + placeholders))})

          m11e_summary_key = f"evidence/dev_full/run_control/{upstream}/m11e_execution_summary.json"
          m11e_snapshot_key = f"evidence/dev_full/run_control/{upstream}/m11e_eval_gate_snapshot.json"
          m11e_summary: dict[str, Any] | None = None
          m11e_snapshot: dict[str, Any] | None = None
          try:
              m11e_summary = s3_get_json(s3, bucket, m11e_summary_key)
              m11e_snapshot = s3_get_json(s3, bucket, m11e_snapshot_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": "m11e_summary_or_snapshot", "error": type(exc).__name__})
              blockers.append({"code": "M11-B6.2", "message": "M11.E evidence unreadable."})

          platform_run_id = ""
          scenario_run_id = ""
          upstream_m11d = ""
          eval_report_key = ""
          leakage_prov_key = ""
          m11d_summary_key = ""
          m11d_snapshot_key = ""
          m11d_summary: dict[str, Any] | None = None
          m11d_snapshot: dict[str, Any] | None = None
          eval_report: dict[str, Any] | None = None
          leakage_prov: dict[str, Any] | None = None

          if m11e_summary and m11e_snapshot:
              if not bool(m11e_summary.get("overall_pass")):
                  blockers.append({"code": "M11-B6.2", "message": "M11.E summary not pass posture."})
              if str(m11e_summary.get("next_gate", "")).strip() != "M11.F_READY":
                  blockers.append({"code": "M11-B6.2", "message": "M11.E next_gate is not M11.F_READY."})
              platform_run_id = str(m11e_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m11e_summary.get("scenario_run_id", "")).strip()
              upstream_m11d = str(m11e_summary.get("upstream_refs", {}).get("m11d_execution_id", "")).strip()
              eval_report_key = str(m11e_snapshot.get("source_refs", {}).get("mf_eval_report_key", "")).strip()
              leakage_prov_key = str(m11e_snapshot.get("source_refs", {}).get("mf_leakage_provenance_check_key", "")).strip()
              if not platform_run_id or not scenario_run_id or not upstream_m11d:
                  blockers.append({"code": "M11-B6.2", "message": "M11.E run scope/upstream refs incomplete."})
              if not eval_report_key or not leakage_prov_key:
                  blockers.append({"code": "M11-B6.2", "message": "M11.E source refs for eval/provenance missing."})

          if upstream_m11d:
              m11d_summary_key = f"evidence/dev_full/run_control/{upstream_m11d}/m11d_execution_summary.json"
              m11d_snapshot_key = f"evidence/dev_full/run_control/{upstream_m11d}/m11d_train_eval_execution_snapshot.json"
              try:
                  m11d_summary = s3_get_json(s3, bucket, m11d_summary_key)
                  m11d_snapshot = s3_get_json(s3, bucket, m11d_snapshot_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": "m11d_summary_or_snapshot", "error": type(exc).__name__})
                  blockers.append({"code": "M11-B6.2", "message": "M11.D evidence unreadable from M11.E refs."})
              if m11d_summary and str(m11d_summary.get("next_gate", "")).strip() != "M11.E_READY":
                  blockers.append({"code": "M11-B6.2", "message": "M11.D next_gate is not M11.E_READY."})

          if eval_report_key:
              try:
                  eval_report = s3_get_json(s3, bucket, eval_report_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": eval_report_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B6.2", "message": "MF eval report unreadable."})
          if leakage_prov_key:
              try:
                  leakage_prov = s3_get_json(s3, bucket, leakage_prov_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": leakage_prov_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B6.2", "message": "MF leakage provenance report unreadable."})

          m10g_fingerprint_ref = ""
          if m11d_snapshot:
              m10g_fingerprint_ref = str(m11d_snapshot.get("input_refs", {}).get("m10g_fingerprint_ref", "")).strip()
              if not m10g_fingerprint_ref:
                  blockers.append({"code": "M11-B6.5", "message": "m10g_fingerprint_ref missing from M11.D lineage input refs."})

          tracking_uri = ""
          workspace_url = ""
          dbx_token = ""
          tracking_uri_path = str(handles.get("SSM_MLFLOW_TRACKING_URI_PATH", "")).strip()
          workspace_path = str(handles.get("SSM_DATABRICKS_WORKSPACE_URL_PATH", "")).strip()
          token_path = str(handles.get("SSM_DATABRICKS_TOKEN_PATH", "")).strip()
          try:
              if tracking_uri_path:
                  tracking_uri = ssm.get_parameter(Name=tracking_uri_path)["Parameter"]["Value"]
              if workspace_path:
                  workspace_url = ssm.get_parameter(Name=workspace_path)["Parameter"]["Value"]
              if token_path:
                  dbx_token = ssm.get_parameter(Name=token_path, WithDecryption=True)["Parameter"]["Value"]
          except (BotoCoreError, ClientError) as exc:
              read_errors.append({"surface": "ssm_mlflow_databricks", "error": type(exc).__name__})
              blockers.append({"code": "M11-B6.3", "message": "Failed to resolve MLflow/Databricks secret surfaces from SSM."})

          if not str(tracking_uri).strip() or not str(workspace_url).strip() or not str(dbx_token).strip():
              blockers.append({"code": "M11-B6.3", "message": "MLflow tracking URI or Databricks workspace/token is empty."})

          if str(handles.get("MLFLOW_HOSTING_MODE", "")).strip() != "databricks_managed":
              blockers.append({"code": "M11-B6.1", "message": "MLFLOW_HOSTING_MODE is not databricks_managed."})

          mlflow_experiment_path = str(handles.get("MLFLOW_EXPERIMENT_PATH", "")).strip()
          mlflow_model_name = str(handles.get("MLFLOW_MODEL_NAME", "")).strip()
          experiment_id = ""
          run_id = ""
          run_status = ""
          run_tags_present: list[str] = []
          run_tags_missing: list[str] = []
          api_error = ""
          logged_metrics: dict[str, float] = {}

          if not blockers:
              try:
                  def find_experiment_id(name: str) -> str:
                      page_token = ""
                      for _ in range(50):
                          payload: dict[str, Any] = {"max_results": 200}
                          if page_token:
                              payload["page_token"] = page_token
                          search_resp = dbx_mlflow_json(
                              base_url=workspace_url,
                              token=dbx_token,
                              method="POST",
                              path="/api/2.0/mlflow/experiments/search",
                              payload=payload,
                          )
                          for row in search_resp.get("experiments", []):
                              if not isinstance(row, dict):
                                  continue
                              if str(row.get("name", "")).strip() == name:
                                  return str(row.get("experiment_id", "")).strip()
                          page_token = str(search_resp.get("next_page_token", "")).strip()
                          if not page_token:
                              break
                      return ""
                  exp_name_q = urllib.parse.quote(mlflow_experiment_path, safe="")
                  local_experiment_id = ""
                  try:
                      exp_resp = dbx_mlflow_json(
                          base_url=workspace_url,
                          token=dbx_token,
                          method="GET",
                          path=f"/api/2.0/mlflow/experiments/get-by-name?experiment_name={exp_name_q}",
                      )
                  except urllib.error.HTTPError as exc:
                      if exc.code == 404:
                          exp_resp = {}
                      else:
                          raise
                  experiment = exp_resp.get("experiment")
                  if isinstance(experiment, dict):
                      local_experiment_id = str(experiment.get("experiment_id", "")).strip()
                  if not local_experiment_id:
                      local_experiment_id = find_experiment_id(mlflow_experiment_path)
                  if not local_experiment_id:
                      try:
                          create_resp = dbx_mlflow_json(
                              base_url=workspace_url,
                              token=dbx_token,
                              method="POST",
                              path="/api/2.0/mlflow/experiments/create",
                              payload={"name": mlflow_experiment_path},
                          )
                          local_experiment_id = str(create_resp.get("experiment_id", "")).strip()
                      except Exception:
                          local_experiment_id = find_experiment_id(mlflow_experiment_path)
                  if not local_experiment_id:
                      raise RuntimeError("experiment_id_missing")
                  experiment_id = local_experiment_id

                  run_name = f"{platform_run_id}:{exec_id}"
                  create_run = dbx_mlflow_json(
                      base_url=workspace_url,
                      token=dbx_token,
                      method="POST",
                      path="/api/2.0/mlflow/runs/create",
                      payload={
                          "experiment_id": experiment_id,
                          "start_time": int(time.time() * 1000),
                          "tags": [
                              {"key": "mlflow.runName", "value": run_name},
                              {"key": "platform_run_id", "value": platform_run_id},
                              {"key": "scenario_run_id", "value": scenario_run_id},
                              {"key": "m11d_execution_id", "value": upstream_m11d},
                              {"key": "m11e_execution_id", "value": upstream},
                              {"key": "m11f_execution_id", "value": exec_id},
                              {"key": "m10g_fingerprint_ref", "value": m10g_fingerprint_ref},
                              {"key": "leakage_provenance_ref", "value": f"s3://{bucket}/{leakage_prov_key}"},
                              {"key": "mf_model_name", "value": mlflow_model_name},
                              {"key": "mlflow_experiment_path", "value": mlflow_experiment_path},
                          ],
                      },
                  )
                  run = create_run.get("run", {})
                  info = run.get("info", {}) if isinstance(run, dict) else {}
                  run_id = str(info.get("run_id", "")).strip()
                  if not run_id:
                      raise RuntimeError("run_id_missing")

                  metrics_src = eval_report.get("metrics", {}) if isinstance(eval_report, dict) else {}
                  if not isinstance(metrics_src, dict):
                      metrics_src = {}
                  ts_ms = int(time.time() * 1000)
                  metric_payload = []
                  for mk in ("accuracy", "precision", "recall"):
                      raw = metrics_src.get(mk)
                      if raw is None:
                          continue
                      try:
                          val = float(raw)
                      except Exception:
                          continue
                      logged_metrics[mk] = val
                      metric_payload.append({"key": mk, "value": val, "timestamp": ts_ms, "step": 0})

                  dbx_mlflow_json(
                      base_url=workspace_url,
                      token=dbx_token,
                      method="POST",
                      path="/api/2.0/mlflow/runs/log-batch",
                      payload={"run_id": run_id, "metrics": metric_payload, "params": [], "tags": []},
                  )

                  dbx_mlflow_json(
                      base_url=workspace_url,
                      token=dbx_token,
                      method="POST",
                      path="/api/2.0/mlflow/runs/update",
                      payload={"run_id": run_id, "status": "FINISHED", "end_time": int(time.time() * 1000)},
                  )

                  run_get = dbx_mlflow_json(
                      base_url=workspace_url,
                      token=dbx_token,
                      method="GET",
                      path=f"/api/2.0/mlflow/runs/get?run_id={urllib.parse.quote(run_id, safe='')}",
                  )
                  run_obj = run_get.get("run", {}) if isinstance(run_get, dict) else {}
                  run_info = run_obj.get("info", {}) if isinstance(run_obj, dict) else {}
                  run_status = str(run_info.get("status", "")).strip()
                  tag_rows = run_obj.get("data", {}).get("tags", []) if isinstance(run_obj.get("data", {}), dict) else []
                  tag_map: dict[str, str] = {}
                  if isinstance(tag_rows, list):
                      for row in tag_rows:
                          if isinstance(row, dict):
                              k = str(row.get("key", "")).strip()
                              v = str(row.get("value", "")).strip()
                              if k:
                                  tag_map[k] = v
                  required_tag_keys = [
                      "platform_run_id",
                      "scenario_run_id",
                      "m11d_execution_id",
                      "m11e_execution_id",
                      "m11f_execution_id",
                      "m10g_fingerprint_ref",
                      "leakage_provenance_ref",
                      "mf_model_name",
                  ]
                  run_tags_present = sorted([k for k in required_tag_keys if k in tag_map and tag_map[k]])
                  run_tags_missing = sorted([k for k in required_tag_keys if k not in tag_map or not tag_map[k]])
                  if run_tags_missing:
                      blockers.append({"code": "M11-B6.5", "message": "Required MLflow lineage tags missing: " + ",".join(run_tags_missing)})
                  if run_status and run_status.upper() not in {"FINISHED", "SCHEDULED", "RUNNING"}:
                      blockers.append({"code": "M11-B6.4", "message": f"MLflow run ended in unexpected status: {run_status}"})
              except Exception as exc:
                  api_error = f"{type(exc).__name__}:{exc}"
                  blockers.append({"code": "M11-B6.4", "message": "Managed MLflow lineage commit failed."})

          if platform_run_id and scenario_run_id and eval_report and leakage_prov:
              eval_scope_ok = (
                  str(eval_report.get("platform_run_id", "")).strip() == platform_run_id
                  and str(eval_report.get("scenario_run_id", "")).strip() == scenario_run_id
              )
              leakage_scope_ok = (
                  str(leakage_prov.get("platform_run_id", "")).strip() == platform_run_id
                  and str(leakage_prov.get("scenario_run_id", "")).strip() == scenario_run_id
              )
              if not eval_scope_ok or not leakage_scope_ok:
                  blockers.append({"code": "M11-B6.5", "message": "Run-scope mismatch across eval/leakage lineage refs."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.G_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_G" if overall_pass else "HOLD_REMEDIATE"

          lineage_snapshot = {
              "captured_at_utc": now(),
              "phase": "M11.F",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_refs": {
                  "m11e_execution_id": upstream,
                  "m11d_execution_id": upstream_m11d,
              },
              "handles": {
                  "MLFLOW_HOSTING_MODE": str(handles.get("MLFLOW_HOSTING_MODE", "")),
                  "MLFLOW_EXPERIMENT_PATH": mlflow_experiment_path,
                  "MLFLOW_MODEL_NAME": mlflow_model_name,
                  "SSM_MLFLOW_TRACKING_URI_PATH": tracking_uri_path,
                  "SSM_DATABRICKS_WORKSPACE_URL_PATH": workspace_path,
                  "SSM_DATABRICKS_TOKEN_PATH": token_path,
              },
              "mlflow": {
                  "tracking_uri": str(tracking_uri).strip(),
                  "workspace_url": str(workspace_url).strip(),
                  "experiment_path": mlflow_experiment_path,
                  "experiment_id": experiment_id,
                  "run_id": run_id,
                  "run_status": run_status,
                  "logged_metrics": logged_metrics,
                  "required_tags_present": run_tags_present,
                  "required_tags_missing": run_tags_missing,
                  "api_error": api_error,
              },
              "source_refs": {
                  "m11e_summary_key": m11e_summary_key,
                  "m11e_snapshot_key": m11e_snapshot_key,
                  "m11d_summary_key": m11d_summary_key,
                  "m11d_snapshot_key": m11d_snapshot_key,
                  "mf_eval_report_key": eval_report_key,
                  "mf_leakage_provenance_check_key": leakage_prov_key,
                  "m10g_fingerprint_ref": m10g_fingerprint_ref,
              },
              "read_errors": read_errors,
              "upload_errors": upload_errors,
              "notes": notes,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M11.F",
              "phase_id": "P14",
              "execution_id": exec_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "overall_pass": overall_pass,
          }
          summary = {
              "captured_at_utc": now(),
              "phase": "M11.F",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "upstream_refs": {
                  "m11e_execution_id": upstream,
                  "m11d_execution_id": upstream_m11d,
              },
              "artifact_keys": {
                  "m11f_mlflow_lineage_snapshot": f"evidence/dev_full/run_control/{exec_id}/m11f_mlflow_lineage_snapshot.json",
                  "m11f_blocker_register": f"evidence/dev_full/run_control/{exec_id}/m11f_blocker_register.json",
                  "m11f_execution_summary": f"evidence/dev_full/run_control/{exec_id}/m11f_execution_summary.json",
              },
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          artifacts = {
              "m11f_mlflow_lineage_snapshot.json": lineage_snapshot,
              "m11f_blocker_register.json": blocker_register,
              "m11f_execution_summary.json": summary,
          }
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B6.6", "message": f"Failed to upload {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.G_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_G" if overall_pass else "HOLD_REMEDIATE"
          lineage_snapshot["upload_errors"] = upload_errors
          lineage_snapshot["overall_pass"] = overall_pass
          lineage_snapshot["blocker_count"] = len(blockers)
          lineage_snapshot["next_gate"] = next_gate
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          summary["overall_pass"] = overall_pass
          summary["blocker_count"] = len(blockers)
          summary["verdict"] = verdict
          summary["next_gate"] = next_gate

          for fname, payload in {
              "m11f_mlflow_lineage_snapshot.json": lineage_snapshot,
              "m11f_blocker_register.json": blocker_register,
              "m11f_execution_summary.json": summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "upstream_m11e_execution": upstream,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "evidence_prefix": f"s3://{bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Execute M11.G (managed)
        if: ${{ inputs.m11_subphase == 'G' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M11F_EXEC: ${{ inputs.upstream_m11f_execution }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import hashlib
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, object]:
              out: dict[str, object] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      value: object = raw[1:-1]
                  elif raw.lower() == "true":
                      value = True
                  elif raw.lower() == "false":
                      value = False
                  else:
                      try:
                          value = int(raw) if "." not in raw else float(raw)
                      except ValueError:
                          value = raw
                  out[key] = value
              return out

          def is_placeholder(value: object) -> bool:
              s = str(value).strip()
              sl = s.lower()
              if not s:
                  return True
              if sl in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "placeholder" in sl or "to_pin" in sl:
                  return True
              if "<" in s and ">" in s:
                  return True
              return False

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              raw = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(raw)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          def parse_s3_uri(uri: str) -> tuple[str, str]:
              if not uri.startswith("s3://"):
                  raise ValueError("not_s3_uri")
              rest = uri[5:]
              if "/" not in rest:
                  raise ValueError("missing_key")
              bucket, key = rest.split("/", 1)
              if not bucket or not key:
                  raise ValueError("invalid_s3_uri")
              return bucket, key

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          bucket = os.environ["EVIDENCE_BUCKET"].strip()
          upstream = os.environ["UPSTREAM_M11F_EXEC"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          if not upstream:
              raise SystemExit("UPSTREAM_M11F_EXEC is required for M11.G.")

          s3 = boto3.client("s3", region_name=region)
          sm = boto3.client("sagemaker", region_name=region)

          handles = parse_handles(HANDLES_PATH)
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []
          notes: list[str] = []

          required_handles = [
              "MF_CANDIDATE_BUNDLE_PATH_PATTERN",
              "MF_EVAL_REPORT_PATH_PATTERN",
              "MF_LEAKAGE_PROVENANCE_CHECK_PATH_PATTERN",
              "SM_MODEL_PACKAGE_GROUP_NAME",
              "SM_ENDPOINT_NAME",
              "SM_SERVING_MODE",
              "MLFLOW_MODEL_NAME",
          ]
          missing = [k for k in required_handles if k not in handles]
          placeholders = [k for k in required_handles if k in handles and is_placeholder(handles.get(k))]
          if missing or placeholders:
              blockers.append({"code": "M11-B7.1", "message": "Required candidate-bundle handles unresolved: " + ",".join(sorted(missing + placeholders))})

          m11f_summary_key = f"evidence/dev_full/run_control/{upstream}/m11f_execution_summary.json"
          m11f_snapshot_key = f"evidence/dev_full/run_control/{upstream}/m11f_mlflow_lineage_snapshot.json"
          m11f_summary: dict[str, Any] | None = None
          m11f_snapshot: dict[str, Any] | None = None
          try:
              m11f_summary = s3_get_json(s3, bucket, m11f_summary_key)
              m11f_snapshot = s3_get_json(s3, bucket, m11f_snapshot_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": "m11f_summary_or_snapshot", "error": type(exc).__name__})
              blockers.append({"code": "M11-B7.2", "message": "M11.F evidence unreadable."})

          platform_run_id = ""
          scenario_run_id = ""
          upstream_m11d = ""
          upstream_m11e = ""
          eval_report_key = ""
          leakage_prov_key = ""
          m10g_fingerprint_ref = ""
          mlflow_run_id = ""
          mlflow_experiment_id = ""
          mlflow_experiment_path = ""
          mlflow_run_status = ""
          if m11f_summary and m11f_snapshot:
              if not bool(m11f_summary.get("overall_pass")):
                  blockers.append({"code": "M11-B7.2", "message": "M11.F summary not pass posture."})
              if str(m11f_summary.get("next_gate", "")).strip() != "M11.G_READY":
                  blockers.append({"code": "M11-B7.2", "message": "M11.F next_gate is not M11.G_READY."})
              platform_run_id = str(m11f_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m11f_summary.get("scenario_run_id", "")).strip()
              upstream_m11d = str(m11f_summary.get("upstream_refs", {}).get("m11d_execution_id", "")).strip()
              upstream_m11e = str(m11f_summary.get("upstream_refs", {}).get("m11e_execution_id", "")).strip()
              source_refs = m11f_snapshot.get("source_refs", {}) if isinstance(m11f_snapshot.get("source_refs", {}), dict) else {}
              eval_report_key = str(source_refs.get("mf_eval_report_key", "")).strip()
              leakage_prov_key = str(source_refs.get("mf_leakage_provenance_check_key", "")).strip()
              m10g_fingerprint_ref = str(source_refs.get("m10g_fingerprint_ref", "")).strip()
              mlflow_block = m11f_snapshot.get("mlflow", {}) if isinstance(m11f_snapshot.get("mlflow", {}), dict) else {}
              mlflow_run_id = str(mlflow_block.get("run_id", "")).strip()
              mlflow_experiment_id = str(mlflow_block.get("experiment_id", "")).strip()
              mlflow_experiment_path = str(mlflow_block.get("experiment_path", "")).strip()
              mlflow_run_status = str(mlflow_block.get("run_status", "")).strip()
              if not platform_run_id or not scenario_run_id:
                  blockers.append({"code": "M11-B7.2", "message": "M11.F run scope is incomplete."})
              if not upstream_m11d or not upstream_m11e:
                  blockers.append({"code": "M11-B7.2", "message": "M11.F upstream refs incomplete for M11.D/M11.E."})
              if not eval_report_key:
                  eval_report_key = str(handles.get("MF_EVAL_REPORT_PATH_PATTERN", "")).replace("{platform_run_id}", platform_run_id)
              if not leakage_prov_key:
                  leakage_prov_key = str(handles.get("MF_LEAKAGE_PROVENANCE_CHECK_PATH_PATTERN", "")).replace("{platform_run_id}", platform_run_id)

          m11d_summary_key = f"evidence/dev_full/run_control/{upstream_m11d}/m11d_execution_summary.json" if upstream_m11d else ""
          m11d_snapshot_key = f"evidence/dev_full/run_control/{upstream_m11d}/m11d_train_eval_execution_snapshot.json" if upstream_m11d else ""
          m11e_summary_key = f"evidence/dev_full/run_control/{upstream_m11e}/m11e_execution_summary.json" if upstream_m11e else ""
          m11e_snapshot_key = f"evidence/dev_full/run_control/{upstream_m11e}/m11e_eval_gate_snapshot.json" if upstream_m11e else ""

          m11d_summary: dict[str, Any] | None = None
          m11d_snapshot: dict[str, Any] | None = None
          m11e_summary: dict[str, Any] | None = None
          m11e_snapshot: dict[str, Any] | None = None
          eval_report: dict[str, Any] | None = None
          leakage_prov: dict[str, Any] | None = None

          if m11d_summary_key and m11d_snapshot_key:
              try:
                  m11d_summary = s3_get_json(s3, bucket, m11d_summary_key)
                  m11d_snapshot = s3_get_json(s3, bucket, m11d_snapshot_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": "m11d_summary_or_snapshot", "error": type(exc).__name__})
                  blockers.append({"code": "M11-B7.2", "message": "M11.D evidence unreadable from M11.F refs."})
              if m11d_summary and str(m11d_summary.get("next_gate", "")).strip() != "M11.E_READY":
                  blockers.append({"code": "M11-B7.2", "message": "M11.D next_gate is not M11.E_READY."})

          if m11e_summary_key and m11e_snapshot_key:
              try:
                  m11e_summary = s3_get_json(s3, bucket, m11e_summary_key)
                  m11e_snapshot = s3_get_json(s3, bucket, m11e_snapshot_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": "m11e_summary_or_snapshot", "error": type(exc).__name__})
                  blockers.append({"code": "M11-B7.2", "message": "M11.E evidence unreadable from M11.F refs."})
              if m11e_summary and str(m11e_summary.get("next_gate", "")).strip() != "M11.F_READY":
                  blockers.append({"code": "M11-B7.2", "message": "M11.E next_gate is not M11.F_READY."})

          if eval_report_key:
              try:
                  eval_report = s3_get_json(s3, bucket, eval_report_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": eval_report_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B7.2", "message": "MF eval report unreadable."})

          if leakage_prov_key:
              try:
                  leakage_prov = s3_get_json(s3, bucket, leakage_prov_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": leakage_prov_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B7.2", "message": "MF leakage provenance report unreadable."})

          model_artifact_uri = ""
          training_job_name = ""
          training_status = ""
          transform_job_name = ""
          transform_status = ""
          transform_eval_mode = ""
          if m11d_snapshot:
              jobs = m11d_snapshot.get("jobs", {}) if isinstance(m11d_snapshot.get("jobs", {}), dict) else {}
              tr = jobs.get("training", {}) if isinstance(jobs.get("training", {}), dict) else {}
              tf = jobs.get("transform", {}) if isinstance(jobs.get("transform", {}), dict) else {}
              model_artifact_uri = str(tr.get("model_artifact_uri", "")).strip()
              training_job_name = str(tr.get("job_name", "")).strip()
              training_status = str(tr.get("status", "")).strip()
              transform_job_name = str(tf.get("job_name", "")).strip()
              transform_status = str(tf.get("status", "")).strip()
              transform_eval_mode = str(tf.get("eval_mode", "")).strip()
              if not model_artifact_uri:
                  blockers.append({"code": "M11-B7.4", "message": "Model artifact URI missing from M11.D snapshot."})

          package_group_name = str(handles.get("SM_MODEL_PACKAGE_GROUP_NAME", "")).strip()
          endpoint_name = str(handles.get("SM_ENDPOINT_NAME", "")).strip()
          serving_mode = str(handles.get("SM_SERVING_MODE", "")).strip()
          mlflow_model_name = str(handles.get("MLFLOW_MODEL_NAME", "")).strip()
          candidate_pattern = str(handles.get("MF_CANDIDATE_BUNDLE_PATH_PATTERN", "")).strip()
          bundle_basis = "|".join([platform_run_id, scenario_run_id, upstream_m11d, upstream_m11e, upstream, mlflow_run_id])
          bundle_id = hashlib.sha256(bundle_basis.encode("utf-8")).hexdigest()[:16] if bundle_basis else ""
          candidate_bundle_key = candidate_pattern.replace("{platform_run_id}", platform_run_id).replace("{bundle_id}", bundle_id)
          if "{" in candidate_bundle_key or "}" in candidate_bundle_key or not candidate_bundle_key:
              blockers.append({"code": "M11-B7.1", "message": "MF_CANDIDATE_BUNDLE_PATH_PATTERN did not fully resolve."})

          package_group_arn = ""
          package_group_status = "not_checked"
          model_artifact_readable = False
          model_artifact_error = ""
          if model_artifact_uri:
              try:
                  ab, ak = parse_s3_uri(model_artifact_uri)
                  s3.head_object(Bucket=ab, Key=ak)
                  model_artifact_readable = True
              except Exception as exc:
                  model_artifact_error = f"{type(exc).__name__}:{exc}"

          package_group_materialized = False
          package_group_error = ""
          if package_group_name:
              try:
                  desc = sm.describe_model_package_group(ModelPackageGroupName=package_group_name)
                  package_group_arn = str(desc.get("ModelPackageGroupArn", "")).strip()
                  package_group_status = str(desc.get("ModelPackageGroupStatus", "Unknown")).strip()
                  package_group_materialized = True
              except ClientError as exc:
                  code = str(exc.response.get("Error", {}).get("Code", "")).strip()
                  if code in {"ValidationException", "ResourceNotFound", "ResourceNotFoundException"}:
                      try:
                          create_resp = sm.create_model_package_group(
                              ModelPackageGroupName=package_group_name,
                              ModelPackageGroupDescription=f"M11.G candidate group for {platform_run_id}",
                          )
                          package_group_arn = str(create_resp.get("ModelPackageGroupArn", "")).strip()
                          desc = sm.describe_model_package_group(ModelPackageGroupName=package_group_name)
                          package_group_status = str(desc.get("ModelPackageGroupStatus", "Unknown")).strip()
                          if not package_group_arn:
                              package_group_arn = str(desc.get("ModelPackageGroupArn", "")).strip()
                          package_group_materialized = True
                          notes.append("Model package group created during M11.G closure.")
                      except ClientError as inner:
                          package_group_error = f"{type(inner).__name__}:{inner}"
                  else:
                      package_group_error = f"{type(exc).__name__}:{exc}"
          else:
              package_group_error = "empty_package_group_handle"

          lineage_refs_complete = bool(
              m10g_fingerprint_ref
              and eval_report_key
              and leakage_prov_key
              and mlflow_run_id
              and mlflow_experiment_id
              and mlflow_experiment_path
          )

          run_scope_consistent = True
          if eval_report:
              run_scope_consistent = run_scope_consistent and str(eval_report.get("platform_run_id", "")).strip() == platform_run_id
              run_scope_consistent = run_scope_consistent and str(eval_report.get("scenario_run_id", "")).strip() == scenario_run_id
          if leakage_prov:
              run_scope_consistent = run_scope_consistent and str(leakage_prov.get("platform_run_id", "")).strip() == platform_run_id
              run_scope_consistent = run_scope_consistent and str(leakage_prov.get("scenario_run_id", "")).strip() == scenario_run_id

          checks: list[dict[str, Any]] = [
              {"name": "model_artifact_readable", "pass": model_artifact_readable, "detail": model_artifact_uri if model_artifact_readable else model_artifact_error},
              {"name": "train_transform_completed", "pass": training_status == "Completed" and transform_status == "Completed", "detail": f"training={training_status},transform={transform_status},eval_mode={transform_eval_mode}"},
              {"name": "package_group_materialized", "pass": package_group_materialized, "detail": package_group_arn if package_group_materialized else package_group_error},
              {"name": "serving_handles_pinned", "pass": bool(endpoint_name and serving_mode), "detail": f"endpoint={endpoint_name},mode={serving_mode}"},
              {"name": "lineage_refs_complete", "pass": lineage_refs_complete, "detail": f"mlflow_run_id={mlflow_run_id},fingerprint_ref={m10g_fingerprint_ref}"},
              {"name": "run_scope_consistent", "pass": run_scope_consistent, "detail": f"platform_run_id={platform_run_id},scenario_run_id={scenario_run_id}"},
          ]
          operability_pass = all(bool(c.get("pass")) for c in checks)
          if not operability_pass:
              blockers.append({"code": "M11-B7.4", "message": "Model operability checks failed."})

          candidate_bundle = {
              "captured_at_utc": now(),
              "phase": "M11.G",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "bundle_id": bundle_id,
              "bundle_status": "CANDIDATE",
              "model": {
                  "mlflow_model_name": mlflow_model_name,
                  "model_artifact_uri": model_artifact_uri,
                  "training_job_name": training_job_name,
                  "transform_job_name": transform_job_name,
                  "transform_eval_mode": transform_eval_mode,
              },
              "serving": {
                  "endpoint_name": endpoint_name,
                  "serving_mode": serving_mode,
              },
              "package": {
                  "model_package_group_name": package_group_name,
                  "model_package_group_arn": package_group_arn,
                  "model_package_group_status": package_group_status,
              },
              "metrics": {
                  "eval_metrics": (eval_report or {}).get("metrics", {}) if isinstance(eval_report, dict) else {},
                  "eval_gate_results": (m11e_snapshot or {}).get("gate_results", {}) if isinstance(m11e_snapshot, dict) else {},
              },
              "lineage": {
                  "m11d_execution_id": upstream_m11d,
                  "m11e_execution_id": upstream_m11e,
                  "m11f_execution_id": upstream,
                  "mlflow_experiment_path": mlflow_experiment_path,
                  "mlflow_experiment_id": mlflow_experiment_id,
                  "mlflow_run_id": mlflow_run_id,
                  "mlflow_run_status": mlflow_run_status,
                  "m10g_fingerprint_ref": m10g_fingerprint_ref,
                  "mf_eval_report_key": eval_report_key,
                  "mf_leakage_provenance_check_key": leakage_prov_key,
              },
              "rollback_pointers": {
                  "model_artifact_uri": model_artifact_uri,
                  "m11d_snapshot_key": m11d_snapshot_key,
                  "m11e_snapshot_key": m11e_snapshot_key,
                  "m11f_snapshot_key": m11f_snapshot_key,
              },
          }

          if not blockers:
              try:
                  s3_put_json(s3, bucket, candidate_bundle_key, candidate_bundle)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": candidate_bundle_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B7.3", "message": "Failed to publish candidate bundle artifact."})

          operability_report = {
              "captured_at_utc": now(),
              "phase": "M11.G",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "checks": checks,
              "overall_pass": operability_pass,
              "source_refs": {
                  "m11f_summary_key": m11f_summary_key,
                  "m11f_snapshot_key": m11f_snapshot_key,
                  "m11d_summary_key": m11d_summary_key,
                  "m11d_snapshot_key": m11d_snapshot_key,
                  "m11e_summary_key": m11e_summary_key,
                  "m11e_snapshot_key": m11e_snapshot_key,
                  "mf_eval_report_key": eval_report_key,
                  "mf_leakage_provenance_check_key": leakage_prov_key,
                  "candidate_bundle_key": candidate_bundle_key,
              },
          }

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.H_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_H" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": now(),
              "phase": "M11.G",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_refs": {
                  "m11f_execution_id": upstream,
                  "m11e_execution_id": upstream_m11e,
                  "m11d_execution_id": upstream_m11d,
              },
              "handles": {k: str(handles.get(k, "")) for k in required_handles},
              "candidate_bundle": {
                  "bundle_id": bundle_id,
                  "bundle_key": candidate_bundle_key,
                  "bundle_s3_uri": f"s3://{bucket}/{candidate_bundle_key}" if candidate_bundle_key else "",
                  "model_package_group_name": package_group_name,
                  "model_package_group_arn": package_group_arn,
                  "model_package_group_status": package_group_status,
              },
              "operability_report_key": f"evidence/dev_full/run_control/{exec_id}/m11_model_operability_report.json",
              "source_refs": {
                  "m11f_summary_key": m11f_summary_key,
                  "m11f_snapshot_key": m11f_snapshot_key,
                  "m11d_summary_key": m11d_summary_key,
                  "m11d_snapshot_key": m11d_snapshot_key,
                  "m11e_summary_key": m11e_summary_key,
                  "m11e_snapshot_key": m11e_snapshot_key,
                  "mf_eval_report_key": eval_report_key,
                  "mf_leakage_provenance_check_key": leakage_prov_key,
              },
              "read_errors": read_errors,
              "upload_errors": upload_errors,
              "notes": notes,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M11.G",
              "phase_id": "P14",
              "execution_id": exec_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "overall_pass": overall_pass,
          }
          summary = {
              "captured_at_utc": now(),
              "phase": "M11.G",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "upstream_refs": {
                  "m11f_execution_id": upstream,
                  "m11e_execution_id": upstream_m11e,
                  "m11d_execution_id": upstream_m11d,
              },
              "artifact_keys": {
                  "mf_candidate_bundle_key": candidate_bundle_key,
                  "m11g_candidate_bundle_snapshot": f"evidence/dev_full/run_control/{exec_id}/m11g_candidate_bundle_snapshot.json",
                  "m11_model_operability_report": f"evidence/dev_full/run_control/{exec_id}/m11_model_operability_report.json",
                  "m11g_blocker_register": f"evidence/dev_full/run_control/{exec_id}/m11g_blocker_register.json",
                  "m11g_execution_summary": f"evidence/dev_full/run_control/{exec_id}/m11g_execution_summary.json",
              },
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          artifacts = {
              "m11_candidate_bundle.json": candidate_bundle,
              "m11g_candidate_bundle_snapshot.json": snapshot,
              "m11_model_operability_report.json": operability_report,
              "m11g_blocker_register.json": blocker_register,
              "m11g_execution_summary.json": summary,
          }
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              if fname == "m11_candidate_bundle.json":
                  continue
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B7.5", "message": f"Failed to upload {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.H_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_H" if overall_pass else "HOLD_REMEDIATE"
          snapshot["upload_errors"] = upload_errors
          snapshot["overall_pass"] = overall_pass
          snapshot["blocker_count"] = len(blockers)
          snapshot["next_gate"] = next_gate
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          summary["overall_pass"] = overall_pass
          summary["blocker_count"] = len(blockers)
          summary["verdict"] = verdict
          summary["next_gate"] = next_gate
          operability_report["overall_pass"] = operability_pass

          for fname, payload in {
              "m11g_candidate_bundle_snapshot.json": snapshot,
              "m11_model_operability_report.json": operability_report,
              "m11g_blocker_register.json": blocker_register,
              "m11g_execution_summary.json": summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "upstream_m11f_execution": upstream,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "candidate_bundle_s3_uri": f"s3://{bucket}/{candidate_bundle_key}" if candidate_bundle_key else "",
                      "evidence_prefix": f"s3://{bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Execute M11.H (managed)
        if: ${{ inputs.m11_subphase == 'H' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M11G_EXEC: ${{ inputs.upstream_m11g_execution }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import hashlib
          import json
          import os
          import re
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, object]:
              out: dict[str, object] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      value: object = raw[1:-1]
                  elif raw.lower() == "true":
                      value = True
                  elif raw.lower() == "false":
                      value = False
                  else:
                      try:
                          value = int(raw) if "." not in raw else float(raw)
                      except ValueError:
                          value = raw
                  out[key] = value
              return out

          def is_placeholder(value: object) -> bool:
              s = str(value).strip()
              sl = s.lower()
              if not s:
                  return True
              if sl in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "placeholder" in sl or "to_pin" in sl:
                  return True
              if "<" in s and ">" in s:
                  return True
              return False

          def s3_get_bytes(s3: Any, bucket: str, key: str) -> bytes:
              return s3.get_object(Bucket=bucket, Key=key)["Body"].read()

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              payload = json.loads(s3_get_bytes(s3, bucket, key).decode("utf-8"))
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          bucket = os.environ["EVIDENCE_BUCKET"].strip()
          upstream = os.environ["UPSTREAM_M11G_EXEC"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          if not upstream:
              raise SystemExit("UPSTREAM_M11G_EXEC is required for M11.H.")

          s3 = boto3.client("s3", region_name=region)
          handles = parse_handles(HANDLES_PATH)

          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []
          notes: list[str] = []

          required_handles = [
              "MPR_ROLLBACK_DRILL_PATH_PATTERN",
              "MF_CANDIDATE_BUNDLE_PATH_PATTERN",
              "SM_ENDPOINT_NAME",
              "SM_SERVING_MODE",
          ]
          missing = [k for k in required_handles if k not in handles]
          placeholders = [k for k in required_handles if k in handles and is_placeholder(handles.get(k))]
          if missing or placeholders:
              blockers.append({"code": "M11-B8.1", "message": "Required M11.H handles unresolved: " + ",".join(sorted(missing + placeholders))})

          m11g_summary_key = f"evidence/dev_full/run_control/{upstream}/m11g_execution_summary.json"
          m11g_snapshot_key = f"evidence/dev_full/run_control/{upstream}/m11g_candidate_bundle_snapshot.json"
          m11g_operability_key = f"evidence/dev_full/run_control/{upstream}/m11_model_operability_report.json"

          m11g_summary: dict[str, Any] | None = None
          m11g_snapshot: dict[str, Any] | None = None
          m11g_operability: dict[str, Any] | None = None

          try:
              m11g_summary = s3_get_json(s3, bucket, m11g_summary_key)
              m11g_snapshot = s3_get_json(s3, bucket, m11g_snapshot_key)
              m11g_operability = s3_get_json(s3, bucket, m11g_operability_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": "m11g_summary_snapshot_operability", "error": type(exc).__name__})
              blockers.append({"code": "M11-B8.2", "message": "M11.G upstream evidence unreadable."})

          platform_run_id = ""
          scenario_run_id = ""
          candidate_bundle_key = ""
          candidate_bundle: dict[str, Any] | None = None
          candidate_bundle_sha256_a = ""
          candidate_bundle_sha256_b = ""
          reproducibility_pass = False
          run_scope_consistent = False
          lineage_refs_complete = False
          rollback_recipe_key = ""
          rollback_report_key = ""
          mpr_rollback_key = ""

          if m11g_summary and m11g_snapshot and m11g_operability:
              if not bool(m11g_summary.get("overall_pass")):
                  blockers.append({"code": "M11-B8.2", "message": "M11.G summary is not pass posture."})
              if str(m11g_summary.get("next_gate", "")).strip() != "M11.H_READY":
                  blockers.append({"code": "M11-B8.2", "message": "M11.G next_gate is not M11.H_READY."})
              if int(m11g_summary.get("blocker_count", 0)) != 0:
                  blockers.append({"code": "M11-B8.2", "message": "M11.G blocker_count is not zero."})
              if not bool(m11g_operability.get("overall_pass")):
                  blockers.append({"code": "M11-B8.2", "message": "M11.G operability report is not pass posture."})

              platform_run_id = str(m11g_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m11g_summary.get("scenario_run_id", "")).strip()
              if not platform_run_id or not scenario_run_id:
                  blockers.append({"code": "M11-B8.2", "message": "M11.G run scope missing for platform/scenario ids."})

              artifact_keys = m11g_summary.get("artifact_keys", {})
              if isinstance(artifact_keys, dict):
                  candidate_bundle_key = str(artifact_keys.get("mf_candidate_bundle_key", "")).strip()
              if not candidate_bundle_key:
                  snap_bundle = m11g_snapshot.get("candidate_bundle", {})
                  if isinstance(snap_bundle, dict):
                      candidate_bundle_key = str(snap_bundle.get("bundle_key", "")).strip()
              if not candidate_bundle_key:
                  blockers.append({"code": "M11-B8.2", "message": "Candidate bundle key missing from M11.G evidence."})

              if candidate_bundle_key:
                  try:
                      raw_a = s3_get_bytes(s3, bucket, candidate_bundle_key)
                      raw_b = s3_get_bytes(s3, bucket, candidate_bundle_key)
                      candidate_bundle_sha256_a = hashlib.sha256(raw_a).hexdigest()
                      candidate_bundle_sha256_b = hashlib.sha256(raw_b).hexdigest()
                      parsed = json.loads(raw_a.decode("utf-8"))
                      if not isinstance(parsed, dict):
                          raise ValueError("candidate_bundle_not_object")
                      candidate_bundle = parsed
                  except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                      read_errors.append({"surface": candidate_bundle_key, "error": type(exc).__name__})
                      blockers.append({"code": "M11-B8.2", "message": "Candidate bundle artifact unreadable."})

          if platform_run_id:
              rollback_recipe_key = f"evidence/runs/{platform_run_id}/learning/ofs/rollback_recipe.json"
              rollback_report_key = f"evidence/runs/{platform_run_id}/learning/ofs/rollback_drill_report.json"
              try:
                  s3_get_json(s3, bucket, rollback_recipe_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": rollback_recipe_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B8.3", "message": "Rollback recipe artifact unreadable."})
              try:
                  s3_get_json(s3, bucket, rollback_report_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": rollback_report_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B8.3", "message": "Rollback drill report artifact unreadable."})

          mpr_pattern = str(handles.get("MPR_ROLLBACK_DRILL_PATH_PATTERN", "")).strip()
          if mpr_pattern and platform_run_id:
              mpr_rollback_key = mpr_pattern.replace("{platform_run_id}", platform_run_id)
          if not mpr_rollback_key or "{" in mpr_rollback_key or "}" in mpr_rollback_key:
              blockers.append({"code": "M11-B8.1", "message": "MPR_ROLLBACK_DRILL_PATH_PATTERN did not fully resolve."})

          if candidate_bundle:
              run_scope_consistent = (
                  str(candidate_bundle.get("platform_run_id", "")).strip() == platform_run_id
                  and str(candidate_bundle.get("scenario_run_id", "")).strip() == scenario_run_id
              )
              lineage = candidate_bundle.get("lineage", {})
              if isinstance(lineage, dict):
                  lineage_refs_complete = bool(
                      str(lineage.get("m11d_execution_id", "")).strip()
                      and str(lineage.get("m11e_execution_id", "")).strip()
                      and str(lineage.get("m11f_execution_id", "")).strip()
                      and str(lineage.get("mlflow_run_id", "")).strip()
                  )
              reproducibility_pass = bool(
                  candidate_bundle_sha256_a
                  and candidate_bundle_sha256_b
                  and candidate_bundle_sha256_a == candidate_bundle_sha256_b
                  and run_scope_consistent
                  and lineage_refs_complete
              )
              if not reproducibility_pass:
                  blockers.append({"code": "M11-B8.4", "message": "Reproducibility checks failed for candidate bundle."})

          reproducibility_check = {
              "captured_at_utc": now(),
              "phase": "M11.H",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_refs": {
                  "m11g_execution_id": upstream,
              },
              "candidate_bundle_key": candidate_bundle_key,
              "checks": [
                  {
                      "name": "candidate_bundle_hash_stable",
                      "pass": bool(candidate_bundle_sha256_a and candidate_bundle_sha256_b and candidate_bundle_sha256_a == candidate_bundle_sha256_b),
                      "detail": f"sha256_a={candidate_bundle_sha256_a},sha256_b={candidate_bundle_sha256_b}",
                  },
                  {
                      "name": "run_scope_consistent",
                      "pass": run_scope_consistent,
                      "detail": f"platform_run_id={platform_run_id},scenario_run_id={scenario_run_id}",
                  },
                  {
                      "name": "lineage_refs_complete",
                      "pass": lineage_refs_complete,
                      "detail": "required lineage refs: m11d,m11e,m11f,mlflow_run_id",
                  },
              ],
              "overall_pass": reproducibility_pass,
          }

          safe_disable_snapshot = {
              "captured_at_utc": now(),
              "phase": "M11.H",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "safe_disable_controls": {
                  "endpoint_name": str(handles.get("SM_ENDPOINT_NAME", "")).strip(),
                  "serving_mode": str(handles.get("SM_SERVING_MODE", "")).strip(),
                  "candidate_bundle_promotion_allowed": False,
                  "rollback_path_type": "run_scoped_mpr_artifact",
              },
              "source_refs": {
                  "m11g_summary_key": m11g_summary_key,
                  "m11g_snapshot_key": m11g_snapshot_key,
                  "m11g_operability_key": m11g_operability_key,
                  "candidate_bundle_key": candidate_bundle_key,
                  "ofs_rollback_recipe_key": rollback_recipe_key,
                  "ofs_rollback_drill_report_key": rollback_report_key,
              },
              "target_refs": {
                  "mpr_rollback_drill_key": mpr_rollback_key,
              },
          }

          if not blockers and mpr_rollback_key:
              mpr_rollback_payload = {
                  "captured_at_utc": now(),
                  "phase": "M11.H",
                  "phase_id": "P14",
                  "execution_id": exec_id,
                  "platform_run_id": platform_run_id,
                  "scenario_run_id": scenario_run_id,
                  "status": "SAFE_DISABLE_ROLLBACK_READY",
                  "candidate_bundle_key": candidate_bundle_key,
                  "source_refs": {
                      "ofs_rollback_recipe_key": rollback_recipe_key,
                      "ofs_rollback_drill_report_key": rollback_report_key,
                      "m11g_execution_id": upstream,
                  },
              }
              try:
                  s3_put_json(s3, bucket, mpr_rollback_key, mpr_rollback_payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": mpr_rollback_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B8.5", "message": "Failed to publish run-scoped MPR rollback drill artifact."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.I_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_I" if overall_pass else "HOLD_REMEDIATE"

          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M11.H",
              "phase_id": "P14",
              "execution_id": exec_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "overall_pass": overall_pass,
          }
          execution_summary = {
              "captured_at_utc": now(),
              "phase": "M11.H",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "upstream_refs": {
                  "m11g_execution_id": upstream,
              },
              "artifact_keys": {
                  "m11_reproducibility_check": f"evidence/dev_full/run_control/{exec_id}/m11_reproducibility_check.json",
                  "m11h_safe_disable_rollback_snapshot": f"evidence/dev_full/run_control/{exec_id}/m11h_safe_disable_rollback_snapshot.json",
                  "m11h_blocker_register": f"evidence/dev_full/run_control/{exec_id}/m11h_blocker_register.json",
                  "m11h_execution_summary": f"evidence/dev_full/run_control/{exec_id}/m11h_execution_summary.json",
                  "mpr_rollback_drill_key": mpr_rollback_key,
              },
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          artifacts = {
              "m11_reproducibility_check.json": reproducibility_check,
              "m11h_safe_disable_rollback_snapshot.json": safe_disable_snapshot,
              "m11h_blocker_register.json": blocker_register,
              "m11h_execution_summary.json": execution_summary,
          }
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B8.5", "message": f"Failed to upload {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.I_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_I" if overall_pass else "HOLD_REMEDIATE"
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          execution_summary["overall_pass"] = overall_pass
          execution_summary["blocker_count"] = len(blockers)
          execution_summary["next_gate"] = next_gate
          execution_summary["verdict"] = verdict
          reproducibility_check["overall_pass"] = reproducibility_pass
          safe_disable_snapshot["overall_pass"] = overall_pass
          safe_disable_snapshot["next_gate"] = next_gate
          safe_disable_snapshot["read_errors"] = read_errors
          safe_disable_snapshot["upload_errors"] = upload_errors
          safe_disable_snapshot["notes"] = notes

          for fname, payload in {
              "m11_reproducibility_check.json": reproducibility_check,
              "m11h_safe_disable_rollback_snapshot.json": safe_disable_snapshot,
              "m11h_blocker_register.json": blocker_register,
              "m11h_execution_summary.json": execution_summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "upstream_m11g_execution": upstream,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "mpr_rollback_drill_s3_uri": f"s3://{bucket}/{mpr_rollback_key}" if mpr_rollback_key else "",
                      "evidence_prefix": f"s3://{bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Execute M11.I (managed)
        if: ${{ inputs.m11_subphase == 'I' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M11H_EXEC: ${{ inputs.upstream_m11h_execution }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import json
          import os
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              raw = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(raw)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          def as_int(v: Any, default: int = -1) -> int:
              try:
                  return int(str(v).strip())
              except Exception:
                  return default

          def truthy(v: Any) -> bool:
              if isinstance(v, bool):
                  return v
              if isinstance(v, str):
                  return v.strip().lower() == "true"
              return bool(v)

          def summary_key(exec_id: str, phase_letter: str) -> str:
              return f"evidence/dev_full/run_control/{exec_id}/m11{phase_letter.lower()}_execution_summary.json"

          def to_s3_uri(bucket: str, key_or_uri: str) -> str:
              s = str(key_or_uri or "").strip()
              if not s:
                  return ""
              if s.startswith("s3://"):
                  return s
              return f"s3://{bucket}/{s}"

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          bucket = os.environ["EVIDENCE_BUCKET"].strip()
          upstream_h = os.environ["UPSTREAM_M11H_EXEC"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"
          if not upstream_h:
              raise SystemExit("UPSTREAM_M11H_EXEC is required for M11.I.")

          s3 = boto3.client("s3", region_name=region)
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []
          notes: list[str] = []

          chain_ids: dict[str, str] = {k: "" for k in "ABCDEFGH"}
          summaries: dict[str, dict[str, Any]] = {}

          def load_summary(letter: str, execution_id: str) -> dict[str, Any] | None:
              if not execution_id:
                  blockers.append({"code": "M11-B9", "message": f"M11.{letter} execution id is missing."})
                  return None
              key = summary_key(execution_id, letter)
              try:
                  payload = s3_get_json(s3, bucket, key)
                  summaries[letter] = payload
                  return payload
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B9", "message": f"M11.{letter} summary is unreadable."})
                  return None

          chain_ids["H"] = upstream_h
          h = load_summary("H", chain_ids["H"])
          if h:
              up_h = h.get("upstream_refs", {}) if isinstance(h.get("upstream_refs", {}), dict) else {}
              chain_ids["G"] = str(up_h.get("m11g_execution_id", "")).strip()

          g = load_summary("G", chain_ids["G"])
          if g:
              up_g = g.get("upstream_refs", {}) if isinstance(g.get("upstream_refs", {}), dict) else {}
              chain_ids["F"] = str(up_g.get("m11f_execution_id", "")).strip()
              chain_ids["E"] = str(up_g.get("m11e_execution_id", "")).strip()
              chain_ids["D"] = str(up_g.get("m11d_execution_id", "")).strip()

          f = load_summary("F", chain_ids["F"])
          e = load_summary("E", chain_ids["E"])
          d = load_summary("D", chain_ids["D"])
          if d:
              up_d = d.get("upstream_refs", {}) if isinstance(d.get("upstream_refs", {}), dict) else {}
              chain_ids["C"] = str(up_d.get("m11c_execution_id", "")).strip()
              chain_ids["B"] = str(up_d.get("m11b_execution_id", "")).strip()
              chain_ids["A"] = str(up_d.get("m11a_execution_id", "")).strip()

          a = load_summary("A", chain_ids["A"])
          b = load_summary("B", chain_ids["B"])
          c = load_summary("C", chain_ids["C"])

          # Chain coherence checks across summary refs.
          if f:
              up_f = f.get("upstream_refs", {}) if isinstance(f.get("upstream_refs", {}), dict) else {}
              if str(up_f.get("m11e_execution_id", "")).strip() != chain_ids["E"] or str(up_f.get("m11d_execution_id", "")).strip() != chain_ids["D"]:
                  blockers.append({"code": "M11-B9", "message": "M11.F upstream refs do not align with M11.E/M11.D chain ids."})
          if e:
              up_e = e.get("upstream_refs", {}) if isinstance(e.get("upstream_refs", {}), dict) else {}
              if str(up_e.get("m11d_execution_id", "")).strip() != chain_ids["D"]:
                  blockers.append({"code": "M11-B9", "message": "M11.E upstream ref does not align with M11.D chain id."})
          if d:
              up_d = d.get("upstream_refs", {}) if isinstance(d.get("upstream_refs", {}), dict) else {}
              if (
                  str(up_d.get("m11c_execution_id", "")).strip() != chain_ids["C"]
                  or str(up_d.get("m11b_execution_id", "")).strip() != chain_ids["B"]
                  or str(up_d.get("m11a_execution_id", "")).strip() != chain_ids["A"]
              ):
                  blockers.append({"code": "M11-B9", "message": "M11.D upstream refs do not align with M11.C/M11.B/M11.A chain ids."})

          expected_next = {
              "A": "M11.B_READY",
              "B": "M11.C_READY",
              "C": "M11.D_READY",
              "D": "M11.E_READY",
              "E": "M11.F_READY",
              "F": "M11.G_READY",
              "G": "M11.H_READY",
              "H": "M11.I_READY",
          }

          platform_run_id = ""
          scenario_run_id = ""
          if h:
              platform_run_id = str(h.get("platform_run_id", "")).strip()
              scenario_run_id = str(h.get("scenario_run_id", "")).strip()
          if not platform_run_id or not scenario_run_id:
              blockers.append({"code": "M11-B9", "message": "M11.H run scope is incomplete."})

          rollup_rows: list[dict[str, Any]] = []
          for letter in "ABCDEFGH":
              s = summaries.get(letter)
              phase_name = f"M11.{letter}"
              expected_gate = expected_next[letter]
              if not s:
                  rollup_rows.append(
                      {
                          "phase": phase_name,
                          "execution_id": chain_ids.get(letter, ""),
                          "row_pass": False,
                          "expected_next_gate": expected_gate,
                          "actual_next_gate": "",
                          "overall_pass": False,
                          "blocker_count": None,
                          "run_scope_match": False,
                      }
                  )
                  continue
              row_exec = str(s.get("execution_id", "")).strip()
              row_overall = truthy(s.get("overall_pass", False))
              row_blocker_count = as_int(s.get("blocker_count", None), default=-1)
              row_next_gate = str(s.get("next_gate", "")).strip()
              row_scope_match = (
                  str(s.get("platform_run_id", "")).strip() == platform_run_id
                  and str(s.get("scenario_run_id", "")).strip() == scenario_run_id
              )
              row_pass = bool(
                  row_exec == chain_ids.get(letter, "")
                  and row_overall
                  and row_blocker_count == 0
                  and row_next_gate == expected_gate
                  and row_scope_match
              )
              if not row_pass:
                  blockers.append({"code": "M11-B9", "message": f"{phase_name} summary failed rollup checks."})
              rollup_rows.append(
                  {
                      "phase": phase_name,
                      "execution_id": row_exec,
                      "row_pass": row_pass,
                      "expected_next_gate": expected_gate,
                      "actual_next_gate": row_next_gate,
                      "overall_pass": row_overall,
                      "blocker_count": row_blocker_count,
                      "run_scope_match": row_scope_match,
                  }
              )

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          p14_verdict = "ADVANCE_TO_P15" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M11.J_READY" if overall_pass else "HOLD_REMEDIATE"

          g_artifacts = g.get("artifact_keys", {}) if isinstance((g or {}).get("artifact_keys", {}), dict) else {}
          d_artifacts = d.get("artifact_keys", {}) if isinstance((d or {}).get("artifact_keys", {}), dict) else {}
          e_artifacts = e.get("artifact_keys", {}) if isinstance((e or {}).get("artifact_keys", {}), dict) else {}
          f_artifacts = f.get("artifact_keys", {}) if isinstance((f or {}).get("artifact_keys", {}), dict) else {}
          h_artifacts = h.get("artifact_keys", {}) if isinstance((h or {}).get("artifact_keys", {}), dict) else {}

          verdict_payload = {
              "captured_at_utc": now(),
              "phase": "M11.I",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": p14_verdict,
              "next_gate": next_gate,
              "upstream_refs": {
                  "m11a_execution_id": chain_ids["A"],
                  "m11b_execution_id": chain_ids["B"],
                  "m11c_execution_id": chain_ids["C"],
                  "m11d_execution_id": chain_ids["D"],
                  "m11e_execution_id": chain_ids["E"],
                  "m11f_execution_id": chain_ids["F"],
                  "m11g_execution_id": chain_ids["G"],
                  "m11h_execution_id": chain_ids["H"],
              },
              "rollup_matrix": rollup_rows,
          }

          handoff_payload = {
              "captured_at_utc": now(),
              "phase": "M11.I",
              "phase_id": "P14",
              "execution_id": exec_id,
              "handoff_from": "M11.I",
              "handoff_to": "M12",
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "p14_verdict": p14_verdict,
              "next_gate": next_gate,
              "m12_entry_ready": overall_pass,
              "m12_entry_gate": {
                  "required_verdict": "ADVANCE_TO_P15",
                  "requires_m11_status": "DONE",
                  "next_gate": "M12_READY",
              },
              "required_refs": {
                  "m11i_p14_gate_verdict_ref": f"evidence/dev_full/run_control/{exec_id}/m11i_p14_gate_verdict.json",
                  "mf_candidate_bundle_ref": to_s3_uri(bucket, g_artifacts.get("mf_candidate_bundle_key", "")),
                  "m11_model_operability_report_ref": to_s3_uri(bucket, g_artifacts.get("m11_model_operability_report", "")),
                  "m11_eval_report_ref": to_s3_uri(bucket, d_artifacts.get("m11_eval_report", "")),
                  "m11_eval_vs_baseline_report_ref": to_s3_uri(bucket, e_artifacts.get("m11_eval_vs_baseline_report", "")),
                  "mf_leakage_provenance_check_ref": to_s3_uri(bucket, e_artifacts.get("mf_leakage_provenance_check", "")),
                  "m11_reproducibility_check_ref": to_s3_uri(bucket, h_artifacts.get("m11_reproducibility_check", "")),
                  "mpr_rollback_drill_ref": to_s3_uri(bucket, h_artifacts.get("mpr_rollback_drill_key", "")),
                  "m11f_mlflow_lineage_snapshot_ref": to_s3_uri(bucket, f_artifacts.get("m11f_mlflow_lineage_snapshot", "")),
              },
              "source_chain": [
                  {
                      "phase": row["phase"],
                      "execution_id": row["execution_id"],
                      "row_pass": row["row_pass"],
                  }
                  for row in rollup_rows
              ],
              "blockers": blockers,
          }

          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M11.I",
              "phase_id": "P14",
              "execution_id": exec_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "overall_pass": overall_pass,
          }

          execution_summary = {
              "captured_at_utc": now(),
              "phase": "M11.I",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": p14_verdict,
              "next_gate": next_gate,
              "upstream_refs": {
                  "m11a_execution_id": chain_ids["A"],
                  "m11b_execution_id": chain_ids["B"],
                  "m11c_execution_id": chain_ids["C"],
                  "m11d_execution_id": chain_ids["D"],
                  "m11e_execution_id": chain_ids["E"],
                  "m11f_execution_id": chain_ids["F"],
                  "m11g_execution_id": chain_ids["G"],
                  "m11h_execution_id": chain_ids["H"],
              },
              "artifact_keys": {
                  "m11i_p14_gate_verdict": f"evidence/dev_full/run_control/{exec_id}/m11i_p14_gate_verdict.json",
                  "m12_handoff_pack": f"evidence/dev_full/run_control/{exec_id}/m12_handoff_pack.json",
                  "m11i_blocker_register": f"evidence/dev_full/run_control/{exec_id}/m11i_blocker_register.json",
                  "m11i_execution_summary": f"evidence/dev_full/run_control/{exec_id}/m11i_execution_summary.json",
              },
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          artifacts = {
              "m11i_p14_gate_verdict.json": verdict_payload,
              "m12_handoff_pack.json": handoff_payload,
              "m11i_blocker_register.json": blocker_register,
              "m11i_execution_summary.json": execution_summary,
          }
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B10", "message": f"Failed to upload {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          p14_verdict = "ADVANCE_TO_P15" if overall_pass else "HOLD_REMEDIATE"
          next_gate = "M11.J_READY" if overall_pass else "HOLD_REMEDIATE"
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          execution_summary["overall_pass"] = overall_pass
          execution_summary["blocker_count"] = len(blockers)
          execution_summary["verdict"] = p14_verdict
          execution_summary["next_gate"] = next_gate
          verdict_payload["overall_pass"] = overall_pass
          verdict_payload["blocker_count"] = len(blockers)
          verdict_payload["verdict"] = p14_verdict
          verdict_payload["next_gate"] = next_gate
          handoff_payload["p14_verdict"] = p14_verdict
          handoff_payload["next_gate"] = next_gate
          handoff_payload["m12_entry_ready"] = overall_pass
          handoff_payload["blockers"] = blockers

          for fname, payload in {
              "m11i_p14_gate_verdict.json": verdict_payload,
              "m12_handoff_pack.json": handoff_payload,
              "m11i_blocker_register.json": blocker_register,
              "m11i_execution_summary.json": execution_summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "upstream_m11h_execution": upstream_h,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": p14_verdict,
                      "m12_handoff_s3_uri": f"s3://{bucket}/{run_control_prefix}m12_handoff_pack.json",
                      "evidence_prefix": f"s3://{bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Upload local run artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dev-full-m11-${{ inputs.m11_subphase }}-${{ steps.run_meta.outputs.execution_id }}
          path: ${{ steps.run_meta.outputs.run_dir }}
          if-no-files-found: warn
