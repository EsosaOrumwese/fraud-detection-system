name: dev-full-m11-managed

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: AWS region
        required: true
        default: eu-west-2
        type: string
      aws_role_to_assume:
        description: OIDC role ARN
        required: true
        type: string
      evidence_bucket:
        description: S3 evidence bucket
        required: true
        default: fraud-platform-dev-full-evidence
        type: string
      m11_subphase:
        description: "M11 subphase to execute (single-runner lane)"
        required: true
        default: D
        type: choice
        options:
          - D
          - E
          - F
          - G
          - H
          - I
          - J
      upstream_m11c_execution:
        description: Upstream M11.C execution id
        required: true
        default: m11c_input_immutability_20260226T192723Z
        type: string
      upstream_m11d_execution:
        description: Upstream M11.D execution id (required for M11.E)
        required: false
        default: m11d_train_eval_execution_20260227T052312Z
        type: string
      upstream_m11e_execution:
        description: Upstream M11.E execution id (required for M11.F)
        required: false
        default: m11e_eval_gate_20260227T061316Z
        type: string
      m11d_execution_id:
        description: Optional fixed execution id
        required: false
        default: ""
        type: string
      m11e_execution_id:
        description: Optional fixed execution id for M11.E
        required: false
        default: ""
        type: string
      m11f_execution_id:
        description: Optional fixed execution id for M11.F
        required: false
        default: ""
        type: string
      poll_timeout_minutes:
        description: Timeout minutes
        required: true
        default: "45"
        type: string
      poll_interval_seconds:
        description: Poll interval seconds
        required: true
        default: "20"
        type: string
      m11d_training_instance_type:
        description: SageMaker training instance type
        required: true
        default: ml.m5.large
        type: string
      m11d_transform_instance_type:
        description: SageMaker transform instance type
        required: true
        default: ml.c4.xlarge
        type: string
      m11d_require_managed_transform:
        description: Fail closed unless managed batch transform completes
        required: true
        default: "true"
        type: choice
        options:
          - "true"
          - "false"

permissions:
  contents: read
  id-token: write

concurrency:
  group: dev-full-m11-managed-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  run_m11_managed:
    name: Run M11.${{ inputs.m11_subphase }} managed lane
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate subphase support
        shell: bash
        run: |
          set -euo pipefail
          if [[ "${{ inputs.m11_subphase }}" != "D" && "${{ inputs.m11_subphase }}" != "E" && "${{ inputs.m11_subphase }}" != "F" ]]; then
            echo "Unsupported m11_subphase='${{ inputs.m11_subphase }}'."
            echo "Supported subphases for this workflow: D, E, F."
            exit 1
          fi

      - name: Reject static AWS credential posture
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${AWS_ACCESS_KEY_ID:-}" || -n "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
            echo "Static AWS credentials are forbidden; use OIDC role assumption."
            exit 1
          fi

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: ${{ inputs.aws_region }}
          role-to-assume: ${{ inputs.aws_role_to_assume }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install runtime dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          python -m pip install boto3 botocore "sagemaker<3" "xgboost==1.7.6"

      - name: Compute execution metadata
        id: run_meta
        shell: bash
        run: |
          set -euo pipefail
          TS="$(date -u +'%Y%m%dT%H%M%SZ')"
          if [[ "${{ inputs.m11_subphase }}" == "D" ]]; then
            if [[ -n "${{ inputs.m11d_execution_id }}" ]]; then
              EXEC_ID="${{ inputs.m11d_execution_id }}"
            else
              EXEC_ID="m11d_train_eval_execution_${TS}"
            fi
          elif [[ "${{ inputs.m11_subphase }}" == "E" ]]; then
            if [[ -n "${{ inputs.m11e_execution_id }}" ]]; then
              EXEC_ID="${{ inputs.m11e_execution_id }}"
            else
              EXEC_ID="m11e_eval_gate_${TS}"
            fi
          else
            if [[ -n "${{ inputs.m11f_execution_id }}" ]]; then
              EXEC_ID="${{ inputs.m11f_execution_id }}"
            else
              EXEC_ID="m11f_mlflow_lineage_${TS}"
            fi
          fi
          echo "execution_id=${EXEC_ID}" >> "$GITHUB_OUTPUT"
          echo "run_dir=runs/dev_substrate/dev_full/m11/${EXEC_ID}" >> "$GITHUB_OUTPUT"

      - name: Execute M11.D (managed)
        if: ${{ inputs.m11_subphase == 'D' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M11C_EXEC: ${{ inputs.upstream_m11c_execution }}
          AWS_REGION: ${{ inputs.aws_region }}
          POLL_TIMEOUT_MINUTES: ${{ inputs.poll_timeout_minutes }}
          POLL_INTERVAL_SECONDS: ${{ inputs.poll_interval_seconds }}
          TRAINING_INSTANCE_TYPE: ${{ inputs.m11d_training_instance_type }}
          TRANSFORM_INSTANCE_TYPE: ${{ inputs.m11d_transform_instance_type }}
          REQUIRE_MANAGED_TRANSFORM: ${{ inputs.m11d_require_managed_transform }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import io, json, os, tarfile, time, hashlib, re, tempfile
          from datetime import datetime, timezone
          from pathlib import Path
          import boto3
          from botocore.exceptions import ClientError, BotoCoreError
          from sagemaker import image_uris

          def now():
              return datetime.now(timezone.utc).isoformat().replace('+00:00','Z')
          def s3json(s3,b,k):
              return json.loads(s3.get_object(Bucket=b,Key=k)['Body'].read().decode('utf-8'))
          def putj(s3,b,k,p):
              s3.put_object(Bucket=b,Key=k,Body=json.dumps(p,indent=2,ensure_ascii=True).encode('utf-8'),ContentType='application/json')
          def putt(s3,b,k,t,ct='text/csv'):
              s3.put_object(Bucket=b,Key=k,Body=t.encode('utf-8'),ContentType=ct)
          def wait_train(sm,name,to,poll):
              st=time.time()
              while True:
                  d=sm.describe_training_job(TrainingJobName=name)
                  s=d.get('TrainingJobStatus','')
                  if s in ('Completed','Failed','Stopped'): return d
                  if time.time()-st>to: raise TimeoutError('training timeout')
                  time.sleep(poll)
          def wait_xf(sm,name,to,poll):
              st=time.time()
              while True:
                  d=sm.describe_transform_job(TransformJobName=name)
                  s=d.get('TransformJobStatus','')
                  if s in ('Completed','Failed','Stopped'): return d
                  if time.time()-st>to: raise TimeoutError('transform timeout')
                  time.sleep(poll)
          def job(s):
              s=re.sub(r'[^A-Za-z0-9-]+','-',s).strip('-')
              return (s[:63] or 'm11d-job').rstrip('-')
          def mk(i,seed):
              f1=((i+seed%17)%100)/100.0
              f2=((i*7+seed%31)%100)/100.0
              f3=((i*13+seed%47)%100)/100.0
              y=1 if (1.3*f1+0.9*f2+0.6*f3) >= (0.95+(seed%9)*0.01) else 0
              return y,round(f1,6),round(f2,6),round(f3,6)
          def metric_pack(labels,preds):
              if not labels or len(labels)!=len(preds):
                  return {'accuracy':0.0,'precision':0.0,'recall':0.0}
              tp=fp=tn=fn=0
              for y,p in zip(labels,preds):
                  yh=1 if p>=0.5 else 0
                  if y==1 and yh==1: tp+=1
                  elif y==0 and yh==1: fp+=1
                  elif y==0 and yh==0: tn+=1
                  else: fn+=1
              n=len(labels)
              return {
                  'accuracy':round((tp+tn)/n,6),
                  'precision':round(tp/max(tp+fp,1),6),
                  'recall':round(tp/max(tp+fn,1),6),
              }

          exec_id=os.environ['EXEC_ID']
          run_dir=Path(os.environ['RUN_DIR'])
          bkt=os.environ['EVIDENCE_BUCKET']
          up=os.environ['UPSTREAM_M11C_EXEC']
          region=os.environ['AWS_REGION']
          timeout=max(int(os.environ.get('POLL_TIMEOUT_MINUTES','45')),1)*60
          poll=max(int(os.environ.get('POLL_INTERVAL_SECONDS','20')),5)
          training_instance_type=str(os.environ.get('TRAINING_INSTANCE_TYPE','ml.m5.large')).strip() or 'ml.m5.large'
          transform_instance_type=str(os.environ.get('TRANSFORM_INSTANCE_TYPE','ml.c4.xlarge')).strip() or 'ml.c4.xlarge'
          require_managed_transform=str(os.environ.get('REQUIRE_MANAGED_TRANSFORM','true')).strip().lower() == 'true'

          s3=boto3.client('s3',region_name=region)
          ssm=boto3.client('ssm',region_name=region)
          sm=boto3.client('sagemaker',region_name=region)
          blockers=[]
          errs=[]
          advisories=[]

          m11c_key=f'evidence/dev_full/run_control/{up}/m11c_execution_summary.json'
          m11c=s3json(s3,bkt,m11c_key)
          if not m11c.get('overall_pass'): blockers.append({'code':'M11-B4','message':'M11.C not pass','surface':m11c_key})
          if m11c.get('next_gate')!='M11.D_READY': blockers.append({'code':'M11-B4','message':'M11.C next_gate mismatch','surface':m11c_key})
          pr=str(m11c.get('platform_run_id','')).strip(); sr=str(m11c.get('scenario_run_id','')).strip()
          if not pr or not sr: blockers.append({'code':'M11-B4','message':'run scope missing','surface':m11c_key})

          m11b_exec=m11c.get('upstream_refs',{}).get('m11b_execution_id','')
          m11a_exec=m11c.get('upstream_refs',{}).get('m11a_execution_id','')
          m11b_snap=s3json(s3,bkt,f'evidence/dev_full/run_control/{m11b_exec}/m11b_sagemaker_readiness_snapshot.json') if m11b_exec else {}
          m11a_snap=s3json(s3,bkt,f'evidence/dev_full/run_control/{m11a_exec}/m11a_handle_closure_snapshot.json') if m11a_exec else {}

          role=m11b_snap.get('ssm_checks',{}).get('ssm_sagemaker_role_arn_value','')
          role_path=''
          train_prefix='fraud-platform-dev-full-mtrain'
          batch_prefix='fraud-platform-dev-full-mbatch'
          eval_pat=f'evidence/runs/{pr}/learning/mf/eval_report.json'
          budget_pat=f'evidence/dev_full/run_control/{exec_id}/phase_budget_envelope.json'
          for row in m11b_snap.get('handle_matrix',[]):
              if row.get('handle')=='SSM_SAGEMAKER_MODEL_EXEC_ROLE_ARN_PATH': role_path=row.get('value','')
          for row in m11a_snap.get('handle_matrix',[]):
              h=row.get('handle'); v=row.get('value','')
              if h=='SM_TRAINING_JOB_NAME_PREFIX': train_prefix=v
              if h=='SM_BATCH_TRANSFORM_JOB_NAME_PREFIX': batch_prefix=v
              if h=='MF_EVAL_REPORT_PATH_PATTERN': eval_pat=v.replace('{platform_run_id}',pr)
              if h=='PHASE_BUDGET_ENVELOPE_PATH_PATTERN': budget_pat=v.replace('{phase_execution_id}',exec_id)

          if not role and role_path:
              try: role=ssm.get_parameter(Name=role_path)['Parameter']['Value']
              except Exception as e: blockers.append({'code':'M11-B4','message':'role resolve failed','surface':role_path}); errs.append(str(e))
          if not role: blockers.append({'code':'M11-B4','message':'role unresolved','surface':'ROLE_SAGEMAKER_EXECUTION'})

          m11c_snap=s3json(s3,bkt,f'evidence/dev_full/run_control/{up}/m11c_input_immutability_snapshot.json')
          fp_ref=m11c_snap.get('resolved_refs',{}).get('m10g_fingerprint_ref','')
          if not fp_ref: blockers.append({'code':'M11-B4','message':'fingerprint ref missing','surface':'m11c_snapshot'})
          if fp_ref.startswith('s3://'):
              x=fp_ref[5:].split('/',1); fb, fk=x[0],x[1]
          else:
              fb, fk=bkt, fp_ref.lstrip('/')
          fps=s3json(s3,fb,fk).get('fingerprint_sha256','') if fp_ref else ''
          if not fps: blockers.append({'code':'M11-B4','message':'fingerprint digest missing','surface':fp_ref or 'm10g_fingerprint'})

          envelope={
              'captured_at_utc':now(),'phase':'M11.D','phase_id':'P14','execution_id':exec_id,
              'platform_run_id':pr,'scenario_run_id':sr,
              'runtime_budget':{'target_minutes':45,'hard_alert_minutes':60,'configured_timeout_minutes':timeout//60,'poll_interval_seconds':poll},
              'status':'EMITTED_PRE_LAUNCH'
          }
          putj(s3,bkt,budget_pat.lstrip('/'),envelope)

          tdesc={}; xdesc={}; metrics={'accuracy':0.0,'precision':0.0,'recall':0.0}; model=''; tjob=''; xjob=''; mdata=''; outk=''
          eval_mode='managed_batch_transform'
          inp={}
          if not blockers:
              seed=int(fps[:8],16)
              rows=[mk(i,seed) for i in range(240)]
              tr,va,te=rows[:160],rows[160:200],rows[200:240]
              labels=[r[0] for r in te]
              pfx=f'evidence/runs/{pr}/learning/mf/input/{exec_id}'
              ktr=f'{pfx}/train/train.csv'; kva=f'{pfx}/validation/validation.csv'; kte=f'{pfx}/test/test_features.csv'; kl=f'{pfx}/test/test_labels.json'
              putt(s3,bkt,ktr,'\n'.join(f"{y},{a},{b},{c}" for y,a,b,c in tr)+'\n')
              putt(s3,bkt,kva,'\n'.join(f"{y},{a},{b},{c}" for y,a,b,c in va)+'\n')
              putt(s3,bkt,kte,'\n'.join(f"{a},{b},{c}" for _,a,b,c in te)+'\n')
              putj(s3,bkt,kl,{'labels':labels,'platform_run_id':pr,'scenario_run_id':sr,'execution_id':exec_id})
              inp={'train_key':ktr,'validation_key':kva,'test_features_key':kte,'test_labels_key':kl}

              image=image_uris.retrieve(
                  framework='xgboost',
                  region=region,
                  version='1.7-1',
                  image_scope='training',
                  instance_type=training_instance_type,
              )
              suf=hashlib.sha256(f'{exec_id}|{fps}'.encode()).hexdigest()[:10]
              tjob=job(f'{train_prefix}-{suf}'); xjob=job(f'{batch_prefix}-{suf}'); model=job(f'{train_prefix}-model-{suf}')
              tr_uri=f"s3://{bkt}/{ktr.rsplit('/',1)[0]}/"
              va_uri=f"s3://{bkt}/{kva.rsplit('/',1)[0]}/"
              # Use object-specific S3 URI so transform does not ingest sibling JSON labels.
              te_uri=f"s3://{bkt}/{kte}"
              out_train=f's3://{bkt}/evidence/runs/{pr}/learning/mf/train_output/{exec_id}/'
              out_xf=f's3://{bkt}/evidence/runs/{pr}/learning/mf/transform_output/{exec_id}/'
              try:
                  sm.create_training_job(
                    TrainingJobName=tjob,
                    AlgorithmSpecification={'TrainingImage':image,'TrainingInputMode':'File'},
                    RoleArn=role,
                    InputDataConfig=[
                      {'ChannelName':'train','DataSource':{'S3DataSource':{'S3DataType':'S3Prefix','S3Uri':tr_uri,'S3DataDistributionType':'FullyReplicated'}},'ContentType':'text/csv'},
                      {'ChannelName':'validation','DataSource':{'S3DataSource':{'S3DataType':'S3Prefix','S3Uri':va_uri,'S3DataDistributionType':'FullyReplicated'}},'ContentType':'text/csv'}],
                    OutputDataConfig={'S3OutputPath':out_train},
                    ResourceConfig={'InstanceType':training_instance_type,'InstanceCount':1,'VolumeSizeInGB':30},
                    StoppingCondition={'MaxRuntimeInSeconds':timeout},
                    HyperParameters={'objective':'binary:logistic','eval_metric':'auc','num_round':'25','max_depth':'4','eta':'0.2','subsample':'0.8','verbosity':'0'} )
                  tdesc=wait_train(sm,tjob,timeout,poll)
                  if tdesc.get('TrainingJobStatus')!='Completed': raise RuntimeError(f"training status={tdesc.get('TrainingJobStatus')}")
                  mdata=tdesc.get('ModelArtifacts',{}).get('S3ModelArtifacts','')
                  if not mdata: raise RuntimeError('missing model artifact')
                  preds=[]
                  labels=json.loads(s3.get_object(Bucket=bkt,Key=kl)['Body'].read().decode('utf-8')).get('labels',[])
                  try:
                      sm.create_model(ModelName=model,ExecutionRoleArn=role,PrimaryContainer={'Image':image,'ModelDataUrl':mdata})
                      sm.create_transform_job(
                        TransformJobName=xjob,ModelName=model,
                        TransformInput={'DataSource':{'S3DataSource':{'S3DataType':'S3Prefix','S3Uri':te_uri}},'ContentType':'text/csv','SplitType':'Line'},
                        TransformOutput={'S3OutputPath':out_xf,'AssembleWith':'Line','Accept':'text/csv'},
                        TransformResources={'InstanceType':transform_instance_type,'InstanceCount':1})
                      xdesc=wait_xf(sm,xjob,timeout,poll)
                      if xdesc.get('TransformJobStatus')!='Completed': raise RuntimeError(f"transform status={xdesc.get('TransformJobStatus')}")
                      out_uri=xdesc.get('TransformOutput',{}).get('S3OutputPath','')
                      xb,xp=out_uri[5:].split('/',1)
                      objs=s3.list_objects_v2(Bucket=xb,Prefix=xp).get('Contents',[])
                      outs=[o.get('Key','') for o in objs if o.get('Key','').endswith('.out')]
                      if not outs: raise RuntimeError('no transform output')
                      outk=outs[0]
                      pred_raw=s3.get_object(Bucket=xb,Key=outk)['Body'].read().decode('utf-8').splitlines()
                      for line in pred_raw:
                          tok=line.strip().split(',')[0].strip()
                          if tok:
                              try: preds.append(float(tok))
                              except Exception: pass
                  except Exception as transform_exc:
                      msg=str(transform_exc)
                      if 'ResourceLimitExceeded' in msg and 'transform job usage' in msg:
                          if require_managed_transform:
                              raise RuntimeError(
                                  f"managed transform required but unavailable ({transform_instance_type}): {msg}"
                              )
                          eval_mode='fallback_local_model_eval'
                          xdesc={'TransformJobStatus':'SKIPPED_RESOURCE_LIMIT','FailureReason':msg}
                          advisories.append({
                              'code':'M11D-AD1',
                              'message':'Batch transform quota unavailable; using local model-artifact evaluation fallback.',
                              'surface':'sagemaker_transform_quota',
                          })
                          import xgboost as xgb
                          mb,mk=mdata[5:].split('/',1)
                          blob=s3.get_object(Bucket=mb,Key=mk)['Body'].read()
                          with tarfile.open(fileobj=io.BytesIO(blob), mode='r:gz') as tf:
                              members=[m for m in tf.getmembers() if m.isfile()]
                              if not members:
                                  raise RuntimeError('no model file in model artifact tar')
                              chosen=next((m for m in members if 'xgboost' in m.name.lower() or 'model' in m.name.lower()), members[0])
                              with tf.extractfile(chosen) as fh:
                                  model_bytes=fh.read()
                          with tempfile.NamedTemporaryFile(suffix='.model') as model_file:
                              model_file.write(model_bytes)
                              model_file.flush()
                              booster=xgb.Booster()
                              booster.load_model(model_file.name)
                          feat_lines=s3.get_object(Bucket=bkt,Key=kte)['Body'].read().decode('utf-8').splitlines()
                          rows_x=[]
                          for line in feat_lines:
                              raw=line.strip()
                              if not raw:
                                  continue
                              rows_x.append([float(v) for v in raw.split(',')])
                          if not rows_x:
                              raise RuntimeError('empty fallback feature rows')
                          preds=booster.predict(xgb.DMatrix(rows_x)).tolist()
                      else:
                          raise
                  metrics=metric_pack(labels,preds)
              except Exception as e:
                  blockers.append({'code':'M11-B4','message':f'managed train/eval failed: {type(e).__name__}','surface':'sagemaker'})
                  errs.append(str(e))

          if not blockers:
              putj(s3,bkt,eval_pat.lstrip('/'),{
                'captured_at_utc':now(),'phase':'M11.D','phase_id':'P14','execution_id':exec_id,
                'platform_run_id':pr,'scenario_run_id':sr,
                'training_job_name':tjob,'transform_job_name':xjob,'eval_mode':eval_mode,'metrics':metrics,'status':'COMMITTED',
                'upstream_m11c_execution':up})

          tel=None; xel=None; total=0.0
          if tdesc.get('TrainingStartTime') and tdesc.get('TrainingEndTime'):
              tel=max((tdesc['TrainingEndTime']-tdesc['TrainingStartTime']).total_seconds(),0.0); total+=tel
          if xdesc.get('TransformStartTime') and xdesc.get('TransformEndTime'):
              xel=max((xdesc['TransformEndTime']-xdesc['TransformStartTime']).total_seconds(),0.0); total+=xel

          ok=len(blockers)==0
          next_gate='M11.E_READY' if ok else 'HOLD_REMEDIATE'
          verdict='ADVANCE_TO_M11_E' if ok else 'HOLD_REMEDIATE'

          snap={'captured_at_utc':now(),'phase':'M11.D','phase_id':'P14','execution_id':exec_id,'platform_run_id':pr,'scenario_run_id':sr,
                'upstream_refs':{'m11c_execution_id':up,'m11b_execution_id':m11b_exec,'m11a_execution_id':m11a_exec},
                'input_refs':{'m10g_fingerprint_ref':fp_ref,'deterministic_input_keys':inp},
                'budget_envelope_key':budget_pat.lstrip('/'),'eval_report_key':eval_pat.lstrip('/'),
                'jobs':{'training':{'job_name':tjob,'status':tdesc.get('TrainingJobStatus',''),'failure_reason':tdesc.get('FailureReason',''),'model_artifact_uri':mdata,'elapsed_seconds':tel},
                        'transform':{'job_name':xjob,'status':xdesc.get('TransformJobStatus',''),'failure_reason':xdesc.get('FailureReason',''),'model_name':model,'output_key':outk,'elapsed_seconds':xel,'eval_mode':eval_mode}},
                'metrics':metrics,'runtime':{
                    'elapsed_seconds':round(total,3),
                    'budget_target_minutes':45,
                    'budget_alert_minutes':60,
                    'poll_timeout_minutes':timeout//60,
                    'poll_interval_seconds':poll,
                    'training_instance_type':training_instance_type,
                    'transform_instance_type':transform_instance_type,
                    'require_managed_transform':require_managed_transform,
                },
                'read_errors':errs,'advisories':advisories, 'overall_pass':ok,'blocker_count':len(blockers),'next_gate':next_gate}
          br={'captured_at_utc':now(),'phase':'M11.D','phase_id':'P14','execution_id':exec_id,'blocker_count':len(blockers),'blockers':blockers,'overall_pass':ok}
          smy={'captured_at_utc':now(),'phase':'M11.D','phase_id':'P14','execution_id':exec_id,'platform_run_id':pr,'scenario_run_id':sr,
               'overall_pass':ok,'blocker_count':len(blockers),'verdict':verdict,'next_gate':next_gate,
               'upstream_refs':{'m11c_execution_id':up,'m11b_execution_id':m11b_exec,'m11a_execution_id':m11a_exec},
               'artifact_keys':{'m11_phase_budget_envelope':budget_pat.lstrip('/'),'m11d_train_eval_execution_snapshot':f'evidence/dev_full/run_control/{exec_id}/m11d_train_eval_execution_snapshot.json','m11d_blocker_register':f'evidence/dev_full/run_control/{exec_id}/m11d_blocker_register.json','m11d_execution_summary':f'evidence/dev_full/run_control/{exec_id}/m11d_execution_summary.json','m11_eval_report':eval_pat.lstrip('/')}}

          run_dir.mkdir(parents=True,exist_ok=True)
          arts={'m11_phase_budget_envelope.json':envelope,'m11d_train_eval_execution_snapshot.json':snap,'m11d_blocker_register.json':br,'m11d_execution_summary.json':smy}
          for fn,p in arts.items():
              (run_dir/fn).write_text(json.dumps(p,indent=2,ensure_ascii=True)+'\n',encoding='utf-8')
          pref=f'evidence/dev_full/run_control/{exec_id}/'
          for fn,p in arts.items():
              putj(s3,bkt,pref+fn,p)

          print(json.dumps({'execution_id':exec_id,'overall_pass':ok,'blocker_count':len(blockers),'next_gate':next_gate,'verdict':verdict,'evidence_prefix':f's3://{bkt}/{pref}'},indent=2,ensure_ascii=True))
          if not ok: raise SystemExit(1)
          PY

      - name: Execute M11.E (managed)
        if: ${{ inputs.m11_subphase == 'E' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M11D_EXEC: ${{ inputs.upstream_m11d_execution }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import json, os, re
          from decimal import Decimal, InvalidOperation
          from pathlib import Path
          from datetime import datetime, timezone
          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, object]:
              out: dict[str, object] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      value: object = raw[1:-1]
                  elif raw.lower() == "true":
                      value = True
                  elif raw.lower() == "false":
                      value = False
                  else:
                      try:
                          value = int(raw) if "." not in raw else float(raw)
                      except ValueError:
                          value = raw
                  out[key] = value
              return out

          def is_placeholder(value: object) -> bool:
              s = str(value).strip()
              sl = s.lower()
              if not s:
                  return True
              if sl in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "placeholder" in sl or "to_pin" in sl:
                  return True
              if "<" in s and ">" in s:
                  return True
              return False

          def as_decimal(value: object, key: str) -> Decimal:
              try:
                  return Decimal(str(value).strip())
              except (InvalidOperation, ValueError) as exc:
                  raise ValueError(f"invalid_decimal:{key}:{value}") from exc

          def s3_get_json(s3: object, bucket: str, key: str) -> dict[str, object]:
              body = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(body)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: object, bucket: str, key: str, payload: dict[str, object]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          bucket = os.environ["EVIDENCE_BUCKET"].strip()
          upstream = os.environ["UPSTREAM_M11D_EXEC"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"

          if not upstream:
              raise SystemExit("UPSTREAM_M11D_EXEC is required for M11.E.")

          s3 = boto3.client("s3", region_name=region)
          handles = parse_handles(HANDLES_PATH)
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []
          notes: list[str] = []

          required_handle_keys = [
              "MF_EVAL_ACCURACY_MIN",
              "MF_EVAL_PRECISION_MIN",
              "MF_EVAL_RECALL_MIN",
              "MF_EVAL_BASELINE_DELTA_ACCURACY_MIN",
              "MF_EVAL_BASELINE_DELTA_PRECISION_MIN",
              "MF_EVAL_BASELINE_DELTA_RECALL_MIN",
              "MF_EVAL_LEAKAGE_HARD_FAIL",
              "MF_EVAL_STABILITY_MAX_DELTA_PCT",
              "MF_EVAL_REPORT_PATH_PATTERN",
              "MF_LEAKAGE_PROVENANCE_CHECK_PATH_PATTERN",
              "LEARNING_LEAKAGE_GUARDRAIL_REPORT_PATH_PATTERN",
          ]
          missing_handles = [k for k in required_handle_keys if k not in handles]
          placeholder_handles = [k for k in required_handle_keys if k in handles and is_placeholder(handles.get(k))]
          if missing_handles or placeholder_handles:
              blockers.append({
                  "code": "M11-B5.1",
                  "message": "Required M11.E handles unresolved: "
                  + ",".join(sorted(missing_handles + placeholder_handles)),
              })

          m11d_summary_key = f"evidence/dev_full/run_control/{upstream}/m11d_execution_summary.json"
          m11d_snapshot_key = f"evidence/dev_full/run_control/{upstream}/m11d_train_eval_execution_snapshot.json"
          m11d_summary: dict[str, object] | None = None
          m11d_snapshot: dict[str, object] | None = None
          try:
              m11d_summary = s3_get_json(s3, bucket, m11d_summary_key)
              m11d_snapshot = s3_get_json(s3, bucket, m11d_snapshot_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": "m11d_summary_or_snapshot", "error": type(exc).__name__})
              blockers.append({"code": "M11-B5.2", "message": "Authoritative M11.D evidence unreadable."})

          platform_run_id = ""
          scenario_run_id = ""
          eval_report_key = ""
          leakage_guardrail_key = ""
          leakage_prov_key = ""
          eval_report: dict[str, object] | None = None
          leakage_guardrail: dict[str, object] | None = None

          if m11d_summary and m11d_snapshot:
              if not bool(m11d_summary.get("overall_pass")):
                  blockers.append({"code": "M11-B5.2", "message": "M11.D summary not pass posture."})
              if str(m11d_summary.get("next_gate", "")).strip() != "M11.E_READY":
                  blockers.append({"code": "M11-B5.2", "message": "M11.D next_gate is not M11.E_READY."})
              if str(m11d_snapshot.get("next_gate", "")).strip() != "M11.E_READY":
                  blockers.append({"code": "M11-B5.2", "message": "M11.D snapshot next_gate is not M11.E_READY."})
              platform_run_id = str(m11d_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m11d_summary.get("scenario_run_id", "")).strip()
              if not platform_run_id or not scenario_run_id:
                  blockers.append({"code": "M11-B5.2", "message": "Run scope unresolved from M11.D summary."})

          if platform_run_id:
              eval_report_pat = str(handles.get("MF_EVAL_REPORT_PATH_PATTERN", "")).strip()
              leakage_guardrail_pat = str(handles.get("LEARNING_LEAKAGE_GUARDRAIL_REPORT_PATH_PATTERN", "")).strip()
              leakage_prov_pat = str(handles.get("MF_LEAKAGE_PROVENANCE_CHECK_PATH_PATTERN", "")).strip()
              eval_report_key = eval_report_pat.replace("{platform_run_id}", platform_run_id)
              leakage_guardrail_key = leakage_guardrail_pat.replace("{platform_run_id}", platform_run_id)
              leakage_prov_key = leakage_prov_pat.replace("{platform_run_id}", platform_run_id)
              try:
                  eval_report = s3_get_json(s3, bucket, eval_report_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": eval_report_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B5.2", "message": "MF eval report unreadable."})
              try:
                  leakage_guardrail = s3_get_json(s3, bucket, leakage_guardrail_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": leakage_guardrail_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B5.3", "message": "Learning leakage guardrail report unreadable."})

          policy = {}
          for k in required_handle_keys:
              if k in handles:
                  policy[k] = handles[k]

          metric_min = {}
          delta_min = {}
          try:
              metric_min = {
                  "accuracy": as_decimal(handles.get("MF_EVAL_ACCURACY_MIN", "0"), "MF_EVAL_ACCURACY_MIN"),
                  "precision": as_decimal(handles.get("MF_EVAL_PRECISION_MIN", "0"), "MF_EVAL_PRECISION_MIN"),
                  "recall": as_decimal(handles.get("MF_EVAL_RECALL_MIN", "0"), "MF_EVAL_RECALL_MIN"),
              }
              delta_min = {
                  "accuracy": as_decimal(handles.get("MF_EVAL_BASELINE_DELTA_ACCURACY_MIN", "0"), "MF_EVAL_BASELINE_DELTA_ACCURACY_MIN"),
                  "precision": as_decimal(handles.get("MF_EVAL_BASELINE_DELTA_PRECISION_MIN", "0"), "MF_EVAL_BASELINE_DELTA_PRECISION_MIN"),
                  "recall": as_decimal(handles.get("MF_EVAL_BASELINE_DELTA_RECALL_MIN", "0"), "MF_EVAL_BASELINE_DELTA_RECALL_MIN"),
              }
              stability_max_delta_pct = as_decimal(
                  handles.get("MF_EVAL_STABILITY_MAX_DELTA_PCT", "0"),
                  "MF_EVAL_STABILITY_MAX_DELTA_PCT",
              )
          except ValueError as exc:
              blockers.append({"code": "M11-B5.1", "message": str(exc)})
              metric_min = {"accuracy": Decimal("0"), "precision": Decimal("0"), "recall": Decimal("0")}
              delta_min = {"accuracy": Decimal("0"), "precision": Decimal("0"), "recall": Decimal("0")}
              stability_max_delta_pct = Decimal("0")

          leakage_hard_fail = str(handles.get("MF_EVAL_LEAKAGE_HARD_FAIL", "true")).strip().lower() == "true"

          candidate_metrics_dec = {"accuracy": Decimal("0"), "precision": Decimal("0"), "recall": Decimal("0")}
          performance_gate_rows: list[dict[str, object]] = []
          leakage_gate_pass = True
          stability_gate_pass = True
          stability_delta_pct = Decimal("0")

          if eval_report:
              metrics_obj = eval_report.get("metrics", {})
              if not isinstance(metrics_obj, dict):
                  blockers.append({"code": "M11-B5.2", "message": "Eval report metrics missing/non-object."})
              else:
                  for metric_name in ("accuracy", "precision", "recall"):
                      if metric_name not in metrics_obj:
                          blockers.append({"code": "M11-B5.2", "message": f"Eval report metric missing: {metric_name}."})
                          continue
                      try:
                          candidate_metrics_dec[metric_name] = as_decimal(metrics_obj.get(metric_name), metric_name)
                      except ValueError as exc:
                          blockers.append({"code": "M11-B5.2", "message": str(exc)})

          if leakage_guardrail:
              leakage_guardrail_pass = bool(leakage_guardrail.get("overall_pass", False))
              if leakage_hard_fail and not leakage_guardrail_pass:
                  leakage_gate_pass = False
                  blockers.append({"code": "M11-B5.3", "message": "Leakage guardrail failed under hard-fail policy."})
          else:
              if leakage_hard_fail:
                  leakage_gate_pass = False
                  blockers.append({"code": "M11-B5.3", "message": "Leakage hard-fail policy enabled but guardrail evidence missing."})
              else:
                  notes.append("Leakage hard-fail disabled; missing guardrail evidence tolerated.")

          for metric_name in ("accuracy", "precision", "recall"):
              candidate = candidate_metrics_dec.get(metric_name, Decimal("0"))
              floor = metric_min.get(metric_name, Decimal("0"))
              delta = candidate - floor
              floor_ok = candidate >= floor
              delta_ok = delta >= delta_min.get(metric_name, Decimal("0"))
              row = {
                  "metric": metric_name,
                  "candidate": float(candidate),
                  "baseline_policy_floor": float(floor),
                  "delta_vs_floor": float(delta),
                  "required_delta_floor": float(delta_min.get(metric_name, Decimal("0"))),
                  "floor_ok": floor_ok,
                  "delta_ok": delta_ok,
                  "pass": bool(floor_ok and delta_ok),
              }
              performance_gate_rows.append(row)
              if not row["pass"]:
                  blockers.append({"code": "M11-B5.4", "message": f"Performance gate failed for {metric_name}."})

          if eval_report:
              raw_stability = eval_report.get("stability_delta_pct")
              if raw_stability is None:
                  stability_delta_pct = Decimal("0")
                  notes.append("stability_delta_pct absent in eval report; using deterministic single-run delta=0 for M11.E.")
              else:
                  try:
                      stability_delta_pct = as_decimal(raw_stability, "stability_delta_pct")
                  except ValueError:
                      stability_delta_pct = Decimal("9999")
              stability_gate_pass = stability_delta_pct <= stability_max_delta_pct
              if not stability_gate_pass:
                  blockers.append({"code": "M11-B5.5", "message": "Stability tolerance exceeded."})

          leakage_provenance_check = {
              "captured_at_utc": now(),
              "phase": "M11.E",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_m11d_execution": upstream,
              "source_refs": {
                  "m11d_summary": f"s3://{bucket}/{m11d_summary_key}",
                  "m11d_snapshot": f"s3://{bucket}/{m11d_snapshot_key}",
                  "mf_eval_report": f"s3://{bucket}/{eval_report_key}" if eval_report_key else None,
                  "learning_leakage_guardrail_report": f"s3://{bucket}/{leakage_guardrail_key}" if leakage_guardrail_key else None,
              },
              "checks": {
                  "run_scope_present": bool(platform_run_id and scenario_run_id),
                  "eval_report_readable": bool(eval_report is not None),
                  "leakage_guardrail_readable": bool(leakage_guardrail is not None),
                  "leakage_hard_fail_enabled": leakage_hard_fail,
                  "leakage_gate_pass": leakage_gate_pass,
              },
              "overall_pass": len(blockers) == 0,
          }

          baseline_report = {
              "captured_at_utc": now(),
              "phase": "M11.E",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "baseline_mode": "policy_floor",
              "policy": {
                  "metric_minima": {k: float(v) for k, v in metric_min.items()},
                  "baseline_delta_floors": {k: float(v) for k, v in delta_min.items()},
                  "leakage_hard_fail": leakage_hard_fail,
                  "stability_max_delta_pct": float(stability_max_delta_pct),
              },
              "candidate_metrics": {k: float(v) for k, v in candidate_metrics_dec.items()},
              "metric_comparisons": performance_gate_rows,
              "stability": {
                  "observed_delta_pct": float(stability_delta_pct),
                  "max_allowed_delta_pct": float(stability_max_delta_pct),
                  "pass": stability_gate_pass,
              },
              "leakage": {
                  "hard_fail_enabled": leakage_hard_fail,
                  "pass": leakage_gate_pass,
              },
              "overall_pass": len(blockers) == 0,
          }

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.F_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_F" if overall_pass else "HOLD_REMEDIATE"

          snapshot = {
              "captured_at_utc": now(),
              "phase": "M11.E",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_refs": {"m11d_execution_id": upstream},
              "resolved_policy_handles": policy,
              "source_refs": {
                  "m11d_summary_key": m11d_summary_key,
                  "m11d_snapshot_key": m11d_snapshot_key,
                  "mf_eval_report_key": eval_report_key,
                  "learning_leakage_guardrail_key": leakage_guardrail_key,
                  "mf_leakage_provenance_check_key": leakage_prov_key,
              },
              "gate_results": {
                  "compatibility": bool(eval_report is not None),
                  "leakage": leakage_gate_pass,
                  "performance": all(bool(r.get("pass")) for r in performance_gate_rows),
                  "stability": stability_gate_pass,
              },
              "read_errors": read_errors,
              "upload_errors": upload_errors,
              "notes": notes,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M11.E",
              "phase_id": "P14",
              "execution_id": exec_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "overall_pass": overall_pass,
          }
          summary = {
              "captured_at_utc": now(),
              "phase": "M11.E",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "upstream_refs": {"m11d_execution_id": upstream},
              "artifact_keys": {
                  "m11_eval_vs_baseline_report": f"evidence/dev_full/run_control/{exec_id}/m11_eval_vs_baseline_report.json",
                  "m11e_eval_gate_snapshot": f"evidence/dev_full/run_control/{exec_id}/m11e_eval_gate_snapshot.json",
                  "m11e_blocker_register": f"evidence/dev_full/run_control/{exec_id}/m11e_blocker_register.json",
                  "m11e_execution_summary": f"evidence/dev_full/run_control/{exec_id}/m11e_execution_summary.json",
                  "mf_leakage_provenance_check": leakage_prov_key,
              },
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          artifacts = {
              "m11_eval_vs_baseline_report.json": baseline_report,
              "m11_leakage_provenance_check.json": leakage_provenance_check,
              "m11e_eval_gate_snapshot.json": snapshot,
              "m11e_blocker_register.json": blocker_register,
              "m11e_execution_summary.json": summary,
          }
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B5.6", "message": f"Failed to upload {fname}."})

          if leakage_prov_key:
              try:
                  s3_put_json(s3, bucket, leakage_prov_key.lstrip("/"), leakage_provenance_check)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": leakage_prov_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B5.6", "message": "Failed to publish leakage provenance check."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.F_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_F" if overall_pass else "HOLD_REMEDIATE"
          snapshot["upload_errors"] = upload_errors
          snapshot["overall_pass"] = overall_pass
          snapshot["blocker_count"] = len(blockers)
          snapshot["next_gate"] = next_gate
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          summary["overall_pass"] = overall_pass
          summary["blocker_count"] = len(blockers)
          summary["verdict"] = verdict
          summary["next_gate"] = next_gate

          # Rewrite local and durable summaries with final status after upload attempts.
          for fname, payload in {
              "m11e_eval_gate_snapshot.json": snapshot,
              "m11e_blocker_register.json": blocker_register,
              "m11e_execution_summary.json": summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "upstream_m11d_execution": upstream,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "evidence_prefix": f"s3://{bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Execute M11.F (managed)
        if: ${{ inputs.m11_subphase == 'F' }}
        shell: bash
        env:
          EXEC_ID: ${{ steps.run_meta.outputs.execution_id }}
          RUN_DIR: ${{ steps.run_meta.outputs.run_dir }}
          EVIDENCE_BUCKET: ${{ inputs.evidence_bucket }}
          UPSTREAM_M11E_EXEC: ${{ inputs.upstream_m11e_execution }}
          AWS_REGION: ${{ inputs.aws_region }}
        run: |
          set -euo pipefail
          python - <<'PY'
          from __future__ import annotations
          import json
          import os
          import re
          import time
          import urllib.error
          import urllib.parse
          import urllib.request
          from datetime import datetime, timezone
          from pathlib import Path
          from typing import Any

          import boto3
          from botocore.exceptions import BotoCoreError, ClientError

          HANDLES_PATH = Path("docs/model_spec/platform/migration_to_dev/dev_full_handles.registry.v0.md")

          def now() -> str:
              return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

          def parse_handles(path: Path) -> dict[str, object]:
              out: dict[str, object] = {}
              rx = re.compile(r"^\* `([^`=]+?)\s*=\s*([^`]+)`")
              for line in path.read_text(encoding="utf-8").splitlines():
                  m = rx.match(line.strip())
                  if not m:
                      continue
                  key = m.group(1).strip()
                  raw = m.group(2).strip()
                  if raw.startswith('"') and raw.endswith('"'):
                      value: object = raw[1:-1]
                  elif raw.lower() == "true":
                      value = True
                  elif raw.lower() == "false":
                      value = False
                  else:
                      try:
                          value = int(raw) if "." not in raw else float(raw)
                      except ValueError:
                          value = raw
                  out[key] = value
              return out

          def is_placeholder(value: object) -> bool:
              s = str(value).strip()
              sl = s.lower()
              if not s:
                  return True
              if sl in {"tbd", "todo", "none", "null", "unset"}:
                  return True
              if "placeholder" in sl or "to_pin" in sl:
                  return True
              if "<" in s and ">" in s:
                  return True
              return False

          def s3_get_json(s3: Any, bucket: str, key: str) -> dict[str, Any]:
              raw = s3.get_object(Bucket=bucket, Key=key)["Body"].read().decode("utf-8")
              payload = json.loads(raw)
              if not isinstance(payload, dict):
                  raise ValueError("json_not_object")
              return payload

          def s3_put_json(s3: Any, bucket: str, key: str, payload: dict[str, Any]) -> None:
              body = (json.dumps(payload, indent=2, ensure_ascii=True) + "\n").encode("utf-8")
              s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")
              s3.head_object(Bucket=bucket, Key=key)

          def dedupe(blockers: list[dict[str, str]]) -> list[dict[str, str]]:
              out: list[dict[str, str]] = []
              seen: set[tuple[str, str]] = set()
              for b in blockers:
                  code = str(b.get("code", "")).strip()
                  msg = str(b.get("message", "")).strip()
                  sig = (code, msg)
                  if sig in seen:
                      continue
                  seen.add(sig)
                  out.append({"code": code, "message": msg})
              return out

          def dbx_json(base_url: str, token: str, method: str, path: str, payload: dict[str, Any] | None = None) -> dict[str, Any]:
              url = f"{base_url.rstrip('/')}{path}"
              data = None
              headers = {
                  "Authorization": f"Bearer {token}",
                  "Content-Type": "application/json",
              }
              if payload is not None:
                  data = json.dumps(payload).encode("utf-8")
              req = urllib.request.Request(url=url, data=data, headers=headers, method=method.upper())
              with urllib.request.urlopen(req, timeout=60) as resp:
                  body = resp.read().decode("utf-8")
                  return json.loads(body) if body.strip() else {}

          def dbx_mlflow_json(base_url: str, token: str, method: str, path: str, payload: dict[str, Any] | None = None) -> dict[str, Any]:
              candidates = [path]
              if path.startswith("/api/2.0/mlflow/"):
                  candidates.append(path.replace("/api/2.0/mlflow/", "/api/2.0/preview/mlflow/", 1))
              last_404: Exception | None = None
              for p in candidates:
                  try:
                      return dbx_json(base_url=base_url, token=token, method=method, path=p, payload=payload)
                  except urllib.error.HTTPError as exc:
                      if exc.code == 404:
                          last_404 = exc
                          continue
                      raise
              if last_404 is not None:
                  raise last_404
              raise RuntimeError("mlflow_endpoint_resolution_failed")

          exec_id = os.environ["EXEC_ID"].strip()
          run_dir = Path(os.environ["RUN_DIR"].strip())
          bucket = os.environ["EVIDENCE_BUCKET"].strip()
          upstream = os.environ["UPSTREAM_M11E_EXEC"].strip()
          region = os.environ.get("AWS_REGION", "eu-west-2").strip() or "eu-west-2"

          if not upstream:
              raise SystemExit("UPSTREAM_M11E_EXEC is required for M11.F.")

          s3 = boto3.client("s3", region_name=region)
          ssm = boto3.client("ssm", region_name=region)
          handles = parse_handles(HANDLES_PATH)
          blockers: list[dict[str, str]] = []
          read_errors: list[dict[str, str]] = []
          upload_errors: list[dict[str, str]] = []
          notes: list[str] = []

          required_handles = [
              "MLFLOW_HOSTING_MODE",
              "MLFLOW_EXPERIMENT_PATH",
              "MLFLOW_MODEL_NAME",
              "SSM_MLFLOW_TRACKING_URI_PATH",
              "SSM_DATABRICKS_WORKSPACE_URL_PATH",
              "SSM_DATABRICKS_TOKEN_PATH",
          ]
          missing = [k for k in required_handles if k not in handles]
          placeholders = [k for k in required_handles if k in handles and is_placeholder(handles.get(k))]
          if missing or placeholders:
              blockers.append({"code": "M11-B6.1", "message": "Required lineage handles unresolved: " + ",".join(sorted(missing + placeholders))})

          m11e_summary_key = f"evidence/dev_full/run_control/{upstream}/m11e_execution_summary.json"
          m11e_snapshot_key = f"evidence/dev_full/run_control/{upstream}/m11e_eval_gate_snapshot.json"
          m11e_summary: dict[str, Any] | None = None
          m11e_snapshot: dict[str, Any] | None = None
          try:
              m11e_summary = s3_get_json(s3, bucket, m11e_summary_key)
              m11e_snapshot = s3_get_json(s3, bucket, m11e_snapshot_key)
          except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
              read_errors.append({"surface": "m11e_summary_or_snapshot", "error": type(exc).__name__})
              blockers.append({"code": "M11-B6.2", "message": "M11.E evidence unreadable."})

          platform_run_id = ""
          scenario_run_id = ""
          upstream_m11d = ""
          eval_report_key = ""
          leakage_prov_key = ""
          m11d_summary_key = ""
          m11d_snapshot_key = ""
          m11d_summary: dict[str, Any] | None = None
          m11d_snapshot: dict[str, Any] | None = None
          eval_report: dict[str, Any] | None = None
          leakage_prov: dict[str, Any] | None = None

          if m11e_summary and m11e_snapshot:
              if not bool(m11e_summary.get("overall_pass")):
                  blockers.append({"code": "M11-B6.2", "message": "M11.E summary not pass posture."})
              if str(m11e_summary.get("next_gate", "")).strip() != "M11.F_READY":
                  blockers.append({"code": "M11-B6.2", "message": "M11.E next_gate is not M11.F_READY."})
              platform_run_id = str(m11e_summary.get("platform_run_id", "")).strip()
              scenario_run_id = str(m11e_summary.get("scenario_run_id", "")).strip()
              upstream_m11d = str(m11e_summary.get("upstream_refs", {}).get("m11d_execution_id", "")).strip()
              eval_report_key = str(m11e_snapshot.get("source_refs", {}).get("mf_eval_report_key", "")).strip()
              leakage_prov_key = str(m11e_snapshot.get("source_refs", {}).get("mf_leakage_provenance_check_key", "")).strip()
              if not platform_run_id or not scenario_run_id or not upstream_m11d:
                  blockers.append({"code": "M11-B6.2", "message": "M11.E run scope/upstream refs incomplete."})
              if not eval_report_key or not leakage_prov_key:
                  blockers.append({"code": "M11-B6.2", "message": "M11.E source refs for eval/provenance missing."})

          if upstream_m11d:
              m11d_summary_key = f"evidence/dev_full/run_control/{upstream_m11d}/m11d_execution_summary.json"
              m11d_snapshot_key = f"evidence/dev_full/run_control/{upstream_m11d}/m11d_train_eval_execution_snapshot.json"
              try:
                  m11d_summary = s3_get_json(s3, bucket, m11d_summary_key)
                  m11d_snapshot = s3_get_json(s3, bucket, m11d_snapshot_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": "m11d_summary_or_snapshot", "error": type(exc).__name__})
                  blockers.append({"code": "M11-B6.2", "message": "M11.D evidence unreadable from M11.E refs."})
              if m11d_summary and str(m11d_summary.get("next_gate", "")).strip() != "M11.E_READY":
                  blockers.append({"code": "M11-B6.2", "message": "M11.D next_gate is not M11.E_READY."})

          if eval_report_key:
              try:
                  eval_report = s3_get_json(s3, bucket, eval_report_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": eval_report_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B6.2", "message": "MF eval report unreadable."})
          if leakage_prov_key:
              try:
                  leakage_prov = s3_get_json(s3, bucket, leakage_prov_key)
              except (BotoCoreError, ClientError, ValueError, json.JSONDecodeError) as exc:
                  read_errors.append({"surface": leakage_prov_key, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B6.2", "message": "MF leakage provenance report unreadable."})

          m10g_fingerprint_ref = ""
          if m11d_snapshot:
              m10g_fingerprint_ref = str(m11d_snapshot.get("input_refs", {}).get("m10g_fingerprint_ref", "")).strip()
              if not m10g_fingerprint_ref:
                  blockers.append({"code": "M11-B6.5", "message": "m10g_fingerprint_ref missing from M11.D lineage input refs."})

          tracking_uri = ""
          workspace_url = ""
          dbx_token = ""
          tracking_uri_path = str(handles.get("SSM_MLFLOW_TRACKING_URI_PATH", "")).strip()
          workspace_path = str(handles.get("SSM_DATABRICKS_WORKSPACE_URL_PATH", "")).strip()
          token_path = str(handles.get("SSM_DATABRICKS_TOKEN_PATH", "")).strip()
          try:
              if tracking_uri_path:
                  tracking_uri = ssm.get_parameter(Name=tracking_uri_path)["Parameter"]["Value"]
              if workspace_path:
                  workspace_url = ssm.get_parameter(Name=workspace_path)["Parameter"]["Value"]
              if token_path:
                  dbx_token = ssm.get_parameter(Name=token_path, WithDecryption=True)["Parameter"]["Value"]
          except (BotoCoreError, ClientError) as exc:
              read_errors.append({"surface": "ssm_mlflow_databricks", "error": type(exc).__name__})
              blockers.append({"code": "M11-B6.3", "message": "Failed to resolve MLflow/Databricks secret surfaces from SSM."})

          if not str(tracking_uri).strip() or not str(workspace_url).strip() or not str(dbx_token).strip():
              blockers.append({"code": "M11-B6.3", "message": "MLflow tracking URI or Databricks workspace/token is empty."})

          if str(handles.get("MLFLOW_HOSTING_MODE", "")).strip() != "databricks_managed":
              blockers.append({"code": "M11-B6.1", "message": "MLFLOW_HOSTING_MODE is not databricks_managed."})

          mlflow_experiment_path = str(handles.get("MLFLOW_EXPERIMENT_PATH", "")).strip()
          mlflow_model_name = str(handles.get("MLFLOW_MODEL_NAME", "")).strip()
          experiment_id = ""
          run_id = ""
          run_status = ""
          run_tags_present: list[str] = []
          run_tags_missing: list[str] = []
          api_error = ""
          logged_metrics: dict[str, float] = {}

          if not blockers:
              try:
                  exp_name_q = urllib.parse.quote(mlflow_experiment_path, safe="")
                  exp_resp = dbx_mlflow_json(
                      base_url=workspace_url,
                      token=dbx_token,
                      method="GET",
                      path=f"/api/2.0/mlflow/experiments/get-by-name?experiment_name={exp_name_q}",
                  )
                  experiment = exp_resp.get("experiment")
                  if not isinstance(experiment, dict):
                      create_resp = dbx_mlflow_json(
                          base_url=workspace_url,
                          token=dbx_token,
                          method="POST",
                          path="/api/2.0/mlflow/experiments/create",
                          payload={"name": mlflow_experiment_path},
                      )
                      experiment_id = str(create_resp.get("experiment_id", "")).strip()
                  else:
                      experiment_id = str(experiment.get("experiment_id", "")).strip()
                  if not experiment_id:
                      raise RuntimeError("experiment_id_missing")

                  run_name = f"{platform_run_id}:{exec_id}"
                  create_run = dbx_mlflow_json(
                      base_url=workspace_url,
                      token=dbx_token,
                      method="POST",
                      path="/api/2.0/mlflow/runs/create",
                      payload={
                          "experiment_id": experiment_id,
                          "start_time": int(time.time() * 1000),
                          "tags": [
                              {"key": "mlflow.runName", "value": run_name},
                              {"key": "platform_run_id", "value": platform_run_id},
                              {"key": "scenario_run_id", "value": scenario_run_id},
                              {"key": "m11d_execution_id", "value": upstream_m11d},
                              {"key": "m11e_execution_id", "value": upstream},
                              {"key": "m11f_execution_id", "value": exec_id},
                              {"key": "m10g_fingerprint_ref", "value": m10g_fingerprint_ref},
                              {"key": "leakage_provenance_ref", "value": f"s3://{bucket}/{leakage_prov_key}"},
                              {"key": "mf_model_name", "value": mlflow_model_name},
                          ],
                      },
                  )
                  run = create_run.get("run", {})
                  info = run.get("info", {}) if isinstance(run, dict) else {}
                  run_id = str(info.get("run_id", "")).strip()
                  if not run_id:
                      raise RuntimeError("run_id_missing")

                  metrics_src = eval_report.get("metrics", {}) if isinstance(eval_report, dict) else {}
                  if not isinstance(metrics_src, dict):
                      metrics_src = {}
                  ts_ms = int(time.time() * 1000)
                  metric_payload = []
                  for mk in ("accuracy", "precision", "recall"):
                      raw = metrics_src.get(mk)
                      if raw is None:
                          continue
                      try:
                          val = float(raw)
                      except Exception:
                          continue
                      logged_metrics[mk] = val
                      metric_payload.append({"key": mk, "value": val, "timestamp": ts_ms, "step": 0})

                  dbx_mlflow_json(
                      base_url=workspace_url,
                      token=dbx_token,
                      method="POST",
                      path="/api/2.0/mlflow/runs/log-batch",
                      payload={"run_id": run_id, "metrics": metric_payload, "params": [], "tags": []},
                  )

                  dbx_mlflow_json(
                      base_url=workspace_url,
                      token=dbx_token,
                      method="POST",
                      path="/api/2.0/mlflow/runs/update",
                      payload={"run_id": run_id, "status": "FINISHED", "end_time": int(time.time() * 1000)},
                  )

                  run_get = dbx_mlflow_json(
                      base_url=workspace_url,
                      token=dbx_token,
                      method="GET",
                      path=f"/api/2.0/mlflow/runs/get?run_id={urllib.parse.quote(run_id, safe='')}",
                  )
                  run_obj = run_get.get("run", {}) if isinstance(run_get, dict) else {}
                  run_info = run_obj.get("info", {}) if isinstance(run_obj, dict) else {}
                  run_status = str(run_info.get("status", "")).strip()
                  tag_rows = run_obj.get("data", {}).get("tags", []) if isinstance(run_obj.get("data", {}), dict) else []
                  tag_map: dict[str, str] = {}
                  if isinstance(tag_rows, list):
                      for row in tag_rows:
                          if isinstance(row, dict):
                              k = str(row.get("key", "")).strip()
                              v = str(row.get("value", "")).strip()
                              if k:
                                  tag_map[k] = v
                  required_tag_keys = [
                      "platform_run_id",
                      "scenario_run_id",
                      "m11d_execution_id",
                      "m11e_execution_id",
                      "m11f_execution_id",
                      "m10g_fingerprint_ref",
                      "leakage_provenance_ref",
                      "mf_model_name",
                  ]
                  run_tags_present = sorted([k for k in required_tag_keys if k in tag_map and tag_map[k]])
                  run_tags_missing = sorted([k for k in required_tag_keys if k not in tag_map or not tag_map[k]])
                  if run_tags_missing:
                      blockers.append({"code": "M11-B6.5", "message": "Required MLflow lineage tags missing: " + ",".join(run_tags_missing)})
                  if run_status and run_status.upper() not in {"FINISHED", "SCHEDULED", "RUNNING"}:
                      blockers.append({"code": "M11-B6.4", "message": f"MLflow run ended in unexpected status: {run_status}"})
              except Exception as exc:
                  api_error = f"{type(exc).__name__}:{exc}"
                  blockers.append({"code": "M11-B6.4", "message": "Managed MLflow lineage commit failed."})

          if platform_run_id and scenario_run_id and eval_report and leakage_prov:
              eval_scope_ok = (
                  str(eval_report.get("platform_run_id", "")).strip() == platform_run_id
                  and str(eval_report.get("scenario_run_id", "")).strip() == scenario_run_id
              )
              leakage_scope_ok = (
                  str(leakage_prov.get("platform_run_id", "")).strip() == platform_run_id
                  and str(leakage_prov.get("scenario_run_id", "")).strip() == scenario_run_id
              )
              if not eval_scope_ok or not leakage_scope_ok:
                  blockers.append({"code": "M11-B6.5", "message": "Run-scope mismatch across eval/leakage lineage refs."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.G_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_G" if overall_pass else "HOLD_REMEDIATE"

          lineage_snapshot = {
              "captured_at_utc": now(),
              "phase": "M11.F",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "upstream_refs": {
                  "m11e_execution_id": upstream,
                  "m11d_execution_id": upstream_m11d,
              },
              "handles": {
                  "MLFLOW_HOSTING_MODE": str(handles.get("MLFLOW_HOSTING_MODE", "")),
                  "MLFLOW_EXPERIMENT_PATH": mlflow_experiment_path,
                  "MLFLOW_MODEL_NAME": mlflow_model_name,
                  "SSM_MLFLOW_TRACKING_URI_PATH": tracking_uri_path,
                  "SSM_DATABRICKS_WORKSPACE_URL_PATH": workspace_path,
                  "SSM_DATABRICKS_TOKEN_PATH": token_path,
              },
              "mlflow": {
                  "tracking_uri": str(tracking_uri).strip(),
                  "workspace_url": str(workspace_url).strip(),
                  "experiment_id": experiment_id,
                  "run_id": run_id,
                  "run_status": run_status,
                  "logged_metrics": logged_metrics,
                  "required_tags_present": run_tags_present,
                  "required_tags_missing": run_tags_missing,
                  "api_error": api_error,
              },
              "source_refs": {
                  "m11e_summary_key": m11e_summary_key,
                  "m11e_snapshot_key": m11e_snapshot_key,
                  "m11d_summary_key": m11d_summary_key,
                  "m11d_snapshot_key": m11d_snapshot_key,
                  "mf_eval_report_key": eval_report_key,
                  "mf_leakage_provenance_check_key": leakage_prov_key,
                  "m10g_fingerprint_ref": m10g_fingerprint_ref,
              },
              "read_errors": read_errors,
              "upload_errors": upload_errors,
              "notes": notes,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "next_gate": next_gate,
          }
          blocker_register = {
              "captured_at_utc": now(),
              "phase": "M11.F",
              "phase_id": "P14",
              "execution_id": exec_id,
              "blocker_count": len(blockers),
              "blockers": blockers,
              "overall_pass": overall_pass,
          }
          summary = {
              "captured_at_utc": now(),
              "phase": "M11.F",
              "phase_id": "P14",
              "execution_id": exec_id,
              "platform_run_id": platform_run_id,
              "scenario_run_id": scenario_run_id,
              "overall_pass": overall_pass,
              "blocker_count": len(blockers),
              "verdict": verdict,
              "next_gate": next_gate,
              "upstream_refs": {
                  "m11e_execution_id": upstream,
                  "m11d_execution_id": upstream_m11d,
              },
              "artifact_keys": {
                  "m11f_mlflow_lineage_snapshot": f"evidence/dev_full/run_control/{exec_id}/m11f_mlflow_lineage_snapshot.json",
                  "m11f_blocker_register": f"evidence/dev_full/run_control/{exec_id}/m11f_blocker_register.json",
                  "m11f_execution_summary": f"evidence/dev_full/run_control/{exec_id}/m11f_execution_summary.json",
              },
          }

          run_dir.mkdir(parents=True, exist_ok=True)
          artifacts = {
              "m11f_mlflow_lineage_snapshot.json": lineage_snapshot,
              "m11f_blocker_register.json": blocker_register,
              "m11f_execution_summary.json": summary,
          }
          for fname, payload in artifacts.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

          run_control_prefix = f"evidence/dev_full/run_control/{exec_id}/"
          for fname, payload in artifacts.items():
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except (BotoCoreError, ClientError) as exc:
                  upload_errors.append({"surface": run_control_prefix + fname, "error": type(exc).__name__})
                  blockers.append({"code": "M11-B6.6", "message": f"Failed to upload {fname}."})

          blockers = dedupe(blockers)
          overall_pass = len(blockers) == 0
          next_gate = "M11.G_READY" if overall_pass else "HOLD_REMEDIATE"
          verdict = "ADVANCE_TO_M11_G" if overall_pass else "HOLD_REMEDIATE"
          lineage_snapshot["upload_errors"] = upload_errors
          lineage_snapshot["overall_pass"] = overall_pass
          lineage_snapshot["blocker_count"] = len(blockers)
          lineage_snapshot["next_gate"] = next_gate
          blocker_register["blocker_count"] = len(blockers)
          blocker_register["blockers"] = blockers
          blocker_register["overall_pass"] = overall_pass
          summary["overall_pass"] = overall_pass
          summary["blocker_count"] = len(blockers)
          summary["verdict"] = verdict
          summary["next_gate"] = next_gate

          for fname, payload in {
              "m11f_mlflow_lineage_snapshot.json": lineage_snapshot,
              "m11f_blocker_register.json": blocker_register,
              "m11f_execution_summary.json": summary,
          }.items():
              (run_dir / fname).write_text(json.dumps(payload, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")
              try:
                  s3_put_json(s3, bucket, run_control_prefix + fname, payload)
              except Exception:
                  pass

          print(
              json.dumps(
                  {
                      "execution_id": exec_id,
                      "upstream_m11e_execution": upstream,
                      "overall_pass": overall_pass,
                      "blocker_count": len(blockers),
                      "next_gate": next_gate,
                      "verdict": verdict,
                      "evidence_prefix": f"s3://{bucket}/{run_control_prefix}",
                  },
                  indent=2,
                  ensure_ascii=True,
              )
          )
          if not overall_pass:
              raise SystemExit(1)
          PY

      - name: Upload local run artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dev-full-m11-${{ inputs.m11_subphase }}-${{ steps.run_meta.outputs.execution_id }}
          path: ${{ steps.run_meta.outputs.run_dir }}
          if-no-files-found: warn
